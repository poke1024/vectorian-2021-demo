{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e2ae0-3099-4e08-9aa2-70a567b9163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"code\")\n",
    "import nbutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702daa3-f241-431a-88de-e97c8cb8d49b",
   "metadata": {},
   "source": [
    "We first tell the notebook logic whether we have a full bokeh server. This *is* the case for local jupyter installations, but is *not* the case for notebooks running on mybinder - in the latter case we have some limits on interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6be29-90ac-4c33-a466-a1739dd4d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nbutils.initialize(has_bokeh_server=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-texas",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b7cb2-2542-4573-ba41-ba27214c8e1f",
   "metadata": {},
   "source": [
    "In this notebook we will set forth the full stack of decisions that need to be taken in order to compute multi-token (i.e. sentence or document) similarities from embeddings. Not only will we work with different embeddings but also with different ways of leveraging these embeddings to compute phrase similarities. While our examples are following single paths of computation, we contextualize our decisions and allow interactive readers to take different decisions in terms of embeddings, algorithms and parameters.\n",
    "\n",
    "We will experiment with four classes of embeddings (see the diagram below for a classification):\n",
    "\n",
    "* Static token embeddings: these operate on the token level such. We experiment with GloVe (Pennington et al. 2014), fastText (Mikolov et al., 2017) and Numberbatch (Speer et al, 2018). We use these three to compute token similarity and combine this with alignment algorithms (such as Waterman-Smith-Beyer) to compute document similarity. We also investigate the effect of stacking two static embeddings (fastText and Numberbatch).\n",
    "* Contextual token embeddings: these also operate on the token level, i.e. embeddings that change according to a specific token instance's context. In this notebook we experiment with using such token embeddings from a sentence bert model.\n",
    "* Document embeddings derived from specially trained models. Document embeddings represent one document via one single embedding. We use document embeddings obtained from a BERT model. More specifically, we use a Sentence-BERT model trained for the semantic textual similarity (STS) task (Reimers and Gurevych, 2019).\n",
    "* Document embeddings derived from token embeddings. We also experiment with averaging different kinds of token embeddings (static and contextual) to derive document embeddings.\n",
    "\n",
    "![Different kinds of embeddings](miscellaneous/diagram_embeddings.svg)\n",
    "\n",
    "For reasons of limited RAM and download times, we use small or compressed versions of the static embeddings we work with. For GloVe, we use the official 50-dimensional version of the 6B variant. For fastText we use a version that was compressed using the standard settings in https://github.com/avidale/compress-fasttext. For Numberbatch we use a 50-dimension version that was reduced using a standard PCA.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-guidance",
   "metadata": {},
   "source": [
    "# Technical Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.io\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-capital",
   "metadata": {},
   "source": [
    "# Choosing Static Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-professional",
   "metadata": {},
   "source": [
    "First we need:\n",
    "    \n",
    "    * a set of documents of search over (i.e. our corpus)\n",
    "    * a set of word embeddings to employ for these searches\n",
    "    \n",
    "For the latter, we turn to Vectorian's embedding zoo, which offers a number of pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import Zoo\n",
    "\n",
    "#Zoo.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-burden",
   "metadata": {},
   "source": [
    "Let's load the static embeddings as described above from Vectorian's model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import StackedEmbedding\n",
    "\n",
    "emb_glove = Zoo.load('glove-6B-50')\n",
    "emb_numberbatch = Zoo.load('numberbatch-19.08-en-50')\n",
    "emb_fasttext = Zoo.load('fasttext-en-mini')\n",
    "emb_fasttext_numberbatch = StackedEmbedding([emb_fasttext, emb_numberbatch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0a78e-030c-49ea-b49e-718fefebeeb6",
   "metadata": {},
   "source": [
    "We also instantiate an NLP parser based on sentence bert and a shim to use this model's token embeddings in the Vectorian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fe0d8-8f4d-4af8-9d19-4a7e3c7280c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(nbutils)\n",
    "\n",
    "nlp = nbutils.make_nlp()\n",
    "\n",
    "from vectorian.embeddings import SentenceBertEmbedding\n",
    "emb_sbert = SentenceBertEmbedding(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-length",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-apache",
   "metadata": {},
   "source": [
    "First load our gold standard that contains our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/raw_data/gold.json\", \"r\") as f:\n",
    "    gold = nbutils.Gold(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.phrases[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.matches('to be or not to be')[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-pasta",
   "metadata": {},
   "source": [
    "We are now ready to build a Vectorian session that contains our documents and embeddings. We use preprocessed corpus data. For details, how this was achieved, see `code/prepare_corpus.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.session import LabSession\n",
    "from vectorian.corpus import Corpus\n",
    "\n",
    "session = LabSession(\n",
    "    Corpus.load(\"data/processed_data/corpus\"),\n",
    "    embeddings=[\n",
    "        emb_sbert,\n",
    "        emb_glove,\n",
    "        emb_numberbatch,\n",
    "        emb_fasttext,\n",
    "        emb_fasttext_numberbatch],\n",
    "    normalizers=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-canal",
   "metadata": {},
   "source": [
    "Let's take a look at the gold standard we imported and whose documents now live inside `session`. We have 20 queries (blue circles), and for each query we have a number of documents (green circles) that we regard as correct matches to these queries. All documents that are not attached to the query are - by definition - no correct matches.\n",
    "\n",
    "Note for interactive users: you can hover your mouse over the nodes to see their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0a4e2-5640-43eb-8950-394da395f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_gold(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2bcfd-c597-44c3-81bc-34cbab10bdef",
   "metadata": {},
   "source": [
    "# What are Word Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7b8be-f3b6-4604-b924-7a453ddc89d0",
   "metadata": {},
   "source": [
    "We now turn to single word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2633d27-0863-4958-bf06-036b8af37604",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.word_vec(emb_glove, \"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95ce2-cdc8-44dc-a69e-06c3c1feb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.metrics import TokenSimilarity, CosineSimilarity\n",
    "\n",
    "token_sim = TokenSimilarity(\n",
    "    emb_numberbatch,\n",
    "    CosineSimilarity()\n",
    ")\n",
    "\n",
    "session.similarity(token_sim, \"hot\", \"cold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82787688-a005-4bab-bcff-5954b333ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sim = TokenSimilarity(\n",
    "    emb_glove,\n",
    "    CosineSimilarity())\n",
    "\n",
    "session.similarity(token_sim, \"hot\", \"cold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96153b53-e9c4-4201-a55d-3d90e7d4f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sim = TokenSimilarity(\n",
    "    emb_sbert,\n",
    "    CosineSimilarity())\n",
    "\n",
    "a = list(session.documents[0].spans(session.partition(\"document\")))[0][3]\n",
    "b = list(session.documents[3].spans(session.partition(\"document\")))[0][2]\n",
    "session.similarity(token_sim, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c4165-4cd5-4083-92f7-ba28c34e2af3",
   "metadata": {},
   "source": [
    "# Exploring Word Embeddings (and our data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942597d-d5db-48e1-b02f-bb1f8084cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.browse(gold, \"rest is silence\", \"Fig for Fortune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bc950-6e6d-4585-b98f-688484df28e9",
   "metadata": {},
   "source": [
    "This is an example, where similarity and therefore embeddings won't help us much. The syntactic structure is mirrored, and \"silence\" is replaced with \"all but wind\". Even if we focus on nouns only, \"silence\" and \"wind\" are not generally similar. Still an embedding approach should be able to recognize that the  words at the beginning of phrase are exact matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa324fc-042b-474c-9ac2-6c28e61d4a41",
   "metadata": {},
   "source": [
    "If we inspect the cosine similarity of the token \"silence\" with other tokens in the context under three of our embeddings, we see that there is more connection between \"silence\" and \"wind\" than we expected. Still, the absolute value of 0.3 for numberbatch is low. Interestingly, glove associates \"silence\" with \"action\", i.e. an opposite. The phenomenon that embeddings sometimes cluster opposites is a common observation and can be an issue when wanting to differentiate between these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65bcc2-678a-4cdb-8a04-99df700c33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"silence\", \"Fig for Fortune\", n_figures=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41a31d-d7c0-4bb7-8fc7-adb824a7fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.browse(gold, \"sea of troubles\", \"Book of Common Prayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfacd0-75f9-4cb5-a7a6-927b4e82ca42",
   "metadata": {},
   "source": [
    "This is a different example, where similarity computation might help. Here, \"sea\" is replaced by \"waves\", and \"troubles\" by \"troublesome\". We should expect to get reasonable results with results on this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c58bda-6bbc-49b9-8f91-01ec5ea660d4",
   "metadata": {},
   "source": [
    "Indeed, by inspecting the cosine similarity of the token \"sea\" with other tokens in the context, we see that this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae847b0b-2ea7-4e97-994f-405d32e3bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"sea\", \"The Book of Common Prayer\", n_figures=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2c55b-2194-436a-94f8-dde47505d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"troubles\", \"The Book of Common Prayer\", n_figures=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97c62-4bef-48f6-8c08-83b585afd4ca",
   "metadata": {},
   "source": [
    "Note how out-of-vocabulary words like \"troublesomest\" will produce zero similarities under standard key-value embeddings, whereas fastText is still able to produce a vector thanks to subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddff82-b84e-4289-a409-debc26d87287",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"troublesomest\", \"The Book of Common Prayer\", n_figures=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844c0f3-4256-48a2-aac9-5ff65c6c8e6c",
   "metadata": {},
   "source": [
    "# Exploring Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-aggregate",
   "metadata": {},
   "source": [
    "Before we turn to alignment strategies to match sentences token by token, we first look at representing each document with one embedding in order to gather an understanding how different embedding strategies relate to the nearness of documents. We will later turn to individual token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-controversy",
   "metadata": {},
   "source": [
    "We first prepare additional sentence embeddings using SBERT that we will show in our first big visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import CachedPartitionEncoder, SpanEncoder\n",
    "\n",
    "sbert_encoder = CachedPartitionEncoder(SpanEncoder(\n",
    "    lambda texts: [nlp(t).vector for t in texts]))\n",
    "\n",
    "sbert_encoder.try_load(\"data/processed_data/doc_embeddings\")\n",
    "sbert_encoder.cache(session.documents, session.partition(\"document\"))\n",
    "sbert_encoder.save(\"data/processed_data/doc_embeddings\")\n",
    "\n",
    "sbert_encoder_name = nlp.meta[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d93851-2895-422d-92d0-586aa540d480",
   "metadata": {},
   "source": [
    "Now we construct an Explorer class. In addition to providing the SBERT encoder we just built, we configure the Explorer to use averaging to build documents embeddings from token embeddings.\n",
    "\n",
    "Note to interactive readers: you can change the \"mean\" (averaging) method to other methods for computing document tokens as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fb8cc-5328-483c-becd-9a86fb091192",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer = nbutils.DocEmbeddingExplorer(\n",
    "    session=session, nlp=nlp, gold=gold, extra_encoders={sbert_encoder_name: sbert_encoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ecd2c-91c3-4dd5-af4a-ac578d2a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer.plot([\n",
    "    {\"encoder\": \"paraphrase_distilroberta\", \"locator\": (\"fixed\", \"carry coals\"), 'has_tok_emb': False},\n",
    "    {\"encoder\": \"paraphrase_distilroberta\", \"locator\": (\"fixed\", \"an old man is twice\"), 'has_tok_emb': False}\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fbac7-6a37-411a-9611-42022952a250",
   "metadata": {},
   "source": [
    "In the TSNE visualization above, dots are documents and the colors are the query that yields that document in our gold standard. By hovering over dots with the mouse you get details on the document and query the dot represents. Nearby dots of the same color indicate that the embedding tends to cluster documents similar to our gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bcebe-bb6c-467c-a147-d7588e28917b",
   "metadata": {},
   "source": [
    "On the left above, we see that the phrase \"we will not carry coals\" (large green-yellow circle with cross) in located closely to the documents associated with that query (smaller green-yellow circles). Similarly, on the right we see that the phrase \"an old man is twice a child\" clusters with the actual (green) documents we associate with it in our gold standard.\n",
    "\n",
    "For these phrases and documents, the `paraphrase_distilroberta` model does a good job of producing a document embedding that actually separates inherent topics (without us telling it to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer.plot([\n",
    "    {\"encoder\": \"numberbatch\", \"selection\": [\n",
    "        'ww_32c26a7909c83bda',\n",
    "        'ww_b5b8083a6a1282bc',\n",
    "        'ww_9a6cb20b0b157545',\n",
    "        'ww_a6f4b0e3428ad510',\n",
    "        'ww_8e68a517bc3ecceb']}\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93e356-ec78-442b-b445-d9f5b6c2ed25",
   "metadata": {},
   "source": [
    "In the plot above we look at the document embedding produced by a **token-based** embedding. This has the advantage that we can actually look at token embeddings that make up the document embedding (through averaging). On the right side, we see a TSNE plot of all token embeddings that occur in the documents that are selected on the left. The hope is that this visualization will give us a clue why the documents on the left might be clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564ef2d-c165-44ed-a930-f85b9cd7f3d2",
   "metadata": {},
   "source": [
    "The red circles on the left represent contexts that match the phrase \"a horse, a horse, my kingform for a horse\" are mapped. If we look at the token embeddings (that includes documents from other other classes), we indeed see that a grouping happens due to word embeddings clustering around \"horse\" (right side), but we also see a cluster around \"boat\", \"sail\" and \"river\" on the left.\n",
    "\n",
    "In fact context 1 contains \"muscle boat\", context 2 contains \"To swim the river villain\", and context 3 contains \"A boat, a boat\". We thereby see that this kind of unsupervised document clustering clusters items due to inherent qualities that might not actually match our query criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a554d4e-9eb9-49f9-900e-ab554afd2757",
   "metadata": {},
   "source": [
    "Interactive note: you can compute different token embeddings plots by selecting different documents on the mouse (drag the mouse to lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a490bb-2a3c-4ae2-a687-2b74d60d4eb7",
   "metadata": {},
   "source": [
    "# Understanding Alignments (WSB vs WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-reverse",
   "metadata": {},
   "source": [
    "## A Search Query using Alignment over Similar Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aea237-1fe6-4903-89c9-328d7f9790c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_builder(**kwargs):\n",
    "    return nbutils.InteractiveIndexBuilder(session, nlp, partition_encoders={\n",
    "        sbert_encoder_name: sbert_encoder\n",
    "    }, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder = make_index_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2ff05-1807-4343-a819-9acb55328ccc",
   "metadata": {},
   "source": [
    "What you see above is the description of a search strategy that we will employ in the following sections of this notebook. Interactive readers can switch to the \"Edit\" part and actually explore the setting in more detail and even change it to something completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder.build_index().find(gold.phrases[0], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-helen",
   "metadata": {},
   "source": [
    "## Plotting the NDCG over the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6423e26-7035-4b1a-8f58-28634c95ef62",
   "metadata": {},
   "source": [
    "We first define a strategy for searching the corpus. In the summary below you will find the strategy used for the non-interactive version of this text. In the interactive version, you can click on \"Edit\" and change these settings and rerun the following sections of the notebook accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413a21e-c6cf-4f9d-8da1-3717281da294",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_a = make_index_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb41c7-3a2b-4014-b59d-d79a82701a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorian.alignment\n",
    "\n",
    "index_builder_b = make_index_builder(\n",
    "    strategy=\"Alignment\",\n",
    "    strategy_options={\"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"kusner\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9287a-5cf1-4c55-925b-50090b2a2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_c = make_index_builder(\n",
    "    strategy=\"Alignment\",\n",
    "    strategy_options={\"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"vectorian\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094e187-5337-4258-b2f4-66925dfa70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_d = make_index_builder(\n",
    "    strategy=\"Partition Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b98cf7-3e58-4699-a2e1-52db2996efe1",
   "metadata": {},
   "source": [
    "Now get an overview of the quality of the results we obtain when using the index configures with `index_builder` by computing the NDCG over all queries in our gold standard with regards to the known optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_ndcgs(gold, {\n",
    "    \"wsb\": index_builder_a.build_index(),\n",
    "    \"wmd (kus.)\": index_builder_b.build_index(),\n",
    "    \"wmd (vec.)\": index_builder_c.build_index(),\n",
    "    \"doc emb.\": index_builder_d.build_index()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4d4fd-6fda-48cc-b494-ede28ac06d3f",
   "metadata": {},
   "source": [
    "We see that some queries obtain 100%, i.e. the top results match the optimal ones given in our gold standard. We see that Waterman-Smith-Beyer (WSB) tends to perform a tad better than Word Mover's Distance (WMD), with the exception of \"though this be madness...\", where WMD outperforms WSB. In general the Vectorian modification of WMD, which does not use nbow, performs better than Kusner's original description of WMD. The one exception here \"livers white as milk.\"\n",
    "\n",
    "One advantage of WSB over the full WMD variants is its easy interpretability. WSB allows us to understand as a bijective  mapping between tokens, namely a subset of the query and a subset of the document. For WMD, this assumption of bijection often breaks down. We use this character of WSB in the following section to illustrate which mappings actually occur.\n",
    "\n",
    "Let's look at some queries, where the performance for WSB is bad, and try to understand why our search fails to obtain the optimal results at the top of the result list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-endorsement",
   "metadata": {},
   "source": [
    "## Focussing on single queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235906d-7b57-4f5f-a446-67cee9a722a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder = make_index_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5c430-0b9b-4ac3-86fd-eb01dd623757",
   "metadata": {},
   "source": [
    "For this, we turn to the query with the lowest score \"though this be madness, yet there is a method in it\", and look at its results in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a = nbutils.plot_results(gold, index_builder.build_index(), \"though this be madness\", rank=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c796b4-7ab3-4671-9f30-fc9801cfdfbf",
   "metadata": {},
   "source": [
    "The best match obtained here - on rank 6 - is anchored on two word matches, namely `madness` (a 100% match) and `methods` (a 72% match). The other words are quite different and there is no good alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d210159-ed28-4412-99ad-47956bb0c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_b = nbutils.plot_results(gold, index_builder.build_index(), \"though this be madness\", rank=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5d7f1-3c98-4a90-98ab-7896be9692c1",
   "metadata": {},
   "source": [
    "Above we see the rank 3 result from the same query, which is a false positive - i.e. our search proclaims it is a better result than the one we saw before, but in fact this result is not relevant according to our gold standard. If we analyze why this result gets such a high score nonetheless, we see that \"is\" and \"in\" both contribute big 100% scores. In contrast to the scores before, 100% for \"madness\" and 72% for \"methods\", this partially explains the higher overall score (if we assume for now that the contributions from the other tokens are somewhat similar).\n",
    "\n",
    "Let us try to understand why the \"correct\" (true positive) results are ranked rather low and what we could do about it by looking at the composition of scores for each result we obtain for this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da85ba-0a96-477e-b963-ac2ec97c9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.vis_token_scores(plot_b.matches[:50], highlight={\n",
    "    \"token\": [\"madness\", \"method\"],\n",
    "    \"rank\": [6, 20, 35, 45]\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d9b9a-39ab-4fd0-9e86-a5005d1b5538",
   "metadata": {},
   "source": [
    "The relevant (true positive) results are marked with black triangles. We see that our current search strategy isn't doing a very good job of ranking them highly.\n",
    "\n",
    "Looking at the score composition of the relevant results, we can make out two distinct features: all relevant results show a rather large contribution of either \"madness\" (look at rang 6 and rank 35, for example) and/or a rather large contribution of \"method\" (ranks 45 and 6).\n",
    "\n",
    "However, these contributions do not lead to higher ranks necessarily, since other words such as \"is\", \"this\" and \"though\" score higher for other results: for example, look at the contribution of \"in\" for rank 1.\n",
    "\n",
    "In the next plot below, we visualize this observation using ranks 1, 6 and 35. Compare the rank 1 result on the left - which is a false positive - with the two relevant results on the right. Again, we see that \"in\", \"through\" and \"is\" make up large parts of the score for rank 1, whereas \"madness\" is a considerable factor for the two relevant matches. Unfortunately, this contribution is not sufficient to bring these results to higher ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa2956-8ccb-4208-ace2-b43d4b6098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "@widgets.interact(plot_as=widgets.ToggleButtons(options=['bar', 'pie'], value='pie'))\n",
    "def plot(plot_as):\n",
    "    nbutils.vis_token_scores(plot_b.matches, kind=plot_as, ranks=[1, 6, 35], plot_width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637b6d2-6271-48ed-9375-41b62c2874d9",
   "metadata": {},
   "source": [
    "The distributions of score contributions we just observed are the motivation for our approach to tag-weighted alignments, that are described in (Liebl and Burghardt, 2020). We demonstrate it now, by using a tag-weighted alignment that will weight nouns like \"madness\" and \"method\" 3 times more than other word types. Let's set it up (\"NN\" is a Penn Treebank tag and identifies singular nouns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84e07d-7741-4727-adf1-49064e88b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_weighted_index_builder = make_index_builder(\n",
    "    strategy=\"Tag-Weighted Alignment\",\n",
    "    strategy_options={\"tag_weights\": {\n",
    "        'NN': 3\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a161c3-8dc6-4f73-8426-8533d51a38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_results(gold, tag_weighted_index_builder.build_index(), \"though this be madness\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca966318-3d37-464c-a73d-fd3d153dcabb",
   "metadata": {},
   "source": [
    "This tag-weighting allows to fix move the correct results far to the top, namely to ranks 1, 2, 4 and 6.\n",
    "\n",
    "Note that we can bring rank 73 to rank 15 by increasing the NN weight to 5. But this is sort of an extreme measure and we will not follow it here.\n",
    "\n",
    "Instead we wonder: how will the weighting affect the other queries? Let's re-run the NDCG computation and compare it against unweighted WSB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12fe57-ceef-4450-acab-879e959931d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_unweighted = make_index_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef30a3-2beb-4a02-836a-e1b9a49ede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_ndcgs(gold, {\n",
    "    \"wsb_unweighted\": index_builder_unweighted.build_index(),\n",
    "    \"wsb_weighted\": tag_weighted_index_builder.build_index()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade169b2-5d09-4c9b-a689-ba914adce4c7",
   "metadata": {},
   "source": [
    "So we have shown that we considerably increased our accuracy through employing weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148bad50-844b-4712-ac81-ff286e2dc17a",
   "metadata": {},
   "source": [
    "# Interactive Searches with Your Own Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa98c8f-6119-4589-aaf4-7bb85473dea5",
   "metadata": {},
   "source": [
    "First specify the text documents you want to search through by an upload widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d07be-ff26-4a56-966a-38450fd29c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "upload = widgets.FileUpload(\n",
    "    accept='.txt',\n",
    "    multiple=True\n",
    ")\n",
    "\n",
    "upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd86e58-6973-42e4-9bcd-5baf565dbf22",
   "metadata": {},
   "source": [
    "From this upload widget contents, we now build a Vectorian session we can perform search through. As always with Vectorian session, we need to specify the embeddings the want to employ for searching. We also need an `nlp` instance for importing the text documents. Depending on the size and number of documents, this step can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452ab0d-6644-495d-ab75-31e9b744bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.importers import StringImporter\n",
    "from vectorian.session import LabSession\n",
    "\n",
    "import codecs\n",
    "\n",
    "\n",
    "def files_to_session(upload):\n",
    "    im = StringImporter(nlp)\n",
    "    \n",
    "    if not upload.value:\n",
    "        raise RuntimeError(\"cannot run on empty upload\")\n",
    "\n",
    "    docs = []\n",
    "    for k, data in upload.value.items():\n",
    "        docs.append(im(\n",
    "            codecs.decode(data[\"content\"], encoding=\"utf-8\"),\n",
    "            title=k,\n",
    "            unique_id=k))\n",
    "\n",
    "    return LabSession(\n",
    "        docs,\n",
    "        embeddings=[\n",
    "            emb_numberbatch,\n",
    "            emb_fasttext],\n",
    "        normalizers=\"default\")\n",
    "\n",
    "upload_session = files_to_session(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63e2a0-fa36-489d-aa7a-01718a6585ab",
   "metadata": {},
   "source": [
    "Now we present the full interactive search interface the Vectorian offers (we have hidden it so far and focussed on a subset). Note that in contrast to our experiments earlier, we do not search on the *document* level by default, but rather the *sentence* level - i.e. we split each document into sentences and then search on each sentence. You can change this in the \"Partition\" dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0469956-6e83-49a2-82ba-db7660f6a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_session.interact(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c05c8-31e3-4736-8312-687f48bfc884",
   "metadata": {},
   "source": [
    "# Literaturliste\n",
    "\n",
    "Pennington, Jeffrey, et al. “Glove: Global Vectors for Word Representation.” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, 2014, pp. 1532–43. DOI.org (Crossref), doi:10.3115/v1/D14-1162.\n",
    "\n",
    "Mikolov, Tomas, et al. “Advances in Pre-Training Distributed Word Representations.” ArXiv:1712.09405 [Cs], Dec. 2017. arXiv.org, http://arxiv.org/abs/1712.09405.\n",
    "\n",
    "Speer, Robyn, et al. “ConceptNet 5.5: An Open Multilingual Graph of General Knowledge.” ArXiv:1612.03975 [Cs], Dec. 2018. arXiv.org, http://arxiv.org/abs/1612.03975.\n",
    "\n",
    "Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” ArXiv:1908.10084 [Cs], Aug. 2019. arXiv.org, http://arxiv.org/abs/1908.10084.\n",
    "\n",
    "Liebl, Bernhard, and Manuel Burghardt. “‘Shakespeare in the Vectorian Age’ – An Evaluation of Different Word Embeddings and NLP Parameters for the Detection of Shakespeare Quotes.” Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, 2020, pp. 56–58."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
