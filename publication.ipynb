{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e2ae0-3099-4e08-9aa2-70a567b9163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorian\n",
    "import sys\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "sys.path.append(\"code\")\n",
    "import nbutils\n",
    "import gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702daa3-f241-431a-88de-e97c8cb8d49b",
   "metadata": {},
   "source": [
    "We first tell the notebook logic whether we have a full bokeh server. This *is* the case for local jupyter installations, but is *not* the case for notebooks running on mybinder - in the latter case we have some limits on interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6be29-90ac-4c33-a466-a1739dd4d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import importlib\n",
    "importlib.reload(nbutils)\n",
    "importlib.reload(gold)\n",
    "\n",
    "nbutils.initialize(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6288bc0-18c8-4bec-8300-382cbef61e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.io\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-texas",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b7cb2-2542-4573-ba41-ba27214c8e1f",
   "metadata": {},
   "source": [
    "In this notebook we will set forth the full stack of decisions that need to be taken in order to compute multi-token (i.e. sentence or document) similarities from embeddings. Not only will we work with different embeddings but also with different ways of leveraging these embeddings to compute phrase similarities. While our examples are following single paths of computation, we contextualize our decisions and allow interactive readers to take different decisions in terms of embeddings, algorithms and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77676262-9b46-430d-b575-636e8a50e20d",
   "metadata": {},
   "source": [
    "There are now various established ways to compute embeddings for similarity tasks. A first important distinction is between *token* embeddings and *document* embeddings (see diagram below) - note that we use the terms \"token embeddings\" and \"word embeddings\" interchangeably. While the former imply one embedding (i.e. numeric vector) per token, the latter operate by mapping a whole document (a set of tokens) into one single embedding.\n",
    "\n",
    "There are two common ways to compute document embeddings. One way is to derive them from token embeddings - for example by averaging over them. More complex approaches train dedicated models that are optimized to produce good document embeddings.\n",
    "\n",
    "So on this level, we can differentiate between three kinds of embeddings: pure token embeddings, document embeddings derived from token embeddings, and - finally - document embeddings from dedicated document embedding models (e.g. SBERT).\n",
    "\n",
    "![Different kinds of embeddings](miscellaneous/diagram_embeddings_1.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878e5ad-df86-4817-a047-650b396128a9",
   "metadata": {},
   "source": [
    "For token embeddings, there are also various options, as the diagram below illustrates. The most recent option are contextual token embeddings (also sometimes called *dynamic* embeddings), which will incorporate a specific token's context and can be obtained from architectures like ELMO or BERT. Another option are static token embeddings, which map one token to one embedding, independent of its specific occurence in a text. For an overview of static and contextual embeddings, and their differences, see (Wang et al. 2020).\n",
    "\n",
    "For static embeddings there is now a variety of established options like fastText or GloVe. We can also combine embeddings or stack them (i.e. concatenate embedding vectors) to simply create new embeddings from existing ones.\n",
    "\n",
    "![Different kinds of embeddings](miscellaneous/diagram_embeddings_2.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926628c-6adf-4c33-b472-f9f3fce7910d",
   "metadata": {},
   "source": [
    "In this notebook, we pick four classes of embeddings from the wide range of possibilities just described:\n",
    "\n",
    "* Static token embeddings: these operate on the token level such. We experiment with GloVe (Pennington et al. 2014), fastText (Mikolov et al., 2017) and Numberbatch (Speer et al, 2018). We use these three to compute token similarity and combine this with alignment algorithms (such as Waterman-Smith-Beyer) to compute document similarity. We also investigate the effect of stacking two static embeddings (fastText and Numberbatch).\n",
    "* Contextual token embeddings: these also operate on the token level, i.e. embeddings that change according to a specific token instance's context. In this notebook we experiment with using such token embeddings from a sentence bert model.\n",
    "* Document embeddings derived from specially trained models. Document embeddings represent one document via one single embedding. We use document embeddings obtained from a BERT model. More specifically, we use a Sentence-BERT model trained for the semantic textual similarity (STS) task (Reimers and Gurevych, 2019).\n",
    "* Document embeddings derived from token embeddings. We also experiment with averaging different kinds of token embeddings (static and contextual) to derive document embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-length",
   "metadata": {},
   "source": [
    "# The Gold Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-apache",
   "metadata": {},
   "source": [
    "First load our gold standard that contains our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data = gold.Data(\"data/raw_data/gold.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c3ca1-bf76-4401-80d6-497f311c0af7",
   "metadata": {},
   "source": [
    "Our gold standard consists of a number of `Pattern`s. Each `Pattern` is associated with a phrase, e.g. \"to be or not to be\", which occurs in a rephrased form in other works and contexts. These reoccurences, which model text reuse, are called `Occurrence`s in our data. Each such `Occurrence` carries the actual phrase and a larger context in which it occurs, which together we call the `Evidence`. The data layout for our gold standard looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743966bf-68df-42e8-b70d-2f9975ba0c34",
   "metadata": {},
   "source": [
    "![UML of gold standard data](miscellaneous/gold_uml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09a1ca-db83-4b16-946e-5277aa38e7db",
   "metadata": {},
   "source": [
    "One specific example in this data is the occurence of the pattern \"to be or not to be\" in \"The Phoenix\" as \"to be named or not be named\" (the latter is the `Evidence`, which consists of the actual phrase and a larger context):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e7d9f-4a35-4b02-bf01-00f37a0882af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.Browser(gold_data, \"to be or not to be\", \"The Phoenix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ccacb-0927-48f7-98cd-a9d63806bdcc",
   "metadata": {},
   "source": [
    "Below is a visualization of the full gold standard, with patterns indicated as blue circles and evidence indicated as green circles. Matching evidence and patterns are connected via edges and each bouqet consists of one pattern and the matching instances of text reuse. Interactive readers may want to hover the mouse over the nodes to see their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0a4e2-5640-43eb-8950-394da395f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_gold(gold_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16775582-294a-4c17-b10f-bf0d581f9441",
   "metadata": {},
   "source": [
    "# The Vectorian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c4c2b-7768-41e9-a20a-63e49c32700e",
   "metadata": {},
   "source": [
    "For performing our actual investigations we rely on a framework called The Vectorian, which we first introduced in 2020 (Liebl and Burghardt, 2020). By employing highly optimized algorithms and data structures, the Vectorian allows us to perform rapid searches over the gold standard texts using a variety of approaches and strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97def110-4079-4e59-a1a1-d3b881d4c8f9",
   "metadata": {},
   "source": [
    "In order to use the Vectorian, we need to map the gold standard concepts to Vectorian concepts as follows: the `context` texts in the gold standard `Evidence` items, i.e. the texts and contexts that contains an example of text reuse, are created in the Vectorian as `Document`s. A `Document` in Vectorian terminology is something we can perform a search on. `Document`s in the Vectorian are created using different kinds of `Importer`s that perform necessary natural language processing tasks using an additional `NLP` class (see diagram below). Since this step is also time-consuming, we precomputed this step and use the `Corpus` class to quickly load these preprocessed Documents into this notebook. For details about the full preprocessing, see `code/prepare_corpus.ipynb`. \n",
    "\n",
    "Using the loaded `Document`s and a set of `Embedding`s we want to work with, we can then create a `Session`, that allows us to perform searches. The specific steps we will take are (see top of diagram below):\n",
    "\n",
    "* create a `Partition` (which specifies how Documents should be split into searchable units, e.g. sentences)\n",
    "* create an `Index` for that partition (which specifies the strategy/algorithm we employ for searching)\n",
    "* perform a search on that `Index` (using a query text)\n",
    "* retrieve the `Result` and the `Match`es for that search\n",
    "\n",
    "Later, we will additionally create an `Index` with document embeddings by employing a `PartitionEncoder`, which allows us to specify an embedding that does not operate on the token level, but on the document level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f85364f-9f30-4371-aa97-c3fb3d1a5c24",
   "metadata": {},
   "source": [
    "![UML of important Vectorian classes](miscellaneous/vectorian_session.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612e3fa-35d0-4f80-81df-c2ab6227394b",
   "metadata": {},
   "source": [
    "## Loading Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635d3fb-43a0-45d0-9bad-e452b713104d",
   "metadata": {},
   "source": [
    "We load the static embeddings that were described above from Vectorian's model zoo. This zoo contains a number of prebuilt embeddings for various languages. Given below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c7ccd-2d57-426b-ba7a-cc52d735099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import Zoo\n",
    "Zoo.list()[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67bc2f0-272f-4af8-8de2-951a2aa2a34a",
   "metadata": {},
   "source": [
    "For reasons of limited RAM in the interactive Binder environment (and to limit download times), we use small or compressed versions of the static embeddings we work with:\n",
    "\n",
    "* for GloVe, we use the official 50-dimensional version of the 6B variant.\n",
    "* for fastText we use a version that was compressed using the standard settings in https://github.com/avidale/compress-fasttext.\n",
    "* for Numberbatch we use a 50-dimension version that was reduced using a standard PCA.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6edee-c756-4ef2-9801-00e0145ba65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_embeddings = {}\n",
    "\n",
    "the_embeddings['glove'] = Zoo.load('glove-6B-50')\n",
    "the_embeddings['numberbatch'] = Zoo.load('numberbatch-19.08-en-50')\n",
    "the_embeddings['fasttext'] = Zoo.load('fasttext-en-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9355b-c300-4690-a409-964065b4761e",
   "metadata": {},
   "source": [
    "We also use one stacked embedding, in which we combine fasttext and numberbatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11df296-c928-4a3a-8698-f00937f832e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import StackedEmbedding\n",
    "\n",
    "the_embeddings['fasttext_numberbatch'] = StackedEmbedding([\n",
    "    the_embeddings['fasttext'], the_embeddings['numberbatch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658905-b3e5-496d-9e62-828082dc354a",
   "metadata": {},
   "source": [
    "We instantiate an NLP parser that is able to provide embeddings based on Sentence-BERT (Reimers and Gurevych, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381332f6-02de-4763-a070-9208979b98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nbutils.make_nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c772ad8-af05-4074-9392-b263a5d6b358",
   "metadata": {},
   "source": [
    "Finally, we add a shim that allows us to use Sentence-BERT's contextual token embeddings in the Vectorian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e565e6f-cd21-4b38-9421-78a097608af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import SentenceBertEmbedding\n",
    "\n",
    "the_embeddings['sbert'] = SentenceBertEmbedding(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80e65e-eb0b-40d2-a442-d573fcdcb417",
   "metadata": {},
   "source": [
    "## Creating the Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d584b-18e4-4d1e-ac37-ccbe065549be",
   "metadata": {},
   "source": [
    "The Vectorian `Session` is created with the specified embeddings and the preprocessed documents, which are loaded via the `Corpus` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.session import LabSession\n",
    "from vectorian.corpus import Corpus\n",
    "\n",
    "session = LabSession(\n",
    "    Corpus.load(\"data/processed_data/corpus\"),\n",
    "    embeddings=the_embeddings.values(),\n",
    "    normalizers=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-canal",
   "metadata": {},
   "source": [
    "The session now contains all embeddings we will work with as well as the list of documents that contain the texts from the gold standard `Evidence` items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2bcfd-c597-44c3-81bc-34cbab10bdef",
   "metadata": {},
   "source": [
    "## Word Embeddings and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7b8be-f3b6-4604-b924-7a453ddc89d0",
   "metadata": {},
   "source": [
    "We now take a short look at what constitutes word embeddings. A word embedding is a vector **x** of dimension $n$, i.e. a vector consisting of $n$ scalars.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{x}=(x_1, x_2, ..., x_{n-1}, x_n)\n",
    "\\end{equation*}\n",
    "\n",
    "For example, the compressed numberbatch embedding we use has $n=50$ and thus represents the word \"coffee\" with the following 50 numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2633d27-0863-4958-bf06-036b8af37604",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.GridBox(\n",
    "    [widgets.Label(f\"{x:.2f}\") for x in session.word_vec(the_embeddings[\"numberbatch\"], \"coffee\")],\n",
    "    layout=widgets.Layout(grid_template_columns=\"repeat(10, 50px)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f35262-219c-4764-9568-e2e25f042c45",
   "metadata": {},
   "source": [
    "Since the above representation is hard to grasp, we visualize the values of\n",
    "\n",
    "\\begin{equation*}x_1, x_2, ..., x_{n-1}, x_n\\end{equation*}\n",
    "\n",
    "through different colors (optionally normalizing by $\\mathbf{||x||_2}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344867c-e095-4d6d-baf5-1fa559b37f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(embedding=widgets.Dropdown(\n",
    "    options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "    value=the_embeddings[\"numberbatch\"]), normalize=False)\n",
    "def plot(embedding, normalize):\n",
    "    nbutils.plot_embedding_vectors_val(\n",
    "        [\"sail\", \"boat\", \"coffee\", \"tea\", \"guitar\", \"piano\"],\n",
    "        get_vec=lambda w: session.word_vec(embedding, w),\n",
    "        normalize=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa064d-65d8-49e1-9787-5176d36542d1",
   "metadata": {},
   "source": [
    "Looking at these color patterns, we can gain some intuitive understanding of why and how word embeddings are suitable for word similarity computations. For example, *sail* and *boat* both show a strong activation on dimension 27. Similarly, *guitar* and *piano* share similar values around dimension 24. The words *coffee* and *tea* also share some similar patterns around dimension 2 and dimension 49, that sets them apart from the other four words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f230cc-a265-4a5c-854e-0a30fe86c0f3",
   "metadata": {},
   "source": [
    "A common approach to compute the similarity between two word vectors **u** and **v** in this kind of high-dimensional vector spaces is to compute the cosine of the angle $\\theta$ between the vectors, which is called cosine similarity:\n",
    "\n",
    "\\begin{equation*}\n",
    "cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{||\\mathbf{u}||_2 ||\\mathbf{v}||_2} = \\frac{\\sum_1^n \\mathbf{u}_i \\mathbf{v}_i}{\\sqrt{\\sum_1^n \\mathbf{u}_i^2} \\sqrt{\\sum_1^n \\mathbf{v}_i^2}} = \\sum_1^n \\left( \\frac{\\mathbf{u}}{||\\mathbf{u}||_2} \\right)_i \\left( \\frac{\\mathbf{v}}{||\\mathbf{v}||_2} \\right)_i \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a591ebe-dc7d-4109-a2f5-0480593057ca",
   "metadata": {},
   "source": [
    "A large positive value (i.e. a small $\\theta$ between **u** and **v**) indicates high similarity, whereas a small or even negative value (i.e. a large $\\theta$) indicates low similarity. For a discussion of issues with this notion of similarity, see (Faruqui et al., 2016).\n",
    "\n",
    "The visualization below encodes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \\frac{\\mathbf{u}}{||\\mathbf{u}||_2} \\right)_i \\left( \\frac{\\mathbf{v}}{||\\mathbf{v}||_2} \\right)_i \n",
    "\\end{equation*}\n",
    "\n",
    "for different $i, 1 \\le i \\le n$ through colors to illustrate how different components contribute to the cosine similarity for two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7436d6-bf5a-4e2d-a4b0-0174c0606cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(embedding=widgets.Dropdown(\n",
    "    options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "    value=the_embeddings[\"numberbatch\"]))\n",
    "def plot(embedding):\n",
    "    nbutils.plot_embedding_vectors_mul([\n",
    "        (\"sail\", \"boat\"),\n",
    "        (\"coffee\", \"tea\"),\n",
    "        (\"guitar\", \"piano\")], get_vec=lambda w: session.word_vec(embedding, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb908081-598a-491c-b070-439989691705",
   "metadata": {},
   "source": [
    "A similar investigation into fastText shows similar spots of positive contribution. The situation is more complex due to the higher number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b603839-2495-411d-b9e6-e1d133277f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(embedding=widgets.Dropdown(\n",
    "    options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "    value=the_embeddings[\"fasttext\"]))\n",
    "def plot(embedding):\n",
    "    nbutils.plot_embedding_vectors_mul([\n",
    "        (\"sail\", \"boat\"),\n",
    "        (\"coffee\", \"tea\"),\n",
    "        (\"guitar\", \"piano\")], get_vec=lambda w: session.word_vec(embedding, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4eb6-33c7-4efe-9ea5-5ddf20316bd1",
   "metadata": {},
   "source": [
    "As shown above, computing the cosine similarity is mathematically equivalent to summing up the terms in the diagram above. The overall similarity between *guitar* and *piano* is measured at about 68% with the fastText embedding we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95ce2-cdc8-44dc-a69e-06c3c1feb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.metrics import TokenSimilarity, CosineSimilarity\n",
    "\n",
    "token_sim = TokenSimilarity(\n",
    "    the_embeddings[\"fasttext\"],\n",
    "    CosineSimilarity()\n",
    ")\n",
    "\n",
    "session.similarity(token_sim, \"guitar\", \"piano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe41d8-ef30-41aa-8513-b7fe88affa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mik(words):\n",
    "    from sklearn.decomposition import PCA\n",
    "    import bokeh.models\n",
    "    import bokeh.plotting\n",
    "    import bokeh.io\n",
    "    \n",
    "    vectors = [session.word_vec(emb_fasttext, word) for word in words]\n",
    "\n",
    "    pca = PCA(n_components=2, whiten=True)\n",
    "    v2d = pca.fit(vectors).transform(vectors)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource({\n",
    "        'x': v2d[:, 0],\n",
    "        'y': v2d[:, 1]\n",
    "    })\n",
    "    \n",
    "\n",
    "    p = bokeh.plotting.figure(\n",
    "        plot_width=800,\n",
    "        plot_height=400,\n",
    "        title=\"\",\n",
    "        toolbar_location=None, tools=\"\")\n",
    "    \n",
    "    p.circle(source=source, size=10)\n",
    "    \n",
    "    for i in range(0, len(words), 2):\n",
    "        p.add_layout(bokeh.models.Arrow(end=bokeh.models.NormalHead(line_color=\"black\", line_width=1),\n",
    "            x_start=v2d[i, 0], y_start=v2d[i, 1], x_end=v2d[i + 1, 0], y_end=v2d[i + 1, 1]))\n",
    "    \n",
    "    bokeh.io.show(p)\n",
    "    \n",
    "    \n",
    "plot_mik([\"man\", \"woman\", \"king\", \"queen\", \"prince\", \"princess\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96153b53-e9c4-4201-a55d-3d90e7d4f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sim = TokenSimilarity(\n",
    "    the_embeddings[\"sbert\"],\n",
    "    CosineSimilarity())\n",
    "\n",
    "a = list(session.documents[0].spans(session.partition(\"document\")))[0][3]\n",
    "b = list(session.documents[3].spans(session.partition(\"document\")))[0][2]\n",
    "session.similarity(token_sim, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c4165-4cd5-4083-92f7-ba28c34e2af3",
   "metadata": {},
   "source": [
    "# Word Embeddings for the Specific Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02093eb-ec75-4c21-9230-4bb3f1dd2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = nbutils.TokenSimVis(session, nlp, gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942597d-d5db-48e1-b02f-bb1f8084cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.goto(\"rest is silence\", \"Fig for Fortune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bc950-6e6d-4585-b98f-688484df28e9",
   "metadata": {},
   "source": [
    "The example above is a situation where token similarity - and therefore embeddings - might not help much. While the syntactic structure is mirrored, the term \"silence\" is replaced with \"all but wind\". Even if we focus on nouns only, we would not expect \"silence\" and \"wind\" to be understood to be similar. Still an embedding approach should be able to recognize that the  words at the beginning of phrase are exact matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa324fc-042b-474c-9ac2-6c28e61d4a41",
   "metadata": {},
   "source": [
    "If we inspect the cosine similarity of the token \"silence\" with other tokens in the context under three of our embeddings, we see that there is more connection between \"silence\" and \"wind\" than we expected - esp. with numberbatch. Still, the absolute value of 0.3 for numberbatch is low. Interestingly, glove associates \"silence\" with \"action\", i.e. an opposite. The phenomenon that embeddings sometimes cluster opposites is a common observation and can be an issue when wanting to differentiate between synonyms and antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65bcc2-678a-4cdb-8a04-99df700c33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot(\"silence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41a31d-d7c0-4bb7-8fc7-adb824a7fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.goto(\"sea of troubles\", \"Book of Common Prayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfacd0-75f9-4cb5-a7a6-927b4e82ca42",
   "metadata": {},
   "source": [
    "This is a different situation, where similarity computation might help. Here, \"sea\" is replaced by \"waves\", and \"troubles\" by \"troublesome\". We should expect to get reasonable results with results on this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c58bda-6bbc-49b9-8f91-01ec5ea660d4",
   "metadata": {},
   "source": [
    "Indeed, by inspecting the cosine similarity of the token \"sea\" with other tokens in the context, we see that this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae847b0b-2ea7-4e97-994f-405d32e3bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot(\"sea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2c55b-2194-436a-94f8-dde47505d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot(\"troubles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97c62-4bef-48f6-8c08-83b585afd4ca",
   "metadata": {},
   "source": [
    "Note how out-of-vocabulary words like \"troublesomest\" will produce zero similarities under standard key-value embeddings, whereas fastText is still able to produce a vector thanks to subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddff82-b84e-4289-a409-debc26d87287",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot(\"troublesomest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844c0f3-4256-48a2-aac9-5ff65c6c8e6c",
   "metadata": {},
   "source": [
    "# Exploring Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-aggregate",
   "metadata": {},
   "source": [
    "Before we turn to alignment strategies to match sentences token by token, we first look at representing each document with one single embedding in order to gather an understanding how different embedding strategies relate to the nearness of documents. We will later return to individual token embeddings.\n",
    "\n",
    "We will use two strategies for computing document embeddings:\n",
    "\n",
    "* averaging over token embeddings\n",
    "* computing document embeddings through a dedicated model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-controversy",
   "metadata": {},
   "source": [
    "In order to achieve the latter, we compute document embeddings using Sentence-BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import CachedPartitionEncoder, SpanEncoder\n",
    "\n",
    "sbert_encoder = CachedPartitionEncoder(SpanEncoder(\n",
    "    lambda texts: [nlp(t).vector for t in texts]))\n",
    "\n",
    "sbert_encoder.try_load(\"data/processed_data/doc_embeddings\")\n",
    "sbert_encoder.cache(session.documents, session.partition(\"document\"))\n",
    "sbert_encoder.save(\"data/processed_data/doc_embeddings\")\n",
    "\n",
    "sbert_encoder_name = nlp.meta[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d93851-2895-422d-92d0-586aa540d480",
   "metadata": {},
   "source": [
    "In order to achieve the former, we configure a helper class instance to use averaging to build documents embeddings from token embeddings.\n",
    "\n",
    "Interactive readers may want to change the \"mean\" (averaging) method to other methods for computing document tokens as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fb8cc-5328-483c-becd-9a86fb091192",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer = nbutils.DocEmbeddingExplorer(\n",
    "    session=session, nlp=nlp, gold=gold_data, extra_encoders={sbert_encoder_name: sbert_encoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271e325-7408-4ff1-a274-89f34251ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(embedding=widgets.Dropdown(\n",
    "    options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "    value=the_embeddings[\"numberbatch\"]), normalize=False)\n",
    "def plot(embedding, normalize):\n",
    "    nbutils.plot_embedding_vectors_val(\n",
    "        [\"sail\", \"boat\", \"coffee\", \"tea\", \"guitar\", \"piano\"],\n",
    "        get_vec=lambda w: session.word_vec(embedding, w),\n",
    "        normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ecd2c-91c3-4dd5-af4a-ac578d2a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer.plot([\n",
    "    {\"encoder\": \"paraphrase_distilroberta\", \"locator\": (\"fixed\", \"carry coals\"), 'has_tok_emb': False},\n",
    "    {\"encoder\": \"paraphrase_distilroberta\", \"locator\": (\"fixed\", \"an old man is twice\"), 'has_tok_emb': False}\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fbac7-6a37-411a-9611-42022952a250",
   "metadata": {},
   "source": [
    "In the TSNE visualization above, dots are documents and the colors are the query that yields that document in our gold standard. By hovering over dots with the mouse you get details on the document and query the dot represents. Nearby dots of the same color indicate that the embedding tends to cluster documents similar to our gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bcebe-bb6c-467c-a147-d7588e28917b",
   "metadata": {},
   "source": [
    "On the left above, we see that the phrase \"we will not carry coals\" (large green-yellow circle with cross) in located closely to the documents associated with that query (smaller green-yellow circles). Similarly, on the right we see that the phrase \"an old man is twice a child\" clusters with the actual (green) documents we associate with it in our gold standard.\n",
    "\n",
    "For these phrases and documents, the `paraphrase_distilroberta` model does a good job of producing a document embedding that actually separates inherent topics (without us telling it to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer.plot([\n",
    "    {\"encoder\": \"numberbatch\", \"selection\": [\n",
    "        'ww_32c26a7909c83bda',\n",
    "        'ww_b5b8083a6a1282bc',\n",
    "        'ww_9a6cb20b0b157545',\n",
    "        'ww_a6f4b0e3428ad510',\n",
    "        'ww_8e68a517bc3ecceb']\n",
    "    }\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93e356-ec78-442b-b445-d9f5b6c2ed25",
   "metadata": {},
   "source": [
    "In the plot above we look at the document embedding produced by a **token-based** embedding. This has the advantage that we can actually look at token embeddings that make up the document embedding (through averaging). On the right side, we see a TSNE plot of all token embeddings that occur in the documents that are selected on the left. The hope is that this visualization will give us a clue why the documents on the left might be clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564ef2d-c165-44ed-a930-f85b9cd7f3d2",
   "metadata": {},
   "source": [
    "The red circles on the left represent contexts that match the phrase \"a horse, a horse, my kingform for a horse\" are mapped. If we look at the token embeddings (that includes documents from other other classes), we indeed see that a grouping happens due to word embeddings clustering around \"horse\" (right side), but we also see a cluster around \"boat\", \"sail\" and \"river\" on the left.\n",
    "\n",
    "In fact context 1 contains \"muscle boat\", context 2 contains \"To swim the river villain\", and context 3 contains \"A boat, a boat\". We thereby see that this kind of unsupervised document clustering clusters items due to inherent qualities that might not actually match our query criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a554d4e-9eb9-49f9-900e-ab554afd2757",
   "metadata": {},
   "source": [
    "Interactive note: you can compute different token embeddings plots by selecting different documents on the mouse (drag the mouse to lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a490bb-2a3c-4ae2-a687-2b74d60d4eb7",
   "metadata": {},
   "source": [
    "# Understanding Alignments (WSB vs WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f8934-e492-4244-8acf-2795f3b26172",
   "metadata": {},
   "source": [
    "Various approaches have been proposed. For an overview of sequence alignment algorithms as well as adjacent approaches like Dynamic Time Warping, see Kruskal (Kruskal, 1983). In this section, we use the Waterman-Smith-Beyer (WSB) algorithm which produces optimal local alignments and allows a general (e.g. non-affine) cost function (Waterman and Smith and Beyer, 1974). Other commonly used global alignment algorithms - such as Smith-Waterman and Gotoh - can be regarded as special cases of WSB. In comparison to Needleman-Wunsch, WSB produces local alignments. In contrast to classic formulations of WSB - which often use a fixed substitution cost - we use the word distance from word embeddings to compute the substitution penalty for specific pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da85ca-b472-4e59-846d-0c120454807a",
   "metadata": {},
   "source": [
    "A different approach to compute a measure of similarity between bag of words is the so-called Word Mover's Distance introduced by Kusner et al. (Kusner et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-reverse",
   "metadata": {},
   "source": [
    "## A Search Query using Alignment over Similar Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aea237-1fe6-4903-89c9-328d7f9790c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_builder(**kwargs):\n",
    "    return nbutils.InteractiveIndexBuilder(session, nlp, partition_encoders={\n",
    "        sbert_encoder_name: sbert_encoder\n",
    "    }, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder = make_index_builder()\n",
    "index_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2ff05-1807-4343-a819-9acb55328ccc",
   "metadata": {},
   "source": [
    "What you see above is the description of a search strategy that we will employ in the following sections of this notebook. Interactive readers can switch to the \"Edit\" part and actually explore the setting in more detail and even change it to something completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data.patterns[0].phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder.build_index().find(gold_data.patterns[0].phrase, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-helen",
   "metadata": {},
   "source": [
    "## Plotting the NDCG over the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6423e26-7035-4b1a-8f58-28634c95ef62",
   "metadata": {},
   "source": [
    "We first define a strategy for searching the corpus. In the summary below you will find the strategy used for the non-interactive version of this text. In the interactive version, you can click on \"Edit\" and change these settings and rerun the following sections of the notebook accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413a21e-c6cf-4f9d-8da1-3717281da294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import ipywidgets as widgets\n",
    "\n",
    "index_builders = collections.OrderedDict({\n",
    "    \"wsb\": make_index_builder(\n",
    "        strategy=\"Alignment\",\n",
    "        strategy_options={\"alignment\": vectorian.alignment.WatermanSmithBeyer()}),\n",
    "    \"wmd nbow\": make_index_builder(\n",
    "        strategy=\"Alignment\",\n",
    "        strategy_options={\"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"nbow\")}),\n",
    "    \"wmd bow\": make_index_builder(\n",
    "        strategy=\"Alignment\",\n",
    "        strategy_options={\"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"bow\")}),\n",
    "    \"doc embedding\": make_index_builder(\n",
    "        strategy=\"Partition Embedding\")\n",
    "})\n",
    "\n",
    "accordion = widgets.Accordion(children=[x.displayable for x in index_builders.values()])\n",
    "for i, k in enumerate(index_builders.keys()):\n",
    "    accordion.set_title(i, k)\n",
    "accordion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e816e4e-24a1-4ac0-bd0f-968dc22247da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorian.alignment.WordMoversDistance.wmd(\"nbow\").to_args(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0bd8e-6694-42c7-8dc6-3b01bff840ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorian.alignment.WordMoversDistance.wmd(\"bow\").to_args(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b98cf7-3e58-4699-a2e1-52db2996efe1",
   "metadata": {},
   "source": [
    "Now get an overview of the quality of the results we obtain when using the index configures with `index_builder` by computing the NDCG over all queries in our gold standard with regards to the known optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_ndcgs(gold_data, dict((k, v.build_index()) for k, v in index_builders.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4d4fd-6fda-48cc-b494-ede28ac06d3f",
   "metadata": {},
   "source": [
    "We see that some queries obtain 100%, i.e. the top results match the optimal ones given in our gold standard. We see that Waterman-Smith-Beyer (WSB) tends to perform a tad better than Word Mover's Distance (WMD), with the exception of \"though this be madness...\", where WMD outperforms WSB. In general the Vectorian modification of WMD, which does not use nbow, performs better than Kusner's original description of WMD. The one exception here is \"livers white as milk.\"\n",
    "\n",
    "One advantage of WSB over the full WMD variants is its easy interpretability. WSB allows us to understand as a bijective  mapping between tokens, namely a subset of the query and a subset of the document. For WMD, this assumption of bijection often breaks down. We use this character of WSB in the following section to illustrate which mappings actually occur.\n",
    "\n",
    "Let's look at some queries, where the performance for WSB is bad, and try to understand why our search fails to obtain the optimal results at the top of the result list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-endorsement",
   "metadata": {},
   "source": [
    "## Focussing on single queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235906d-7b57-4f5f-a446-67cee9a722a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder = make_index_builder()\n",
    "index_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5c430-0b9b-4ac3-86fd-eb01dd623757",
   "metadata": {},
   "source": [
    "For this, we turn to the query with the lowest score \"though this be madness, yet there is a method in it\", and look at its results in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a = nbutils.plot_results(gold_data, index_builder.build_index(), \"though this be madness\", rank=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c796b4-7ab3-4671-9f30-fc9801cfdfbf",
   "metadata": {},
   "source": [
    "The best match obtained here - on rank 6 - is anchored on two word matches, namely `madness` (a 100% match) and `methods` (a 72% match). The other words are quite different and there is no good alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d210159-ed28-4412-99ad-47956bb0c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_b = nbutils.plot_results(gold_data, index_builder.build_index(), \"though this be madness\", rank=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5d7f1-3c98-4a90-98ab-7896be9692c1",
   "metadata": {},
   "source": [
    "Above we see the rank 3 result from the same query, which is a false positive - i.e. our search proclaims it is a better result than the one we saw before, but in fact this result is not relevant according to our gold standard. If we analyze why this result gets such a high score nonetheless, we see that \"is\" and \"in\" both contribute big 100% scores. In contrast to the scores before, 100% for \"madness\" and 72% for \"methods\", this partially explains the higher overall score (if we assume for now that the contributions from the other tokens are somewhat similar).\n",
    "\n",
    "Let us try to understand why the \"correct\" (true positive) results are ranked rather low and what we could do about it by looking at the composition of scores for each result we obtain for this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da85ba-0a96-477e-b963-ac2ec97c9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.vis_token_scores(plot_b.matches[:50], highlight={\n",
    "    \"token\": [\"madness\", \"method\"],\n",
    "    \"rank\": [6, 20, 35, 45]\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d9b9a-39ab-4fd0-9e86-a5005d1b5538",
   "metadata": {},
   "source": [
    "The relevant (true positive) results are marked with black triangles. We see that our current search strategy isn't doing a very good job of ranking them highly.\n",
    "\n",
    "Looking at the score composition of the relevant results, we can make out two distinct features: all relevant results show a rather large contribution of either \"madness\" (look at rang 6 and rank 35, for example) and/or a rather large contribution of \"method\" (ranks 45 and 6).\n",
    "\n",
    "However, these contributions do not lead to higher ranks necessarily, since other words such as \"is\", \"this\" and \"though\" score higher for other results: for example, look at the contribution of \"in\" for rank 1.\n",
    "\n",
    "In the next plot below, we visualize this observation using ranks 1, 6 and 35. Compare the rank 1 result on the left - which is a false positive - with the two relevant results on the right. Again, we see that \"in\", \"through\" and \"is\" make up large parts of the score for rank 1, whereas \"madness\" is a considerable factor for the two relevant matches. Unfortunately, this contribution is not sufficient to bring these results to higher ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa2956-8ccb-4208-ace2-b43d4b6098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(plot_as=widgets.ToggleButtons(options=['bar', 'pie'], value='pie'))\n",
    "def plot(plot_as):\n",
    "    nbutils.vis_token_scores(plot_b.matches, kind=plot_as, ranks=[1, 6, 35], plot_width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637b6d2-6271-48ed-9375-41b62c2874d9",
   "metadata": {},
   "source": [
    "The distributions of score contributions we just observed are the motivation for our approach to tag-weighted alignments, that are described in (Liebl and Burghardt, 2020). We demonstrate it now, by using a tag-weighted alignment that will weight nouns like \"madness\" and \"method\" 3 times more than other word types. Let's set it up (\"NN\" is a Penn Treebank tag and identifies singular nouns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84e07d-7741-4727-adf1-49064e88b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_weighted_index_builder = make_index_builder(\n",
    "    strategy=\"Tag-Weighted Alignment\",\n",
    "    strategy_options={\"tag_weights\": {\n",
    "        'NN': 3\n",
    "    }})\n",
    "tag_weighted_index_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a161c3-8dc6-4f73-8426-8533d51a38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_results(gold_data, tag_weighted_index_builder.build_index(), \"though this be madness\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca966318-3d37-464c-a73d-fd3d153dcabb",
   "metadata": {},
   "source": [
    "This tag-weighting allows to fix move the correct results far to the top, namely to ranks 1, 2, 4 and 6.\n",
    "\n",
    "Note that we can bring rank 73 to rank 15 by increasing the NN weight to 5. But this is sort of an extreme measure and we will not follow it here.\n",
    "\n",
    "Instead we wonder: how will the weighting affect the other queries? Let's re-run the NDCG computation and compare it against unweighted WSB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12fe57-ceef-4450-acab-879e959931d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_unweighted = make_index_builder()\n",
    "index_builder_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef30a3-2beb-4a02-836a-e1b9a49ede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_ndcgs(gold_data, {\n",
    "    \"wsb_unweighted\": index_builder_unweighted.build_index(),\n",
    "    \"wsb_weighted\": tag_weighted_index_builder.build_index()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade169b2-5d09-4c9b-a689-ba914adce4c7",
   "metadata": {},
   "source": [
    "So we have shown that we considerably increased our accuracy through employing weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ea111-9c4a-4ec1-8dee-6a5b1a5ef619",
   "metadata": {},
   "source": [
    "# The Influence of Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904f06f-119e-45ef-86c3-1f2e174fdd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builders = {}\n",
    "\n",
    "for e in the_embeddings.values():\n",
    "    index_builders[e.name] = make_index_builder(\n",
    "        strategy=\"Tag-Weighted Alignment\",\n",
    "        strategy_options={\n",
    "            \"tag_weights\": {\n",
    "                'NN': 3\n",
    "            },\n",
    "            \"similarity\": {\n",
    "                \"embedding\": e\n",
    "            }\n",
    "        })\n",
    "    \n",
    "accordion = widgets.Accordion(children=[x.displayable for x in index_builders.values()])\n",
    "for i, k in enumerate(index_builders.keys()):\n",
    "    accordion.set_title(i, k)\n",
    "accordion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115ddd1-7f87-4d02-9aff-7629ad7a6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_ndcgs(gold_data, dict((k, v.build_index()) for k, v in index_builders.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8e383-f589-48f1-af61-f360e221ae83",
   "metadata": {},
   "source": [
    "The caveat here is that we are using compressed embeddings, i.e. we would need to verify these results with uncompressed embeddings. Still, the performance of compressed fasttext seems very solid.\n",
    "\n",
    "In a few queries (\"llo, ho, ho my lord\", \"frailty, thy name is woman\", \"hell itself should gape\"), GloVe gives slightly better results than fastText, but this does not generalize to the overall performance.\n",
    "\n",
    "For some queries (\"I do bear a brain.\", \"O all you host of heaven!\") the embedding does not matter at all.\n",
    "\n",
    "A real competitor to fastText are contextual embedding from Sentence-BERT - however, these are much more expensive in terms of computation time, storage space and code complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148bad50-844b-4712-ac81-ff286e2dc17a",
   "metadata": {},
   "source": [
    "# Interactive Searches with Your Own Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa98c8f-6119-4589-aaf4-7bb85473dea5",
   "metadata": {},
   "source": [
    "First specify the text documents you want to search through by an upload widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d07be-ff26-4a56-966a-38450fd29c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "upload = widgets.FileUpload(\n",
    "    accept='.txt',\n",
    "    multiple=True\n",
    ")\n",
    "\n",
    "upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd86e58-6973-42e4-9bcd-5baf565dbf22",
   "metadata": {},
   "source": [
    "From this upload widget contents, we now build a Vectorian session we can perform search through. As always with Vectorian session, we need to specify the embeddings the want to employ for searching. We also need an `nlp` instance for importing the text documents. Depending on the size and number of documents, this step can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452ab0d-6644-495d-ab75-31e9b744bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.importers import StringImporter\n",
    "from vectorian.session import LabSession\n",
    "\n",
    "import codecs\n",
    "\n",
    "\n",
    "def files_to_session(upload):\n",
    "    im = StringImporter(nlp)\n",
    "    \n",
    "    if not upload.value:\n",
    "        raise RuntimeError(\"cannot run on empty upload\")\n",
    "\n",
    "    docs = []\n",
    "    for k, data in upload.value.items():\n",
    "        docs.append(im(\n",
    "            codecs.decode(data[\"content\"], encoding=\"utf-8\"),\n",
    "            title=k,\n",
    "            unique_id=k))\n",
    "\n",
    "    return LabSession(\n",
    "        docs,\n",
    "        embeddings=[\n",
    "            emb_numberbatch,\n",
    "            emb_fasttext],\n",
    "        normalizers=\"default\")\n",
    "\n",
    "upload_session = files_to_session(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63e2a0-fa36-489d-aa7a-01718a6585ab",
   "metadata": {},
   "source": [
    "Now we present the full interactive search interface the Vectorian offers (we have hidden it so far and focussed on a subset). Note that in contrast to our experiments earlier, we do not search on the *document* level by default, but rather the *sentence* level - i.e. we split each document into sentences and then search on each sentence. You can change this in the \"Partition\" dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0469956-6e83-49a2-82ba-db7660f6a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_session.interact(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c05c8-31e3-4736-8312-687f48bfc884",
   "metadata": {},
   "source": [
    "# Literaturliste\n",
    "\n",
    "Pennington, Jeffrey, et al. “Glove: Global Vectors for Word Representation.” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, 2014, pp. 1532–43. DOI.org (Crossref), doi:10.3115/v1/D14-1162.\n",
    "\n",
    "Mikolov, Tomas, et al. “Advances in Pre-Training Distributed Word Representations.” ArXiv:1712.09405 [Cs], Dec. 2017. arXiv.org, http://arxiv.org/abs/1712.09405.\n",
    "\n",
    "Speer, Robyn, et al. “ConceptNet 5.5: An Open Multilingual Graph of General Knowledge.” ArXiv:1612.03975 [Cs], Dec. 2018. arXiv.org, http://arxiv.org/abs/1612.03975.\n",
    "\n",
    "Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” ArXiv:1908.10084 [Cs], Aug. 2019. arXiv.org, http://arxiv.org/abs/1908.10084.\n",
    "\n",
    "Liebl, Bernhard, and Manuel Burghardt. “‘Shakespeare in the Vectorian Age’ – An Evaluation of Different Word Embeddings and NLP Parameters for the Detection of Shakespeare Quotes.” Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, 2020, pp. 56–58.\n",
    "\n",
    "Kruskal, Joseph B. “An Overview of Sequence Comparison: Time Warps, String Edits, and Macromolecules.” SIAM Review, vol. 25, no. 2, Apr. 1983, pp. 201–37. DOI.org (Crossref), doi:10.1137/1025045.\n",
    "\n",
    "Kusner, Matt J., et al. “From Word Embeddings to Document Distances.” Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, JMLR.org, 2015, pp. 957–66.\n",
    "\n",
    "Waterman, M. S., et al. “Some Biological Sequence Metrics.” Advances in Mathematics, vol. 20, no. 3, June 1976, pp. 367–87. DOI.org (Crossref), doi:10.1016/0001-8708(76)90202-4.\n",
    "\n",
    "Faruqui, Manaal, et al. “Problems With Evaluation of Word Embeddings Using Word Similarity Tasks.” ArXiv:1605.02276 [Cs], May 2016. arXiv.org, http://arxiv.org/abs/1605.02276.\n",
    "\n",
    "Wang, Yuxuan, et al. “From Static to Dynamic Word Representations: A Survey.” International Journal of Machine Learning and Cybernetics, vol. 11, no. 7, July 2020, pp. 1611–30. DOI.org (Crossref), doi:10.1007/s13042-020-01069-8.\n",
    "\n",
    "Nagoudi, El Moatez Billah, and Didier Schwab. “Semantic Similarity of Arabic Sentences with Word Embeddings.” Proceedings of the Third Arabic Natural Language Processing Workshop, Association for Computational Linguistics, 2017, pp. 18–24. DOI.org (Crossref), doi:10.18653/v1/W17-1303."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661a34b-ff96-46b5-afb1-c351082756bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
