{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e2ae0-3099-4e08-9aa2-70a567b9163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"code\")\n",
    "import nbutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702daa3-f241-431a-88de-e97c8cb8d49b",
   "metadata": {},
   "source": [
    "We first tell the notebook logic whether we have a full bokeh server. This *is* the case for local jupyter installations, but is *not* the case for notebooks running on mybinder - in the latter case we have some limits on interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6be29-90ac-4c33-a466-a1739dd4d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nbutils.initialize(has_bokeh_server=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-texas",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b7cb2-2542-4573-ba41-ba27214c8e1f",
   "metadata": {},
   "source": [
    "Let's first get a high level overview of what we are aiming to do technically. We will experiment with four classes of embeddings (see the diagram below for a classification):\n",
    "\n",
    "* Static token embeddings: these operate on the token level such. We experiment with GloVe (Pennington et al. 2014), fastText (Mikolov et al., 2017) and Numberbatch (Speer et al, 2018). We use these three to compute token similarity and combine this with alignment algorithms (such as Waterman-Smith-Beyer) to compute document similarity. We also investigate the effect of stacking two static embeddings (fastText and Numberbatch).\n",
    "* Contextual token embeddings: these also operate on the token level, i.e. embeddings that change according to a specific token instance's context. In this notebook we experiment with using such token embeddings from a sentence bert model.\n",
    "* Document embeddings derived from specially trained models. Document embeddings represent one document via one single embedding. We use document embeddings obtained from a BERT model. More specifically, we use a Sentence-BERT model trained for the semantic textual similarity (STS) task (Reimers and Gurevych, 2019).\n",
    "* Document embeddings derived from token embeddings. We also experiment with averaging different kinds of token embeddings (static and contextual) to derive document embeddings.\n",
    "\n",
    "![Different kinds of embeddings](miscellaneous/diagram_embeddings.svg)\n",
    "\n",
    "For reasons of limited RAM and download times, we use small or compressed versions of the static embeddings we work with. For GloVe, we use the official 50-dimensional version of the 6B variant. For fastText we use a version that was compressed using the standard settings in https://github.com/avidale/compress-fasttext. For Numberbatch we use a 50-dimension version that was reduced using a standard PCA.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-guidance",
   "metadata": {},
   "source": [
    "# Technical Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.io\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-capital",
   "metadata": {},
   "source": [
    "# Choosing Static Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-professional",
   "metadata": {},
   "source": [
    "First we need:\n",
    "    \n",
    "    * a set of documents of search over (i.e. our corpus)\n",
    "    * a set of word embeddings to employ for these searches\n",
    "    \n",
    "For the latter, we turn to Vectorian's embedding zoo, which offers a number of pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import Zoo\n",
    "\n",
    "#Zoo.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-burden",
   "metadata": {},
   "source": [
    "Let's load the static embeddings as described above from Vectorian's model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import StackedEmbedding\n",
    "\n",
    "emb_glove = Zoo.load('glove-6B-50')\n",
    "emb_numberbatch = Zoo.load('numberbatch-19.08-en-50')\n",
    "emb_fasttext = Zoo.load('fasttext-en-mini')\n",
    "emb_fasttext_numberbatch = StackedEmbedding([emb_fasttext, emb_numberbatch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0a78e-030c-49ea-b49e-718fefebeeb6",
   "metadata": {},
   "source": [
    "We also instantiate an NLP parser based on sentence bert and a shim to use this model's token embeddings in the Vectorian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fe0d8-8f4d-4af8-9d19-4a7e3c7280c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(nbutils)\n",
    "\n",
    "nlp = nbutils.make_nlp()\n",
    "\n",
    "from vectorian.embeddings import SentenceBertEmbedding\n",
    "emb_sbert = SentenceBertEmbedding(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-length",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-apache",
   "metadata": {},
   "source": [
    "First load our gold standard that contains our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/raw_data/gold.json\", \"r\") as f:\n",
    "    gold = nbutils.Gold(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.phrases[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.matches('to be or not to be')[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-pasta",
   "metadata": {},
   "source": [
    "We are now ready to build a Vectorian session that contains our documents and embeddings. We use preprocessed corpus data. For details, how this was achieved, see `code/prepare_corpus.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.session import LabSession\n",
    "from vectorian.corpus import Corpus\n",
    "\n",
    "session = LabSession(\n",
    "    Corpus.load(\"data/processed_data/corpus\"),\n",
    "    embeddings=[\n",
    "        emb_sbert,\n",
    "        emb_glove,\n",
    "        emb_numberbatch,\n",
    "        emb_fasttext,\n",
    "        emb_fasttext_numberbatch],\n",
    "    normalizers=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-canal",
   "metadata": {},
   "source": [
    "Let's take a look at the gold standard we imported and whose documents now live inside `session`. We have 20 queries (blue circles), and for each query we have a number of documents (green circles) that we regard as correct matches to these queries. All documents that are not attached to the query are - by definition - no correct matches.\n",
    "\n",
    "Note for interactive users: you can hover your mouse over the nodes to see their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0a4e2-5640-43eb-8950-394da395f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_gold(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2bcfd-c597-44c3-81bc-34cbab10bdef",
   "metadata": {},
   "source": [
    "# What are Word Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7b8be-f3b6-4604-b924-7a453ddc89d0",
   "metadata": {},
   "source": [
    "We now turn to single word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2633d27-0863-4958-bf06-036b8af37604",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.word_vec(emb_glove, \"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95ce2-cdc8-44dc-a69e-06c3c1feb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.metrics import TokenSimilarity, CosineSimilarity\n",
    "\n",
    "token_sim = TokenSimilarity(\n",
    "    emb_numberbatch,\n",
    "    CosineSimilarity()\n",
    ")\n",
    "\n",
    "session.similarity(token_sim, \"hot\", \"cold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82787688-a005-4bab-bcff-5954b333ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sim = TokenSimilarity(\n",
    "    emb_glove,\n",
    "    CosineSimilarity())\n",
    "\n",
    "session.similarity(token_sim, \"hot\", \"cold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96153b53-e9c4-4201-a55d-3d90e7d4f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sim = TokenSimilarity(\n",
    "    emb_sbert,\n",
    "    CosineSimilarity())\n",
    "\n",
    "a = list(session.documents[0].spans(session.partition(\"document\")))[0][3]\n",
    "b = list(session.documents[3].spans(session.partition(\"document\")))[0][2]\n",
    "session.similarity(token_sim, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c4165-4cd5-4083-92f7-ba28c34e2af3",
   "metadata": {},
   "source": [
    "# Exploring Word Embeddings (and our data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942597d-d5db-48e1-b02f-bb1f8084cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.browse(gold, \"rest is silence\", \"Fig for Fortune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bc950-6e6d-4585-b98f-688484df28e9",
   "metadata": {},
   "source": [
    "This is an example, where similarity and therefore embeddings won't help us much. The syntactic structure is mirrored, and \"silence\" is replaced with \"all but wind\". Even if we focus on nouns only, \"silence\" and \"wind\" are not generally similar. Still an embedding approach should be able to recognize that the  words at the beginning of phrase are exact matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa324fc-042b-474c-9ac2-6c28e61d4a41",
   "metadata": {},
   "source": [
    "If we inspect the cosine similarity of the token \"silence\" with other tokens in the context under three of our embeddings, we see that there is more connection between \"silence\" and \"wind\" than we expected. Still, the absolute value of 0.3 for numberbatch is low. Interestingly, glove associates \"silence\" with \"action\", i.e. an opposite. The phenomenon that embeddings sometimes cluster opposites is a common observation and can be an issue when wanting to differentiate between these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65bcc2-678a-4cdb-8a04-99df700c33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"silence\", \"Fig for Fortune\", n_figures=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41a31d-d7c0-4bb7-8fc7-adb824a7fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.browse(gold, \"sea of troubles\", \"Book of Common Prayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfacd0-75f9-4cb5-a7a6-927b4e82ca42",
   "metadata": {},
   "source": [
    "This is a different example, where similarity computation might help. Here, \"sea\" is replaced by \"waves\", and \"troubles\" by \"troublesome\". We should expect to get reasonable results with results on this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c58bda-6bbc-49b9-8f91-01ec5ea660d4",
   "metadata": {},
   "source": [
    "Indeed, by inspecting the cosine similarity of the token \"sea\" with other tokens in the context, we see that this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae847b0b-2ea7-4e97-994f-405d32e3bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"sea\", \"The Book of Common Prayer\", n_figures=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2c55b-2194-436a-94f8-dde47505d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"troubles\", \"The Book of Common Prayer\", n_figures=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97c62-4bef-48f6-8c08-83b585afd4ca",
   "metadata": {},
   "source": [
    "Note how out-of-vocabulary words like \"troublesomest\" will produce zero similarities under standard key-value embeddings, whereas fastText is still able to produce a vector thanks to subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddff82-b84e-4289-a409-debc26d87287",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_token_similarity(session, nlp, gold, \"troublesomest\", \"The Book of Common Prayer\", n_figures=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844c0f3-4256-48a2-aac9-5ff65c6c8e6c",
   "metadata": {},
   "source": [
    "# Exploring Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-aggregate",
   "metadata": {},
   "source": [
    "Before we turn to alignment strategies to match sentences token by token, we first look at representing each document with one embedding in order to gather an understanding how different embedding strategies relate to the nearness of documents. We will later turn to individual token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-controversy",
   "metadata": {},
   "source": [
    "We first prepare additional sentence embeddings using SBERT that we will show in our first big visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import CachedPartitionEncoder, SpanEncoder\n",
    "\n",
    "sbert_encoder = CachedPartitionEncoder(SpanEncoder(\n",
    "    lambda texts: [nlp(t).vector for t in texts]))\n",
    "\n",
    "sbert_encoder.try_load(\"data/processed_data/doc_embeddings\")\n",
    "sbert_encoder.cache(session.documents, session.partition(\"document\"))\n",
    "sbert_encoder.save(\"data/processed_data/doc_embeddings\")\n",
    "\n",
    "sbert_encoder_name = nlp.meta[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d93851-2895-422d-92d0-586aa540d480",
   "metadata": {},
   "source": [
    "Now we construct an Explorer class. In addition to providing the SBERT encoder we just built, we configure the Explorer to use averaging to build documents embeddings from token embeddings.\n",
    "\n",
    "Note to interactive readers: you can change the \"mean\" (averaging) method to other methods for computing document tokens as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fb8cc-5328-483c-becd-9a86fb091192",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer = nbutils.DocEmbeddingExplorer(\n",
    "    session=session, nlp=nlp, gold=gold, extra_encoders={sbert_encoder_name: sbert_encoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ecd2c-91c3-4dd5-af4a-ac578d2a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer.plot([\n",
    "    {\"encoder\": \"paraphrase_distilroberta\", \"locator\": (\"fixed\", \"carry coals\"), 'has_tok_emb': False},\n",
    "    {\"encoder\": \"paraphrase_distilroberta\", \"locator\": (\"fixed\", \"an old man is twice\"), 'has_tok_emb': False}\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fbac7-6a37-411a-9611-42022952a250",
   "metadata": {},
   "source": [
    "In the TSNE visualization above, dots are documents and the colors are the query that yields that document in our gold standard. By hovering over dots with the mouse you get details on the document and query the dot represents. Nearby dots of the same color indicate that the embedding tends to cluster documents similar to our gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5bcebe-bb6c-467c-a147-d7588e28917b",
   "metadata": {},
   "source": [
    "On the left above, we see that the phrase \"we will not carry coals\" (large green-yellow circle with cross) in located closely to the documents associated with that query (smaller green-yellow circles). Similarly, on the right we see that the phrase \"an old man is twice a child\" clusters with the actual (green) documents we associate with it in our gold standard.\n",
    "\n",
    "For these phrases and documents, the `paraphrase_distilroberta` model does a good job of producing a document embedding that actually separates inherent topics (without us telling it to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_explorer.plot([\n",
    "    {\"encoder\": \"numberbatch\", \"selection\": [\n",
    "        'ww_32c26a7909c83bda',\n",
    "        'ww_b5b8083a6a1282bc',\n",
    "        'ww_9a6cb20b0b157545',\n",
    "        'ww_a6f4b0e3428ad510',\n",
    "        'ww_8e68a517bc3ecceb']}\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93e356-ec78-442b-b445-d9f5b6c2ed25",
   "metadata": {},
   "source": [
    "In the plot above we look at the document embedding produced by a **token-based** embedding. This has the advantage that we can actually look at token embeddings that make up the document embedding (through averaging). On the right side, we see a TSNE plot of all token embeddings that occur in the documents that are selected on the left. The hope is that this visualization will give us a clue why the documents on the left might be clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564ef2d-c165-44ed-a930-f85b9cd7f3d2",
   "metadata": {},
   "source": [
    "The red circles on the left represent contexts that match the phrase \"a horse, a horse, my kingform for a horse\" are mapped. If we look at the token embeddings (that includes documents from other other classes), we indeed see that a grouping happens due to word embeddings clustering around \"horse\" (right side), but we also see a cluster around \"boat\", \"sail\" and \"river\" on the left.\n",
    "\n",
    "In fact context 1 contains \"muscle boat\", context 2 contains \"To swim the river villain\", and context 3 contains \"A boat, a boat\". We thereby see that this kind of unsupervised document clustering clusters items due to inherent qualities that might not actually match our query criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a554d4e-9eb9-49f9-900e-ab554afd2757",
   "metadata": {},
   "source": [
    "Interactive note: you can compute different token embeddings plots by selecting different documents on the mouse (drag the mouse to lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a490bb-2a3c-4ae2-a687-2b74d60d4eb7",
   "metadata": {},
   "source": [
    "# Understanding Alignments (WSB vs WMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-reverse",
   "metadata": {},
   "source": [
    "## A Search Query using Alignment over Similar Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aea237-1fe6-4903-89c9-328d7f9790c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(nbutils)\n",
    "\n",
    "\n",
    "def make_index_builder(strategy=\"Alignment\", strategy_options={}):\n",
    "    return nbutils.InteractiveIndexBuilder(session, nlp, partition_encoders={\n",
    "        sbert_encoder_name: sbert_encoder\n",
    "    }, strategy=strategy, strategy_options=strategy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder = make_index_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2ff05-1807-4343-a819-9acb55328ccc",
   "metadata": {},
   "source": [
    "What you see above is the description of a search strategy that we will employ in the following sections of this notebook. Interactive readers can switch to the \"Edit\" part and actually explore the setting in more detail and even change it to something completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder.build_index().find(gold.phrases[0], n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-helen",
   "metadata": {},
   "source": [
    "## Plotting the NDCG over the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6423e26-7035-4b1a-8f58-28634c95ef62",
   "metadata": {},
   "source": [
    "We first define a strategy for searching the corpus. In the summary below you will find the strategy used for the non-interactive version of this text. In the interactive version, you can click on \"Edit\" and change these settings and rerun the following sections of the notebook accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413a21e-c6cf-4f9d-8da1-3717281da294",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_a = make_index_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb41c7-3a2b-4014-b59d-d79a82701a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorian.alignment\n",
    "\n",
    "index_builder_b = make_index_builder(\n",
    "    \"Alignment\", {\"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"kusner\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9287a-5cf1-4c55-925b-50090b2a2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_builder_c = make_index_builder(\n",
    "    \"Alignment\", {\"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"vectorian\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b98cf7-3e58-4699-a2e1-52db2996efe1",
   "metadata": {},
   "source": [
    "Now get an overview of the quality of the results we obtain when using the index configures with `index_builder` by computing the NDCG over all queries in our gold standard with regards to the known optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(nbutils)\n",
    "\n",
    "nbutils.plot_ndcgs(gold, {\n",
    "    \"wsb\": index_builder_a.build_index(),\n",
    "    \"wmd (kusner)\": index_builder_b.build_index(),\n",
    "    \"wmd (vec)\": index_builder_c.build_index()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4d4fd-6fda-48cc-b494-ede28ac06d3f",
   "metadata": {},
   "source": [
    "We see that some queries obtain 100%, i.e. the top results match the optimal ones given in our gold standard. We see that Waterman-Smith-Beyer (WSB) tends to perform a tad better than Word Mover's Distance (WMD), with the exception of \"though this be madness...\", where WMD outperforms WSB. In general the Vectorian modification of WMD, which does not use nbow, performs better than Kusner's original description of WMD. The one exception here \"livers white as milk.\"\n",
    "\n",
    "Let's look at some queries, where the performance for WSB is bad, and try to understand why our search fails to obtain the optimal results at the top of the result list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-endorsement",
   "metadata": {},
   "source": [
    "## Focussing on single queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5c430-0b9b-4ac3-86fd-eb01dd623757",
   "metadata": {},
   "source": [
    "For this, we turn to the query with the lowest score \"though this be madness, yet there is a method in it\", and look at its results in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(nbutils)\n",
    "#nbutils.initialize(False)\n",
    "\n",
    "plot_a = nbutils.plot_results(gold, index_builder.build_index(), \"though this be madness\", rank=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c796b4-7ab3-4671-9f30-fc9801cfdfbf",
   "metadata": {},
   "source": [
    "The best match obtained here - on rank 8 - is anchored on two word matches, namely `madness` and `methods` (which maps to `method` in the original query). The other words are quite different and there is no good alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d210159-ed28-4412-99ad-47956bb0c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_b = nbutils.plot_results(gold, index_builder.build_index(), \"though this be madness\", rank=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcfa0b-016c-45aa-ab67-b6910f08c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_b = nbutils.plot_results(gold, index_builder.build_index(), \"though this be madness\", rank=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5d7f1-3c98-4a90-98ab-7896be9692c1",
   "metadata": {},
   "source": [
    "Above we see the rank 6 result from the same query. We see that \"in\" contributes a large 100% score. This is the motivation for our approach to tag-weighted alignments, that are described in (Liebl and Burghardt, 2020) - we support these in the Vectorian framework, but since we use the `en_paraphrase_distilroberta_base_v1` model as `nlp` in this notebook (which does not provide POS tags), we cannot simply demonstrate it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84e07d-7741-4727-adf1-49064e88b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_weighted_index_builder = make_index_builder(\"Tag-Weighted Alignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763833c0-af3a-480d-ab6d-2ca51f1528f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_results(gold, tag_weighted_index_builder.build_index(), \"though this be madness\", rank=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da85ba-0a96-477e-b963-ac2ec97c9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_token_scores_s(match):\n",
    "    for region in match.regions():\n",
    "        #print(region.gap_penalty, region)\n",
    "        if region.match and region.match.edges:\n",
    "            # use top weighted edge, if more than one.\n",
    "            d = min([e.distance for e in region.match.edges])\n",
    "            yield region.s, 1 - d\n",
    "    \n",
    "\n",
    "def get_token_scores_t(match):\n",
    "    for region in match.regions():\n",
    "        #print(region.gap_penalty, region)\n",
    "        if region.match and region.match.edges:\n",
    "            # use top weighted edge, if more than one.\n",
    "            i = np.argmin([e.distance for e in region.match.edges])\n",
    "            edge = region.match.edges[i] \n",
    "            d = edge.distance\n",
    "            t = edge.t\n",
    "            yield (t.text, t.index), 1 - d\n",
    "\n",
    "    \n",
    "def get_penalty(match):\n",
    "    gap_penalty = 0\n",
    "    for region in match.regions():\n",
    "        gap_penalty += region.gap_penalty\n",
    "    return gap_penalty\n",
    "\n",
    "def compute_score(match):\n",
    "    x = 0\n",
    "    n = 0\n",
    "    for k, v in get_token_scores(match):\n",
    "        x += v\n",
    "        n += 1\n",
    "    print((x - get_penalty(match)) / n)\n",
    "    print(match.score)\n",
    "    \n",
    "class ScoreExplainer:\n",
    "    pass\n",
    "\n",
    "\n",
    "def score_summary(match, get_scores=get_token_scores_s):\n",
    "    # Vectorian computes its alignment scores by summing the query token's scores (each\n",
    "    # between 0 and 1), then subtracting gap penalties, and then normalizing so that the\n",
    "    # resulting score is between 0 and 1 (in the case of untagged alignments, this last\n",
    "    # step simply means dividing by n, if n is the nnumber of matched query tokens).\n",
    "\n",
    "    abs_scores = list(get_scores(match))\n",
    "\n",
    "    gap_penalties = []\n",
    "    for region in match.regions():\n",
    "        if region.gap_penalty > 0:\n",
    "            gap_penalties.append(region.gap_penalty)\n",
    "\n",
    "    max_score = match.score_max\n",
    "\n",
    "    # we produce token scores from which we subtracted the\n",
    "    # global gap penalty. this allows us to return the gap\n",
    "    # penalty as a positive term, instead of a negative\n",
    "    # term - which helps with plotting this data.\n",
    "    \n",
    "    base_score = sum(x[1] for x in abs_scores)\n",
    "    c = (base_score - sum(gap_penalties)) / base_score\n",
    "    tokens = [(k, v * c) for k, v in abs_scores]\n",
    "    \n",
    "    data = {\n",
    "        'tokens': tokens,\n",
    "        'gaps': gap_penalties,\n",
    "        'rest': max_score - (sum(x[1] for x in tokens) + sum(gap_penalties))\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'tokens': [(k, v / max_score) for k, v in data['tokens']],\n",
    "        'gaps': [x / max_score for x in gap_penalties],\n",
    "        'rest': data['rest'] / max_score\n",
    "    }\n",
    "            \n",
    "    \n",
    "import itertools\n",
    "    \n",
    "\n",
    "class TokenColors:\n",
    "    def __init__(self):\n",
    "        import bokeh.palettes\n",
    "\n",
    "        self._colors = bokeh.palettes.Category20[20]\n",
    "\n",
    "        token_color_indices = [i for i in range(20) if i not in (6, 15)]\n",
    "        self._token_colors = [self._colors[i] for i in token_color_indices]\n",
    "    \n",
    "    def tokens(self, n):\n",
    "        return list(itertools.islice(itertools.cycle(self._token_colors), None, n))\n",
    "    \n",
    "    @property\n",
    "    def gap(self):\n",
    "        return self._colors[6]\n",
    "\n",
    "    @property\n",
    "    def err(self):\n",
    "        return self._colors[15]\n",
    "\n",
    "\n",
    "def bar_chart(matches, show_gap_penalty=True):\n",
    "    import bokeh.plotting\n",
    "    import bokeh.models\n",
    "    import bokeh.io\n",
    "    import bokeh.palettes\n",
    "    import numpy as np\n",
    "    import collections\n",
    "\n",
    "    data = collections.defaultdict(list)\n",
    "    tokens = set()\n",
    "\n",
    "    summaries = []\n",
    "    for j, match in enumerate(matches):\n",
    "        summary = score_summary(match, get_token_scores_t)\n",
    "        summaries.append(summary)\n",
    "        for k, v in summary['tokens']:\n",
    "            tokens.add(k)\n",
    "        #print(j + 1, sum([x[1] for x in summary['tokens']]), match.score, len(summary['tokens']))\n",
    "    tokens = sorted(tokens, key=lambda x: x[1])\n",
    "    \n",
    "    def token_key(text, index):\n",
    "        if index is not None:\n",
    "            return f\"{text} [{index}]\"\n",
    "        else:\n",
    "            return text\n",
    "    \n",
    "    for j, summary in enumerate(summaries):\n",
    "        match_tokens = dict(summary['tokens'])\n",
    "        for t in tokens:\n",
    "            x = match_tokens.get(t, 0)\n",
    "            data[token_key(*t)].append(x)\n",
    "        data[\"gap\"].append(sum(summary['gaps']))\n",
    "        data[\"x\"].append(j + 1)\n",
    "      \n",
    "    p = bokeh.plotting.figure(\n",
    "        plot_width=800, plot_height=250,\n",
    "        title=\"\", toolbar_location=None)\n",
    "    \n",
    "    if show_gap_penalty:\n",
    "        elements = [(\"gap\", None)] + tokens\n",
    "    else:\n",
    "        elements = tokens\n",
    "\n",
    "    colors = TokenColors()\n",
    "    vstack = p.vbar_stack(\n",
    "        [token_key(*x) for x in elements[::-1]], x=\"x\", width=0.9,\n",
    "        color=colors.tokens(len(tokens)) + ([colors.gap] if show_gap_penalty else []),\n",
    "        source=bokeh.models.ColumnDataSource(data))\n",
    "    \n",
    "    items = []\n",
    "    for x, vs in zip(elements, vstack[::-1]):\n",
    "        items.append((token_key(*x), [vs]))\n",
    "    \n",
    "    legend = bokeh.models.Legend(items=items, location=(0, -30))\n",
    "    p.add_layout(legend, 'right')\n",
    "\n",
    "    p.xgrid.visible = False\n",
    "    p.xaxis.major_label_orientation = np.pi / 2\n",
    "    p.xaxis.ticker = list(range(1, len(matches) + 1))\n",
    "    \n",
    "    bokeh.io.show(p)\n",
    "    \n",
    "    \n",
    "def token_pie_chart(match):\n",
    "    import bokeh.plotting\n",
    "    import bokeh.models\n",
    "    import bokeh.io\n",
    "    import bokeh.palettes\n",
    "    import numpy as np\n",
    "        \n",
    "    summary = score_summary(match)\n",
    "    colors = TokenColors()\n",
    "\n",
    "    data = {\n",
    "        'element': [x[0] for x in summary['tokens']],\n",
    "        'score': [x[1] for x in summary['tokens']],\n",
    "        'color': colors.tokens(len(summary['tokens']))\n",
    "    }\n",
    "    \n",
    "    data['element'].append('')\n",
    "    data['score'].append(summary['rest'])\n",
    "    data['color'].append(colors.err)\n",
    "    \n",
    "    for gap_penalty in summary['gaps']:\n",
    "        data['element'].append('gap')\n",
    "        data['score'].append(gap_penalty)\n",
    "        data['color'].append(colors.gap)\n",
    "    \n",
    "    data['angle'] = np.array(data['score']) * (2 * np.pi * 0.8)\n",
    "        \n",
    "    data['end_angle'] = np.cumsum(data['angle'])\n",
    "    data['start_angle'] = np.hstack([[0], data['end_angle']])[:-1]\n",
    "\n",
    "    data['start_angle'][-3:] += 2 * np.pi * 0.1\n",
    "    data['end_angle'][-3:] += 2 * np.pi * 0.1\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(data)\n",
    "    \n",
    "    plot_size = 350\n",
    "    p = bokeh.plotting.figure(\n",
    "        plot_width=plot_size, plot_height=plot_size, title=\"\", toolbar_location=None,\n",
    "        tools=\"hover\", tooltips=\"@element: @score\")\n",
    "    \n",
    "    p.annular_wedge(\n",
    "        x=0, y=0, inner_radius=0.25, outer_radius=0.5, direction=\"anticlock\",\n",
    "        start_angle='start_angle',\n",
    "        end_angle='end_angle',\n",
    "        line_color=\"white\", fill_color='color', source=source)  # legend_field='element'\n",
    "    \n",
    "    label_angles = (data['start_angle'] + data['end_angle']) / 2\n",
    "    #label_angles = [x * (np.pi / 180) for x in range(0, 360, 20)]\n",
    "    \n",
    "    def make_label_source(r):\n",
    "        return bokeh.models.ColumnDataSource({\n",
    "            'text': data['element'],\n",
    "            'x': np.cos(label_angles) * r,\n",
    "            'y': np.sin(label_angles) * r,\n",
    "            'angle': label_angles\n",
    "        })\n",
    "    \n",
    "    if True:\n",
    "        labels = bokeh.models.LabelSet(x='x', y='y', text='text',\n",
    "            x_offset=0, y_offset=0, source=make_label_source(0.35),\n",
    "            text_font_size='10pt', angle='angle', render_mode='canvas',\n",
    "            text_baseline=\"middle\")\n",
    "        p.add_layout(labels)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    p.circle(x='x', y='y', source=make_label_source(0.3), color=\"black\")\n",
    "    #p.circle(x=0, y=0, color=\"red\")\n",
    "    \n",
    "    p.axis.axis_label = None\n",
    "    p.axis.visible = False\n",
    "    p.grid.grid_line_color = None\n",
    "\n",
    "    plot_range = 0.55\n",
    "    \n",
    "    p.y_range.start = -plot_range\n",
    "    p.y_range.end = plot_range\n",
    "\n",
    "    p.x_range.start = -plot_range\n",
    "    p.x_range.end = plot_range\n",
    "    \n",
    "    bokeh.io.show(p)\n",
    "\n",
    "    \n",
    "#token_pie_chart(plot_b.selected_match)\n",
    "bar_chart(plot_b.matches[:40], show_gap_penalty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d9b9a-39ab-4fd0-9e86-a5005d1b5538",
   "metadata": {},
   "source": [
    "note the two \"madness\" blocks at ranks 6 and 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1b526-2c4a-4dbb-b34b-edb757496128",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_summary(plot_b.matches[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3c518-5c00-4c20-af8f-ed0ff9477a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_summary(plot_b.matches[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b89aff-4d08-45f9-a4bb-17b1d67c5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbutils.plot_results(gold, index_builder.build_index(), \"though this be madness\", rank=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148bad50-844b-4712-ac81-ff286e2dc17a",
   "metadata": {},
   "source": [
    "# Interactive Searches with Your Own Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa98c8f-6119-4589-aaf4-7bb85473dea5",
   "metadata": {},
   "source": [
    "First specify the text documents you want to search through by an upload widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d07be-ff26-4a56-966a-38450fd29c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "upload = widgets.FileUpload(\n",
    "    accept='.txt',\n",
    "    multiple=True\n",
    ")\n",
    "\n",
    "upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd86e58-6973-42e4-9bcd-5baf565dbf22",
   "metadata": {},
   "source": [
    "From this upload widget contents, we now build a Vectorian session we can perform search through. As always with Vectorian session, we need to specify the embeddings the want to employ for searching. We also need an `nlp` instance for importing the text documents. Depending on the size and number of documents, this step can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452ab0d-6644-495d-ab75-31e9b744bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.importers import StringImporter\n",
    "from vectorian.session import LabSession\n",
    "\n",
    "import codecs\n",
    "\n",
    "\n",
    "def files_to_session(upload):\n",
    "    im = StringImporter(nlp)\n",
    "    \n",
    "    if not upload.value:\n",
    "        raise RuntimeError(\"cannot run on empty upload\")\n",
    "\n",
    "    docs = []\n",
    "    for k, data in upload.value.items():\n",
    "        docs.append(im(\n",
    "            codecs.decode(data[\"content\"], encoding=\"utf-8\"),\n",
    "            title=k,\n",
    "            unique_id=k))\n",
    "\n",
    "    return LabSession(\n",
    "        docs,\n",
    "        embeddings=[\n",
    "            emb_numberbatch,\n",
    "            emb_fasttext],\n",
    "        normalizers=\"default\")\n",
    "\n",
    "upload_session = files_to_session(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63e2a0-fa36-489d-aa7a-01718a6585ab",
   "metadata": {},
   "source": [
    "Now we present the full interactive search interface the Vectorian offers (we have hidden it so far and focussed on a subset). Note that in contrast to our experiments earlier, we do not search on the *document* level by default, but rather the *sentence* level - i.e. we split each document into sentences and then search on each sentence. You can change this in the \"Partition\" dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0469956-6e83-49a2-82ba-db7660f6a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_session.interact(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c05c8-31e3-4736-8312-687f48bfc884",
   "metadata": {},
   "source": [
    "# Literaturliste\n",
    "\n",
    "Pennington, Jeffrey, et al. “Glove: Global Vectors for Word Representation.” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, 2014, pp. 1532–43. DOI.org (Crossref), doi:10.3115/v1/D14-1162.\n",
    "\n",
    "Mikolov, Tomas, et al. “Advances in Pre-Training Distributed Word Representations.” ArXiv:1712.09405 [Cs], Dec. 2017. arXiv.org, http://arxiv.org/abs/1712.09405.\n",
    "\n",
    "Speer, Robyn, et al. “ConceptNet 5.5: An Open Multilingual Graph of General Knowledge.” ArXiv:1612.03975 [Cs], Dec. 2018. arXiv.org, http://arxiv.org/abs/1612.03975.\n",
    "\n",
    "Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” ArXiv:1908.10084 [Cs], Aug. 2019. arXiv.org, http://arxiv.org/abs/1908.10084.\n",
    "\n",
    "Liebl, Bernhard, and Manuel Burghardt. “‘Shakespeare in the Vectorian Age’ – An Evaluation of Different Word Embeddings and NLP Parameters for the Detection of Shakespeare Quotes.” Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, 2020, pp. 56–58."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
