{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190bc476",
   "metadata": {},
   "source": [
    "# “Embed, embed! There’s knocking at the gate.\"\n",
    "##  Detecting Intertextuality with Embeddings and the Vectorian\n",
    "\n",
    "<i>Bernhard Liebl & Manuel Burghardt <br>\n",
    "    Computational Humanities Group, Leipzig University</i>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [1. Introduction](#section_1)\n",
    "    * [1.1 Enter: word embeddings](#section_1_1)\n",
    "    * [1.2 Outline of the notebook](#section_1_2)\n",
    "    * [1.3 Technical setup](#section_1_3)\n",
    "* [2. Data and Tools](#section_2)\n",
    "    * [2.1 Introducing the gold standard dataset](#section_2_1)\n",
    "    * [2.2 Overview of different types of embeddings](#section_2_2)\n",
    "    * [2.3 \"Shapespeare in the Vectorian Age\" – Meet the Vectorian framework](#section_2_3)\n",
    "        * [2.3.1 Loading word embeddings](#section_2_3_1)\n",
    "        * [2.3.2 Creating the session](#section_2_3_2)\n",
    "* [3. Embeddings as a tool for intertextuality research](#section_3)\n",
    "    * [3.1  Exploring word embeddings](#section_3_1)\n",
    "        * [3.1.1 An introduction to word embeddings and token similarity](#section_3_1_1)\n",
    "        * [3.1.2 Detecting Shakespearean intertextuality through word embeddings](#section_3_1_2)\n",
    "    * [3.2 Exploring document embeddings](#section_3_2)\n",
    "    * [3.3 Exploring word mappings: WSB vs. WMD](#section_3_3)\n",
    "        * [3.3.1 Mapping quote queries to longer text documents](#section_3_3_1)\n",
    "        * [3.3.2 Evaluation: Plotting the nDCG over the corpus](#section_3_3_2)\n",
    "        * [3.3.3 Focussing on single queries](#section_3_3_3)\n",
    "    * [3.4 The influence of different embeddings](#section_3_4)\n",
    "* [4. Conclusion](#section_4)\n",
    "* [5. Interactive searches with your own data](#section_5)\n",
    "* [6. References](#section_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a187a5",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"section_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08900f82",
   "metadata": {},
   "source": [
    "The detection of intertextual references in text corpora is a digital humanities topic that has gained a lot of attention in recent years (for instance Bamman & Crane, 2008; Burghardt et al., 2019; Büchler et al., 2013; Forstall et al., 2015; Scheirer et al., 2014). While intertextuality – from a literary studies perspective – describes the phenomenon of one text being present in another text (cf. Genette, 1993), the computational problem at hand is the task of text similarity detection (Bär et al., 2012), and more concretely, semantic similarity detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e786e",
   "metadata": {},
   "source": [
    "In the following example of Shakespearean intertextuality, the words *bleed* and *leak* are semantically (and phonetically) similar, demonstrating that *Star Trek* here is quoting Shakespeare without any doubt: \n",
    "\n",
    "> Shylock: If you prick *us*, do *we* not **bleed**. <br>\n",
    "(Shakespeare; The Merchant of Venice)\n",
    "\n",
    "> Data: If you prick *me*, do *I* not **leak**. <br>\n",
    "(Star Trek: The Next Generation; The Measure of a Man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93736f9",
   "metadata": {},
   "source": [
    "## 1.1 Enter: word embeddings <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d01c6",
   "metadata": {},
   "source": [
    "Over the years, there have been various attempts for measuring semantic similarity, some of them knowledge-based (e.g. based on WordNet), others corpus-based, like LDA (Chandrasekaran & Vijay, 2021). The arrival of word embeddings (Mikolov et al., 2013) has changed the field considerably by introducing a new and fast way to tackle the notion of word meaning. On the one hand, word embeddings are building blocks that can be combined with a number of other methods, such as alignments, soft cosine or Word Mover's Distance, to implement some kind of sentence similarity (Manjavacas et al., 2019). On the other hand, the concept of embeddings can be extended to work on the sentence-level as well, which is a conceptually different approach (Wieting et al., 2016). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c1073",
   "metadata": {},
   "source": [
    "We introduce the **<a href=\"https://github.com/poke1024/vectorian\">Vectorian</a>** as a framework that allows researchers to try out different embedding-based methods for intertextuality detection. In contrast to previous versions of the Vectorian (Liebl & Burghardt, 2020a/b) as a mere web interface with a limited set of static parameters, we now present a clean and completely redesigned API that is showcased in an interactive Jupyter notebook. In this notebook, we first use the Vectorian to build queries where we plug in static word embeddings such as FastText (Mikolov et al., 2018) and GloVe (Pennington et al., 2014). We evaluate the influence of computing similarity through alignments such as Waterman-Smith-Beyer (WSB; Waterman et al., 1976) and two variants of Word Mover’s Distance (WMD; Kusner et al., 2015). We also investigate the performance of state-of-art sentence embeddings like Siamese BERT networks (Reimers & Gurevych, 2019) for the task - both on a document level (as document embeddings) and as contextual token embeddings. Overall, we find that POS tag-weighted WSB with fastText offers highly competitive performance. Readers can upload their own data for performing search queries and try out additional vector space metrics such as p-norms or improved sqrt‐cosine similarity (Sohangir & Wang, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec578f",
   "metadata": {},
   "source": [
    "## 1.2 Outline of the notebook <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354577a",
   "metadata": {},
   "source": [
    "In the notebook, we will go through different examples of intertextuality to demonstrate and explain the implications of different embeddings and similarity measures. To achieve this we provide a small ground truth corpus of intertextual Shakespeare references that can be used for some controlled evaluation experiments. Our main goal is to provide an interactive environment, where researchers can test out different methods for text reuse and intertextuality detection. This notebook thus adds to a critical reflection of digital methods and can help to shed some light on their epistemological implications for the field of computational intertextuality detection. At the end of the notebook, researchers can also easily import their own data and investigate all the showcased methods for their specific texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd01e9",
   "metadata": {},
   "source": [
    "## 1.3 Technical setup <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81509d15",
   "metadata": {},
   "source": [
    "We import a couple of helper functions for visualizations and various computations (`nbutils`), a wrapper to load our gold standard data (`gold`), and finally the Vectorian library (`vectorian`), through which we will perform searches and evaluations later on.\n",
    "\n",
    "In `nbutils.initialize` we check whether there is a [bokeh server](https://docs.bokeh.org/en/latest/index.html) available. This typically *is* the case for local Jupyter installations, but is *not* the case for notebooks running on *mybinder*. In the latter case, the notebook has some limitations regarding interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad4b2be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-size: small;\">Running notebook in <b>SERVER</b> mode.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.1.min.js\": \"YF85VygJKMVnHE+lLv2AM93Vbstr0yo2TbIu5v8se5Rq3UQAUmcuh4aaJwNlpKwa\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.1.min.js\": \"KKuas3gevv3PvrlkyCMzffFeaMq5we/a2QsP5AUoS3mJ0jmaCL7jirFJN3GoE/lM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.1.min.js\": \"MK/uFc3YT18pkvvXRl66tTHjP0/dxoSH2e/eiNMFIguKlun2+WVqaPTWmUy/zvh4\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.1.min.js\": \"YF85VygJKMVnHE+lLv2AM93Vbstr0yo2TbIu5v8se5Rq3UQAUmcuh4aaJwNlpKwa\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.1.min.js\": \"KKuas3gevv3PvrlkyCMzffFeaMq5we/a2QsP5AUoS3mJ0jmaCL7jirFJN3GoE/lM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.1.min.js\": \"MK/uFc3YT18pkvvXRl66tTHjP0/dxoSH2e/eiNMFIguKlun2+WVqaPTWmUy/zvh4\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"code\")  # make \"nbutils\" and \"code\" importable\n",
    "\n",
    "import nbutils, gold, vectorian\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "nbutils.initialize(\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedd8b6-31de-464e-8970-4ec5508e0eb9",
   "metadata": {},
   "source": [
    "# 2. Data and Tools <a class=\"anchor\" id=\"section_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be272b-e11c-47a4-a94e-8da5c061fa15",
   "metadata": {},
   "source": [
    "## 2.1 Introducing the gold standard dataset <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a3660-df73-4444-97e8-f53827fe0bcd",
   "metadata": {},
   "source": [
    "In the following we use a collection of 100 short text snippets (=documents) that quote a total of 20 different Shakespeare phrases. All of these documents were derived from the [WordWeb IDEM portal](http://wordweb-idem.ch/about-us.html). Each document quotes exactly one of the 20 phrases. For some phrases, e.g. \"to be or not to be\", there are more quoting documents than for others (see interactive overview of documents below). If there are multiple documents that quote the same phrase, we selected them in a way each of them does this in a different way. There are no verbatim quotes in the documents, but always more or less complex variations of the original phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19440288-e2ab-4d0d-89c9-3698c4494a9a",
   "metadata": {},
   "source": [
    "We use this collection of quote documents as a gold standard, to be able to assess how well different embeddings work for different types of quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab19081-815e-4b11-80e7-77581c4f2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data = gold.Data(\"data/raw_data/gold.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b075437-2b26-4e02-ad30-eb00d4b92a3d",
   "metadata": {},
   "source": [
    "Technically speaking, our gold standard consists of a number of `Patterns`. Each `Pattern` is associated with a phrase, e.g. \"to be or not to be\", which occurs in a rephrased form in other works and contexts. These reoccurences, which model text reuse, are called `Occurrences` in our data. Each such `Occurrence` carries the actual phrase and a larger context in which it occurs, which together we call the `Evidence`. The data layout for our gold standard looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b47921-fad6-4103-9efe-521ed733c5e1",
   "metadata": {},
   "source": [
    "![UML of gold standard data](miscellaneous/gold_uml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bddd3e-b9dc-44b0-8382-c01a7205185f",
   "metadata": {},
   "source": [
    "One specific example in this data is the `Occurrence` of the `Pattern` \"to be or not to be\" in a `Source` titled \"The Phoenix\". The `Evidence` to be found here is the phrase \"to be named or not be named\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac736ecd-1152-479e-9519-d1a8b1c535b6",
   "metadata": {},
   "source": [
    "All of the 20 quote patterns can be browsed in the 100 associated documents via the following widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeba4fc4-7d00-4edb-824d-a887131c6ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6918dd30854d5e9d4605e9e8ca0bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='pattern:', layout=Layout(width='max-content'), options=(('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbutils.Browser(gold_data, \"to be or not to be\", \"The Phoenix\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b812a3-67fe-4dbe-8aa6-914080433e0c",
   "metadata": {},
   "source": [
    "For a further exploration of the dataset, we also provide a visualization of the gold standard dataset, with `Patterns` indicated as blue circles and `Evidence` indicated as green circles. Matching evidence and patterns are connected via edges and each bouqet consists of one pattern and the matching instances of text reuse. Hovering the mouse over the nodes reveals their actual contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3442b4da-106c-446e-8152-ce9ec583f067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"489af4b7-bdd5-4c39-b1ec-11a5019ef05e\" data-root-id=\"1005\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"ecf8a63e-40fc-4318-ab55-eaf0bf87410c\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"height\":400,\"output_backend\":\"svg\",\"renderers\":[{\"id\":\"1011\"}],\"title\":{\"id\":\"1031\"},\"toolbar\":{\"id\":\"1008\"},\"width\":800,\"x_range\":{\"id\":\"1003\"},\"x_scale\":{\"id\":\"1034\"},\"y_range\":{\"id\":\"1004\"},\"y_scale\":{\"id\":\"1032\"}},\"id\":\"1005\",\"type\":\"Plot\"},{\"attributes\":{\"end\":6.25,\"start\":-1.0},\"id\":\"1003\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"end\":[\"ww_594e076e93ed4ccf\",\"ww_caf27ffdc954c844\",\"ww_66a52436edb537eb\",\"ww_a2a0ab6fbca2028d\",\"ww_90d74567307f904e\",\"ww_669475d1f67eb920\",\"ww_19b54f39c1d245a7\",\"ww_295a0d9237083fe7\",\"ww_971c475bea921406\",\"ww_13c9cec7f15c0056\",\"ww_01d64039ef29add5\",\"ww_22f51d85f231b2c7\",\"ww_6cc712b23bbafc5e\",\"ww_293034ae2a49b418\",\"ww_4c352e527d12797b\",\"ww_f38739f4c2391f47\",\"ww_b0913df56f856f27\",\"ww_7ecb8d5ef454d329\",\"ww_8c5990fb7392c89b\",\"ww_b7e69820855e7384\",\"ww_00ce5b9c822fbaca\",\"ww_3a359a22623b0162\",\"ww_d9ad6d0b3d1a5598\",\"ww_1bfaaa3ed70bb1ac\",\"ww_663f0143320cc310\",\"ww_6821daa19a2840e1\",\"ww_2239f6f9bbcb7446\",\"ww_18fac1f2fb1ce41f\",\"ww_028ce75225433cb6\",\"ww_880962580e78c61f\",\"ww_909a37b5a02277e8\",\"ww_b4cc414fb56c2490\",\"ww_32c26a7909c83bda\",\"ww_782a5cebdaa9d8ae\",\"ww_b5b8083a6a1282bc\",\"ww_a59a84adf088089f\",\"ww_9a6cb20b0b157545\",\"ww_d62468b11823d4c9\",\"ww_4d363cb4783e0b62\",\"ww_6f12e582a20c2675\",\"ww_d2361763e8343194\",\"ww_870f29a7dd6f45d9\",\"ww_e01ecfd8de9acc61\",\"ww_a6f4b0e3428ad510\",\"ww_e2adb2be413fd5a5\",\"ww_b71ba2ae83103344\",\"ww_0dbc3e90bbb46334\",\"ww_d67a1f9d4d50f936\",\"ww_8e68a517bc3ecceb\",\"ww_ff8f000dd9cb7a3b\",\"ww_10ba3fba1c0c1529\",\"ww_3b6425a2159f1d62\",\"ww_8bdc1e03f5a6a253\",\"ww_379b8583a4d90080\",\"ww_d2d2a262485c5d23\",\"ww_07f94c5a14cdc619\",\"ww_235b7f298b54d631\",\"ww_c2f29658555093e7\",\"ww_585eaf1c6368e7af\",\"ww_6e3b338bf16ebf06\",\"ww_98df9ad5de049472\",\"ww_25e51ae4b6234004\",\"ww_ba8b881de92fd8a4\",\"ww_7286ff06f562c07b\",\"ww_1917adced358cb4a\",\"ww_89443854a5459695\",\"ww_b42fadea9f893c6e\",\"ww_57dcec5b9ff2c4b9\",\"ww_aaeb0dba8a5ac513\",\"ww_3bed848d2566166b\",\"ww_aef241d5b9263438\",\"ww_2560885c478fdfe0\",\"ww_ff2cef3775c5331c\",\"ww_58a0dec287a4170c\",\"ww_01bd7d0624c8a955\",\"ww_97b67e22685ce125\",\"ww_415def570b10b5b9\",\"ww_215595adbf8efd41\",\"ww_5cdd4abfb59f6f0f\",\"ww_c9d35c25f4fc4189\",\"ww_7795f933b612f710\",\"ww_0eaf5058b7a910c7\",\"ww_51946dd5d1d57d4a\",\"ww_21a4c5dc5b2f6ed6\",\"ww_994d23875f3f8ca7\",\"ww_f32f713a36985f62\",\"ww_7be6a3588e6b5de0\",\"ww_83e9ca4d103b86e1\",\"ww_95966dab4576ec42\",\"ww_3a6f583779747368\",\"ww_2b785e10c908c7ec\",\"ww_0747839b540f70c7\",\"ww_2e3d7768539ae2ab\",\"ww_a055872533d2ab57\",\"ww_84f20b8b2eb71001\",\"ww_eb3ebae9ea90daa0\",\"ww_6c565da6889f5269\",\"ww_3101379204640286\",\"ww_c6bb592912665657\",\"ww_cd82c881f4f118d0\"],\"start\":[\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"to be or not to be\",\"sea of troubles\",\"sea of troubles\",\"sea of troubles\",\"sea of troubles\",\"sea of troubles\",\"pampered jades of Asia\",\"pampered jades of Asia\",\"pampered jades of Asia\",\"pampered jades of Asia\",\"pampered jades of Asia\",\"The rest is silence.\",\"The rest is silence.\",\"The rest is silence.\",\"The rest is silence.\",\"The rest is silence.\",\"an old man is twice a child\",\"an old man is twice a child\",\"an old man is twice a child\",\"an old man is twice a child\",\"an old man is twice a child\",\"In my mind's eye\",\"In my mind's eye\",\"In my mind's eye\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"a horse, a horse, my kingdom for a horse\",\"go, by Saint Hieronimo\",\"go, by Saint Hieronimo\",\"go, by Saint Hieronimo\",\"go, by Saint Hieronimo\",\"go, by Saint Hieronimo\",\"thereby hangs a tale\",\"thereby hangs a tale\",\"thereby hangs a tale\",\"springes to catch woodcocks\",\"springes to catch woodcocks\",\"springes to catch woodcocks\",\"springes to catch woodcocks\",\"springes to catch woodcocks\",\"springes to catch woodcocks\",\"springes to catch woodcocks\",\"Illo, ho, ho, my lord\",\"Illo, ho, ho, my lord\",\"Illo, ho, ho, my lord\",\"Illo, ho, ho, my lord\",\"Illo, ho, ho, my lord\",\"Illo, ho, ho, my lord\",\"Illo, ho, ho, my lord\",\"though this be madness, yet there is method in it\",\"though this be madness, yet there is method in it\",\"though this be madness, yet there is method in it\",\"though this be madness, yet there is method in it\",\"though this be madness, yet there is method in it\",\"planets strike\",\"planets strike\",\"planets strike\",\"planets strike\",\"planets strike\",\"I do bear a brain.\",\"I do bear a brain.\",\"I do bear a brain.\",\"I do bear a brain.\",\"livers white as milk\",\"livers white as milk\",\"livers white as milk\",\"All the world's a stage\",\"All the world's a stage\",\"we will not carry coals\",\"we will not carry coals\",\"we will not carry coals\",\"we will not carry coals\",\"we will not carry coals\",\"we will not carry coals\",\"we will not carry coals\",\"frailty, thy name is woman\",\"frailty, thy name is woman\",\"frailty, thy name is woman\",\"frailty, thy name is woman\",\"hell itself should gape\",\"hell itself should gape\",\"hell itself should gape\",\"hell itself should gape\",\"hell itself should gape\",\"O all you host of heaven!\",\"O all you host of heaven!\",\"O all you host of heaven!\"]},\"selected\":{\"id\":\"1048\"},\"selection_policy\":{\"id\":\"1047\"}},\"id\":\"1017\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_width\":{\"value\":1.5}},\"id\":\"1026\",\"type\":\"MultiLine\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"1013\"}},\"id\":\"1015\",\"type\":\"CDSView\"},{\"attributes\":{\"edge_renderer\":{\"id\":\"1018\"},\"inspection_policy\":{\"id\":\"1036\"},\"layout_provider\":{\"id\":\"1020\"},\"node_renderer\":{\"id\":\"1014\"},\"selection_policy\":{\"id\":\"1035\"}},\"id\":\"1011\",\"type\":\"GraphRenderer\"},{\"attributes\":{},\"id\":\"1032\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_multi\":null,\"logo\":null,\"tools\":[{\"id\":\"1009\"}]},\"id\":\"1008\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"1013\"},\"glyph\":{\"id\":\"1021\"},\"hover_glyph\":null,\"muted_glyph\":null,\"view\":{\"id\":\"1015\"}},\"id\":\"1014\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data_source\":{\"id\":\"1017\"},\"glyph\":{\"id\":\"1026\"},\"hover_glyph\":null,\"muted_glyph\":null,\"view\":{\"id\":\"1019\"}},\"id\":\"1018\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"graph_layout\":{\"All the world's a stage\":[5.25,0.75],\"I do bear a brain.\":[3.75,0.75],\"Illo, ho, ho, my lord\":[1.5,0.75],\"In my mind's eye\":[3.75,0.0],\"O all you host of heaven!\":[2.25,1.5],\"The rest is silence.\":[2.25,0.0],\"a horse, a horse, my kingdom for a horse\":[4.5,0.0],\"an old man is twice a child\":[3.0,0.0],\"frailty, thy name is woman\":[0.75,1.5],\"go, by Saint Hieronimo\":[5.25,0.0],\"hell itself should gape\":[1.5,1.5],\"livers white as milk\":[4.5,0.75],\"pampered jades of Asia\":[1.5,0.0],\"planets strike\":[3.0,0.75],\"sea of troubles\":[0.75,0.0],\"springes to catch woodcocks\":[0.75,0.75],\"thereby hangs a tale\":[0.0,0.75],\"though this be madness, yet there is method in it\":[2.25,0.75],\"to be or not to be\":[0.0,0.0],\"we will not carry coals\":[0.0,1.5],\"ww_00ce5b9c822fbaca\":[2.449641397948891,-0.2055555864696958],\"ww_01bd7d0624c8a955\":[4.043589262176329,0.7545881934255653],\"ww_01d64039ef29add5\":[0.468446431401725,-0.1460012656328836],\"ww_028ce75225433cb6\":[3.0607370951268127,-0.37702393685107966],\"ww_0747839b540f70c7\":[0.7507759797938198,1.864858772077026],\"ww_07f94c5a14cdc619\":[1.3230879380271903,0.5904060351465262],\"ww_0dbc3e90bbb46334\":[-0.3690326110479096,0.6641076701130442],\"ww_0eaf5058b7a910c7\":[-0.15091288305035414,1.8929590683768156],\"ww_10ba3fba1c0c1529\":[0.6077460319108372,0.5434802927908642],\"ww_13c9cec7f15c0056\":[0.7489805891708855,-0.3636663767089847],\"ww_18fac1f2fb1ce41f\":[3.1852860971190653,-0.3347456376409071],\"ww_1917adced358cb4a\":[2.3887873873115475,0.5847571889252384],\"ww_19b54f39c1d245a7\":[-0.3634949519821691,0.09246545170510855],\"ww_1bfaaa3ed70bb1ac\":[2.3974288738522853,-0.35685168967019154],\"ww_215595adbf8efd41\":[4.811200528738154,0.9529839512016895],\"ww_21a4c5dc5b2f6ed6\":[-0.412730913963686,1.6182396673563002],\"ww_2239f6f9bbcb7446\":[2.9962393486246737,-0.2798121004415269],\"ww_22f51d85f231b2c7\":[0.6294825792327121,-0.2576724283673212],\"ww_235b7f298b54d631\":[1.3353558640635121,0.9665703236782378],\"ww_2560885c478fdfe0\":[3.283910679742572,0.834054621288493],\"ww_25e51ae4b6234004\":[1.629595415061355,0.9701473236371263],\"ww_293034ae2a49b418\":[0.49772506774854164,-0.2913199439353012],\"ww_295a0d9237083fe7\":[-0.3049473644349952,-0.04647242265335863],\"ww_2b785e10c908c7ec\":[0.5252400349762573,1.8406738697977962],\"ww_2e3d7768539ae2ab\":[1.4900462036121351,1.8020383299316547],\"ww_3101379204640286\":[2.4764110535650357,1.790028902377038],\"ww_32c26a7909c83bda\":[4.788000878002825,-0.29828353815295144],\"ww_379b8583a4d90080\":[0.6616109208541586,1.0104802904094132],\"ww_3a359a22623b0162\":[2.2858130096066054,-0.25851219905356376],\"ww_3a6f583779747368\":[0.4958810145613475,1.717984190289812],\"ww_3b6425a2159f1d62\":[0.5406322929974421,0.8540310043243486],\"ww_3bed848d2566166b\":[3.24687525725342,0.6766145941851527],\"ww_415def570b10b5b9\":[4.71856096765035,1.038895384059373],\"ww_4c352e527d12797b\":[1.472265525397184,-0.2604509993951469],\"ww_4d363cb4783e0b62\":[4.866328096017622,-0.06732714451854677],\"ww_51946dd5d1d57d4a\":[-0.2918576121887717,1.8374595901064066],\"ww_57dcec5b9ff2c4b9\":[3.2454636251709994,0.9640689656872234],\"ww_585eaf1c6368e7af\":[1.2694531938485047,0.7971600883904516],\"ww_58a0dec287a4170c\":[3.8742118765108255,1.0628050880098707],\"ww_594e076e93ed4ccf\":[-0.4463412954051369,-0.03276825968408283],\"ww_5cdd4abfb59f6f0f\":[4.828924318006078,0.8112439571547735],\"ww_663f0143320cc310\":[3.2510368257153566,-0.08379151871330395],\"ww_669475d1f67eb920\":[-0.3856882639891115,-0.2873769544274666],\"ww_66a52436edb537eb\":[-0.4245158946166366,-0.1506491637453194],\"ww_6821daa19a2840e1\":[3.229641788755904,-0.22294944119200966],\"ww_6c565da6889f5269\":[1.3292889634579568,1.7985115547040458],\"ww_6cc712b23bbafc5e\":[0.6045703990474106,-0.38991296726918867],\"ww_6e3b338bf16ebf06\":[1.691735396353088,0.7455826919666784],\"ww_6f12e582a20c2675\":[4.837523274152327,-0.19139915558845733],\"ww_7286ff06f562c07b\":[2.374887374106178,1.0341758102108465],\"ww_7795f933b612f710\":[5.564586336200073,0.9532313891301751],\"ww_782a5cebdaa9d8ae\":[4.757577280219361,0.1564717398957968],\"ww_7be6a3588e6b5de0\":[-0.3747949336495363,1.7439346313446527],\"ww_7ecb8d5ef454d329\":[1.4994891850532786,-0.3954795796414185],\"ww_83e9ca4d103b86e1\":[-0.2854773067876793,1.6520091200464349],\"ww_84f20b8b2eb71001\":[1.6602920847856002,1.8107247703121612],\"ww_870f29a7dd6f45d9\":[5.659203324695402,-0.13495474692776327],\"ww_880962580e78c61f\":[4.007881454400906,-0.11413057329001883],\"ww_89443854a5459695\":[2.4995048194775853,0.7624471083603483],\"ww_8bdc1e03f5a6a253\":[0.4274213409279179,0.7072233153551266],\"ww_8c5990fb7392c89b\":[1.2951097825950997,-0.23398914850205021],\"ww_8e68a517bc3ecceb\":[0.3931880344792498,0.8572351521822567],\"ww_909a37b5a02277e8\":[3.8457417032732053,-0.32357700425458963],\"ww_90d74567307f904e\":[-0.15779172308978875,-0.25905780614704715],\"ww_95966dab4576ec42\":[0.6366028922233544,1.8676861976365084],\"ww_971c475bea921406\":[-0.26589175354921746,-0.36676220161556494],\"ww_97b67e22685ce125\":[4.038033998195741,0.8983897762942684],\"ww_98df9ad5de049472\":[1.5247449349787305,0.5199406476695273],\"ww_994d23875f3f8ca7\":[-0.37408956017425543,1.4999950391620152],\"ww_9a6cb20b0b157545\":[4.671598665992832,-0.3589366824378963],\"ww_a055872533d2ab57\":[1.4083689274253706,1.908059781061995],\"ww_a2a0ab6fbca2028d\":[-0.2850273381284049,-0.1932329785372919],\"ww_a59a84adf088089f\":[4.55261407070915,-0.3182880489001013],\"ww_a6f4b0e3428ad510\":[5.598898252308135,0.1067109521464273],\"ww_aaeb0dba8a5ac513\":[3.065213087028497,1.0260859362062202],\"ww_aef241d5b9263438\":[3.179317586175111,1.0760918736927694],\"ww_b0913df56f856f27\":[1.3664787001393406,-0.35927805852467287],\"ww_b42fadea9f893c6e\":[2.2278144331127385,0.995596651456483],\"ww_b4cc414fb56c2490\":[3.965288435650601,-0.2635033886732477],\"ww_b5b8083a6a1282bc\":[4.836011999446118,0.04029673082401403],\"ww_b71ba2ae83103344\":[-0.40670658243800895,0.7879556083147621],\"ww_b7e69820855e7384\":[2.1324143735917604,-0.28615491705687246],\"ww_ba8b881de92fd8a4\":[2.473562509749896,0.9296307099756537],\"ww_c2f29658555093e7\":[1.4772045943856376,1.0352079624346278],\"ww_c6bb592912665657\":[2.3843506191338655,1.870215585321337],\"ww_c9d35c25f4fc4189\":[5.613936890545957,0.8157698547403033],\"ww_caf27ffdc954c844\":[-0.11573682468014207,-0.38495212048398286],\"ww_cd82c881f4f118d0\":[2.2523217266544546,1.8629710384368974],\"ww_d2361763e8343194\":[5.538119509685559,-0.14152358613352717],\"ww_d2d2a262485c5d23\":[0.4659181112072272,0.5858429541431831],\"ww_d62468b11823d4c9\":[4.689596025958257,-0.1701600382424771],\"ww_d67a1f9d4d50f936\":[-0.34905558405823384,0.8983210133356435],\"ww_d9ad6d0b3d1a5598\":[2.240518155710818,-0.3993870686923337],\"ww_e01ecfd8de9acc61\":[5.658639490162339,-0.00029944340468844013],\"ww_e2adb2be413fd5a5\":[5.532105768467317,-0.2682622202114927],\"ww_eb3ebae9ea90daa0\":[1.5644895433515946,1.919046280219165],\"ww_f32f713a36985f62\":[-0.15903788291762827,1.7761166526073462],\"ww_f38739f4c2391f47\":[1.6230559167321377,-0.309280606126258],\"ww_ff2cef3775c5331c\":[3.999647577349792,1.021483650571245],\"ww_ff8f000dd9cb7a3b\":[0.49935873905550365,0.9969218107452462]}},\"id\":\"1020\",\"type\":\"StaticLayoutProvider\"},{\"attributes\":{\"data\":{\"context\":[\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Those Five Questions (Tusculanae) (1561)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Perchance I have not told you all that I think; for <span style=\\\"font-weight:bold;\\\">not to be when you have been,</span> I think is the greatest misery that may be.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Mirum in Modum (1602)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">And which of both (thinkst thou) would Reason choose? <span style=\\\"font-weight:bold;\\\">To be made capable of endless bliss,</span> With possibility the same to lose, And win a Hell, where all is quite amiss; <span style=\\\"font-weight:bold;\\\">Or not to Be at all,</span> both those to miss: Sure, Reas'n the first would chose, because the last Is lowest hell, where highest horror is; For in Not-beings bottom, being fast, Ought would to worse than nought, unworn waste.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Phoenix (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Fidelio:] As also neither to touch, attempt, molest, or incumber any part, or parts whatsoever, either <span style=\\\"font-weight:bold;\\\">to be named or not to be named,</span> either hidden or unhidden, either those that boldly look abroad, or those that dare not show their faces -[.]</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Epigraph (1649)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Killed by ingratitude here blessed within doth rest: <span style=\\\"font-weight:bold;\\\">To marry or not to marry</span> which is best.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Northern Lass (1629)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Trainwell:] Not to be mad, sir, because she is melancholy. Not by taking a wrong course for her recovery to ruin her, and forfeit your judgment. Do you think that commands with chidings, threats or stripes, have power to work upon her, when she has neither will nor reason within herself <span style=\\\"font-weight:bold;\\\">to do, or not to do</span> anything whatsoever?</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Philosophical Rudiments Concerning Government and Society (1651)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Now the gathering together of many men, who deliberate of what is <span style=\\\"font-weight:bold;\\\">to be done or not to be done</span> for the common good of all men, is that which I call a council.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">An Essay Concerning Human Understanding (1689)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">All the Actions, that we have any Idea of, reducing themselves, as has been said, to these two, viz. Thinking and Motion, so far as a Man has a power <span style=\\\"font-weight:bold;\\\">to think, or not to think: to move, or not to move,</span> according to the preference or direction of his own mind, so far is a Man Free.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Dives and Lazarus (1685)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Come now, thou that pretend'st to Act the Man, Something there needs must be, which ne'er began. As all were nothing once, So 'twould be now. A Number from bare Cyphers could not grow. Nothing's a Barren Womb. If that could breed, <span style=\\\"font-weight:bold;\\\">To be and not to be</span> were well agreed.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Duke of Guise (1682)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[King:] Tomorrow Guise is made Lieutenant-General, Why then tomorrow I no more am King; 'Tis time to push my slackened vengeance home, <span style=\\\"font-weight:bold;\\\">To be a King, or not to be at all;</span> The Vow that manacled my Rage is loosed, Even Heaven is wearied with repeated Crimes, Till lightning flashes round to guard the Throne, And the curbed Thunder grumbles to be gone.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Book of Common Prayer (1562)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[...] May so pass <span style=\\\"font-weight:bold;\\\">the waves of this troublesome world</span> [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">On the State of Matrimony (1571)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">So shall we passe through the dangers of <span style=\\\"font-weight:bold;\\\">the troublous sea</span> of this world</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Britain's Remembrancer (1628)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Yet went I to his Work the furthest way: And, travelled, as mine own occasions lay. Which he perceiving, sent a Storm that crest me; Made shipwreck of my hopes; my labour lost me; Befooled my wisdom; of much joy bereft me; Within the <span style=\\\"font-weight:bold;\\\">Sea of many troubles</span> left me; And what with speed and ease I might have done At first; hath long with pain been lingered on.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Loyal Lovers (1652)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Adrastus:] Thanks, good Clarathea , for thy remembrance; For I had almost lost my self in joyes unspeakable. My dear Letesia , (so I dare call the now) Hast thou made choice of any course to steer in this Same <span style=\\\"font-weight:bold;\\\">sea of trouble,</span> mixt with joy?</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Paradise Lost (1667)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">These were from without The growing miseries, which <span style=\\\"font-weight:bold;\\\">Adam</span> saw Already in part, though hid in gloomiest shade, To sorrow abandoned, but worse felt within, And in <span style=\\\"font-weight:bold;\\\">a troubled Sea of passion</span> tossed, Thus to disburden sought with sad complaint [...].</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Ovid's Metamorphoses (1567)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">What? is it I that did behold <span style=\\\"font-weight:bold;\\\">the pampered Jades of Thrace</span> With Mangers full of flesh of men on which they fed apace? Ist I that down at sight thereof their greasy Mangers threw, And both the fatted Jades themselves and eke their master slew?</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Tamburlaine Part 2 (1587)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Tamburlaine:] [...] The <span style=\\\"font-weight:bold;\\\">headstrong jades of Thrace</span> [...] <span style=\\\"font-weight:bold;\\\">Alcides</span> tamed, That King Aegeus fed with human flesh, And made so wanton that they knew their strengths, Were not subdued with valour more divine Than you by this unconquered arm of mine.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Coxcomb (1609)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Dorothy:] God's precious you young contagious whore must you be ticing? and is your flesh so wrank sir, that two may live upon't? I am glad to hear your curtail grown so lusty; he was dry foundered t'other day, weehee <span style=\\\"font-weight:bold;\\\">my pampered jade of Asia.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Rich Cabinet, or Galateo (1616)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Idle Bees gather no honey, and so become drones to rob the hive: thus are <span style=\\\"font-weight:bold;\\\">pampered jades unapt to travel,</span> and lazy rogues unwilling to work.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The World Runs on Wheels (1623)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">In what state I would lean over the Boot, and look, and pry if I saw any of my acquaintance, and then I would stand up, veiling my Bonnet, kissing my right claw, extending my arms as I had been swimming, with God save your Lordship, Worship, or how dost thou honest neighbour or good-fellow? in a word, the Coach made me think myself better than my betters that went on foot, and that I was but little inferior to Tamburlaine, being jolted thus in state by <span style=\\\"font-weight:bold;\\\">those pampered Jades of Belgia</span> [...].</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Ovid's Metamorphoses (1567)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Instead of legs, to both her sides stick fingers long and fine: <span style=\\\"font-weight:bold;\\\">The rest is belly.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">A Fig for Fortune (1596)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Catechysius the Hermit:] Now all is but the action of the Mind, That rectified, <span style=\\\"font-weight:bold;\\\">the rest is all but wind.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Love's Pilgrimage (1616)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Rodorigo:] Why should such plants as you are, pleasure's children That owe their blushing years to gentle objects, Tenderly bred, and brought up in all fulness, Desire the stubborn wars? [Marcantonio:] In those 'tis wonder, That make their ease their god, and not their honour: But noble General, my end is other, Desire of knowledge Sir, and hope of tying Discretion to my time, which only shows me, And not my years, a man, and makes that more. Which we call handsome, <span style=\\\"font-weight:bold;\\\">the rest is but boys' beauty,</span> And with the boy consumed.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Five Love Letters from a Nun to a Cavalier (1678)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">And I dare assure you that it will not be the worse for you neither, if you never set your heart upon any other woman: for certainly a Passion under the degree of mine, will never content you: You may find more Beauty perhaps elswhere (tho' the time was when you found no fault with mine) but you shall never meet with so true a heart; <span style=\\\"font-weight:bold;\\\">and all the rest is nothing.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">To Alexis in Answer to His Poem against Fruition (1688)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Man! our great business and our aim, For whom we spread our fruitless snares, No sooner kindles the designing flame, But to the next bright object bears The Trophies of his conquest and our shame: Inconstancy's the good supreme <span style=\\\"font-weight:bold;\\\">The rest is airy Notion,</span> empty Dream!</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Sententiae Pueriles (1540)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\"><span style=\\\"font-weight:bold;\\\">Old Men are twice Children. Bis pueri senes.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Cordila (1574)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Yet nevertheless, my father did me not mislike: But age so simple is, and easy to subdue: As childhood weak, that's void of wit and reason quite: They think there's nought, you flatter feigned, but all is true: <span style=\\\"font-weight:bold;\\\">Once old and twice a child,</span> 'tis said with you, Which I affirm by proof, that was defined: In age my father had a childish mind.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The New Academy, or The New Exchange (1636)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Ephraim:] But sir, beware you fall not back again Into your childish follies: but go forwards In manly actions, for <span style=\\\"font-weight:bold;\\\">non progredi id est regredi.</span> [Nehemiah:] I know the meaning of that too, Ephraim: that's <span style=\\\"font-weight:bold;\\\">once a man and twice a child.</span> But if I turn child again, while I have teeth in my head, I'll give Mistress Blith leave to dig 'em out with sugar plums, as she almost did these two of 'em yesterday, with her knuckles.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Religio Medici (1642)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">I find my growing Judgement daily instructs me how to be better, but my untamed affections and confirmed vitiosity makes me daily do worse; I find in my confirmed age the same sins I discovered in my youth, I committed many then because I was a child, and because I commit them still I am yet an Infant. Therefore I perceive <span style=\\\"font-weight:bold;\\\">a man may be twice a child</span> before the days of dotage, and stand in need of Aeson's bath before threescore.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Advice to an Old Man of Sixty-Three, about to Marry a Girl of Sixteen (1686)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Now fie upon him! what is Man, Whose life at best is but a span? When to an Inch it dwindles down, Ice in his bones, Snow on his crown, That he within his crazy brain, Kind thoughts of Love should entertain, That he, when Harvest comes should plow, And when 'tis time to reap, go sow, Who in imagination only strong, Though <span style=\\\"font-weight:bold;\\\">twice a Child</span>, can never twice grow young.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Canterbury Tales (1387)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">But yet never Christian Britons so exiled That there near some that in their privacy Honoured Christ and heathen folk beguiled And near the castle such there dwelt three. That one of hem was blind and might not see, But it were <span style=\\\"font-weight:bold;\\\">with the eye of their mind</span> With which men see, after that they [have] been blind.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Acolastus (English) (1540)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Pelargus:] I learn by myself how much the care of the father is toward his son, being absent, or being away from him, because my son is now lately put away from me, my mind can not abide or suffer to be in rest or quiet, but that he diligently haunteth before <span style=\\\"font-weight:bold;\\\">mine eyes, and before my mind.</span> i. but that he continually or still is present or haunteth or walketh to and fro, <span style=\\\"font-weight:bold;\\\">before mine eyes, and in my mind</span> (by reason of my much thinking upon him) I take care or thought for him being absent, not otherwise.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">A Treatise Of Self-Denial (1675)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">You have had many a thought of it, and bestowed many a day's labour for it, and yet do you not know it? O but you never saw it for all this? Answ. It is a spiritual Blessedness that Flesh and Blood can neither enjoy nor see: But <span style=\\\"font-weight:bold;\\\">by the eye of the Mind</span> you have often seen, at least some glimpse of it; You know that it is the present intuition and full fruition of God himself and your glorified Redeemer with his blessed Angels and Saints in perfect Love, and Joy, and Praise. And if you know this, you are not altogether Strangers to Heaven.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Battle of Alcazar (1588)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Muly Mahamet:] Villain, <span style=\\\"font-weight:bold;\\\">a horse.</span> [Boy:] Oh my Lord, if you return you die. [Muly Mahamet:] Villain I say, give me <span style=\\\"font-weight:bold;\\\">a horse to fly,</span> To swim the river villain, and to flie. [Muly Mahamet:] <span style=\\\"font-weight:bold;\\\">A horse, a horse, villain a horse,</span> That I may take the river straight and fly.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Scourge of Villainy (1598)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\"><span style=\\\"font-weight:bold;\\\">A man, a man, a kingdom for a man!</span> Why, how now, currish, mad Athenian? Thou cynic dog, see'st not streets do swarm With troops of men?</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Eastward Ho! (1605)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Security:] [...] <span style=\\\"font-weight:bold;\\\">A boat, a boat, a boat, a full hundred Marks for a boat.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Fawn, or Parasitaster (1605)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Don Zuccone:] [...] <span style=\\\"font-weight:bold;\\\">A fool, a fool, a fool! my Coxcomb for a fool!</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Poor Man's Comfort (1616)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Sigismond:] [...] but where is Europa? See where she swims away upon a bull's back; <span style=\\\"font-weight:bold;\\\">my kingdom for a boat, for a muscle boat;</span> lay more sails on!</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Little French Laywer (1620)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Sampson:] [...] Look up brave friend, I have no means to rescue thee, <span style=\\\"font-weight:bold;\\\">my Kingdom for a sword.</span> [Champernell:] I'll sword you presently. I'll claw your skin-coat too.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Iter Boreale (1647)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Upon this Hill they met; why, he could tell The Inch where Richmond stood, where Richard fell; Besides what of his knowledge he could say, He had Authentic notice from the Play; Which I might guess by his mustering up the Ghosts, And policies not incident to hosts: But chiefly by that one perspicuous thing, Where he mistook a Player for a King, For when he would have said, <span style=\\\"font-weight:bold;\\\">King Richard</span> died, And called <span style=\\\"font-weight:bold;\\\">a horse, a horse,</span> he Burbage cried.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Don Carlos, Prince of Spain (1676)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[King Philip of Spain:] I've all this while been angry but in vain; She heats me first, then strokes me tame again. Oh wert thou true how happy should I be! Think'st Thou that I have Joy to part with thee? No, <span style=\\\"font-weight:bold;\\\">all my Kingdom for the bliss I'd give:</span> Nay though it were not so but to believe.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Spanish Tragedy (1587)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[King:] Who is he that interrupts our business? [Hieronimo:] Not I. (Aside) <span style=\\\"font-weight:bold;\\\">Hieronimo, beware, go by, go by.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Blurt, Master Constable (1601)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Simperina:] <span style=\\\"font-weight:bold;\\\">Go from my window go, go from, and away, go by old Hieronimo</span>; nay and you shrink i'th wetting, walk, walk, walk.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Satiromastix, or The Untrussing of the Humorous Poet (1601)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Tucca:] <span style=\\\"font-weight:bold;\\\">Go by Hieronimo, go by</span>; and here, drop the ten shillings into this basin; do, drop, when Jack?</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Captain (1612)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Fabritio:] [...] And call thee bloody bones, and Spade, and Spitfire, And Gaffer Madman; <span style=\\\"font-weight:bold;\\\">and go by Hieronimo</span>, And will with wisp and come aloft, and crack rope, And old Saint Dennis with the dudgeon Codpiece: And twenty such names.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Rebellion (1633)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[First Tailor:] Who shall act Prince Belthazer and the King? [Third Tailor:] I will do Prince Belthazer too: and for the King Who but I? which of you all has such a face for a King, Or such a leg to trip up the heels of a Traitor? [Second Tailor:] You will do all I think. [Third Tailor:] Yes marry will I; who but Uirmine? yet I will Leave all to play the King: <span style=\\\"font-weight:bold;\\\">Pass by Hieronimo.</span></div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Garland or Chaplet of Laurel (1568)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Yet, though I say it, <span style=\\\"font-weight:bold;\\\">thereby lieth a tale</span> [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Ram Alley (1608)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Sir Oliver Smallshank:] We old men have our crotchets, our conundrums, Our fegares, quirks and quibibles, As well as youth, Justice Tutchim I go To hunt no Buck, but prick a lusty Doe, I go in truth a wooing. [Justice Tutchin:] Then ride with me, I'll bring you to my sister Somerfield. [Sir Oliver Smallshank:] Justice not so: <span style=\\\"font-weight:bold;\\\">by her there hangs a Tale.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">A Maidenhead Well Lost (1629)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Clown:] I confess it; but whether you have served him well, or no, <span style=\\\"font-weight:bold;\\\">there hangs a Tale.</span></div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Tears of an Affectionate Shepherd Sick for Love (1594)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">I have a fine bow, and an ivory arrow: And if thou miss, yet meat thou shalt lack, I'll hang a bag and bottle at thy back. <span style=\\\"font-weight:bold;\\\">Wilt thou set springs in a frosty Night, To catch the long-billed Woodcock</span> and the Snipe? (By the bright glimmering of the Starry light) The Partridge, Phaesant, or the greedy Crype?</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Grimello's Fortunes (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Ganuzido:] Why, what have you taken then? or have you authority to take fools as you finde them in your way? If you have, you may happen yet to be deceived. [Grimello:] Why sir, I set no <span style=\\\"font-weight:bold;\\\">springs for Woodcocks</span>, and though I be no great wise man, yet I can do something else, than shoo the Goose for my living: and therefore, I pray you neither fear your Purse, nor play too much with my folly.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Roaring Girl (1611)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Sir Davy:] Here's <span style=\\\"font-weight:bold;\\\">the spring I ha' set to catch this woodcock in:</span> an action In a false name - unknown to him - is entered. I'th' counter to arrest Jack Dapper.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Laquei Ridiculosi (1613)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">My honest friends that reads, I you beseech To make the best construction of each letter; And not to blame my lavishness of speech, In paying soundly where I am a debtor: My word and credit else you should infringe, Which was <span style=\\\"font-weight:bold;\\\">to catch the Woodcock in a Springe</span>.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The House of Correction (1619)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Alas, poor Book, hunt not thou after praise, Nor dare to stretch thy hand unto the Bays Upon a Poets head: let it suffice To thee and me, the world doth us despise. For 'tis a mad World, and it turns on hinges, Whilst some a-birding go, and <span style=\\\"font-weight:bold;\\\">set their springes For to catch Woodcocks.</span> Others sting and bite Like Wasps and Mastiffs, and do take delight To quarrel with their shadows, nay, themselves, And their own brood.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Citizen Turned Gentleman (1672)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Young Jorden:] By that means we shall get quit of him. [Trickmore:] The posture our affairs are in at present, do not much seem to require his absence, therefore I have contrived a defeat, and will keep him yet in play. I have set another <span style=\\\"font-weight:bold;\\\">Spring, which if it catch the Woodcock</span>, 'twill hold him fast. Look here comes forth our Knight in Petti-coats: muffle yourself up in your Cloak, and be gone. [Young Jorden:] A stately dame on my word.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The London Cuckolds (1682)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Roger:] Her's a Letter, Sir, to be delivered to you with all speed. [Ramble:] Ha - let me see't quickly (Opens it and reads.) From Eugenia [Townly:] Ay the Devil's coming abroad again to hinder your conversion. [Ramble (reads):] Sir, My Husband will be from home all this morning, I am very desirous to be informed the particulars of last nights misfortune; curiosity forces me, in spite of blushes to give you this invitation. Enter at the back-door without knocking, if you meet not Jane below come directly up stairs. - Good. [Townly:] <span style=\\\"font-weight:bold;\\\">Here's another springe laid to catch the Woodcock.</span> [Ramble:] Frank, is not here temptation now, is it to be resisted think you, can flesh and blood forbear going? [Townly:] Truly here is a very fair appearance.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Dutch Courtesan (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Master Mulligrub:] Who goes there: <span style=\\\"font-weight:bold;\\\">Illo, ho, ho:</span> sounds shall I run mad, Loose my wits: shall I be hanged, hark who goes there? Do not fear to be poor Mullegrub, Thou hast a sure stock now.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Poor Man's Comfort (1616)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Sigismond:] <span style=\\\"font-weight:bold;\\\">Ho! illo, illo, illo.</span> [Cazzo:] The game's not up yet Sir. I think some gelder had a hand in the getting of him, he understands no language but the horn.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Thierry, King of France, and His Brother Theodoret (1617)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Devitry:] Here Sir, here, I could beat my brains out, that could not think of boots, boots Sir, wide-topped boots, I shall love them the better whilst I live; but are you sure your Jewels are here Sir? [Protaldye:] Sure sayst thou? ha, ha, ha. [Devitry:] <span style=\\\"font-weight:bold;\\\">So ho, illo ho.</span> [Soldiers within] Here Captain. here. [Protaldye:] 'Foot what do you mean Sir? (Enter Soldiers.) [Devitry:] A trick to boot, say you; here you dull slaves, purchase, purchase the soul of the Rock, Diamonds, sparkling Diamonds.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Child Hath Found His Father, or The Birth of Merlin (1622)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Clown:] If there be any man that wants a name, will come in for conscience sake, and acknowledge himself to be a Whore-Master, he shall have that laid to his charge in an hour, he shall not be rid on in an age; if he have Lands, he shall have an heir, if he have patience, he shall have a wife, if he have neither Lands nor patience, he shall have a whore, so ho boy, so ho, so, so. [Prince Uter (within):] <span style=\\\"font-weight:bold;\\\">So, ho, by, so, ho, illo ho, illo ho.</span> [Clown:] Har, hark sister, there's one hollows to us, what a wicked world's this, a man cannot so soon name a whore but a knave comes presently, and see where he is, stand close a while, sister.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Seven Champions of Christendom (1634)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Calib the Witch:] Ha, ha, ha; and when will that be Tarpax? Vanish like smoke, my fear, come kiss me my Love, Thou hast earned thy breakfast Chuck; here suck thy fill. [Clown (within):] <span style=\\\"font-weight:bold;\\\">Illo ho, ho Illo.</span> [Tarpax:] What mortal's that disturbs us? Shall I blast him? [Calib the Witch:] Hold my Love, 'tis Suckabus our son; fall off.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Triumph of Beauty (1634)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Bottle:] Weather? You are wise; do you think, Sir, I have so little honesty, to be Sir Pandarus to your Melancholy. <span style=\\\"font-weight:bold;\\\">Illo, ho . -</span> [Paris:] What, art thou mad? [Bottle:] You are little better: if you can get their consent. [Paris:] Whose consent? [Bottle:] Hobinoll, Crab, Toad-stool, - <span style=\\\"font-weight:bold;\\\">Illo ho , boys.</span> Some friends of yours, that sent me to hunt out your Highness, your humble Subjects and Play-fellows, that have a mind to be merry.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Macbeth, A Tragedy (1664)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Maid:] Yes, I have heard stories, how some men Have in such lonely places been affrighted With dreadful shapes and noises. (Macduff hollows.) [Lady Macduff:] But hark, my lord sure hollows; 'Tis he; answer him quickly. [Servant:] <span style=\\\"font-weight:bold;\\\">Illo, ho, ho, ho.</span> (Enter Macduff.) [Lady Macduff:] Now I begin to see him: are you afoot, my Lord?</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">King Lear (1605)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Edgar:] O, matter and impertinency mixed, <span style=\\\"font-weight:bold;\\\">Reason in madness!</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Fatal Jealousy (1672)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Don Gerardo:] These Actions sure did seem a perfect madness. [Servant:] It seemed indeed <span style=\\\"font-weight:bold;\\\">a madness methodized</span>, Like theirs who are Transported far with Passion.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">A Sermon Preached at the Funeral of Mr. Henry Stubbs (1676)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">The wonder of that Foreigner in Henry VIII's days, Deus bone quomodo hic vivunt gentes, that saw men killed for being Protestants, and for being Papists, was not so contradictory a subject as the Papists usage of the Saints, a stupendous instance of <span style=\\\"font-weight:bold;\\\">man's madness and Satan's methods</span>, that at the same time can rack, and burn, and Murder Saints, and yet Honour the Relics, Names and Memories of the Dead that were before them.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Devil of a Wife (1686)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Rowland:] Methought <span style=\\\"font-weight:bold;\\\">there was a Method in her madness,</span> she did not know herself i'th Glass.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">A Fool's Preferment (1688)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Lyonel:] [...] (Sings) There's nothing so fatal as Woman, To hurry a Man to his Grave, You must think, you may plot, You may sigh like a Sot: She uses you more like a Slave. But a Bottle, although it be common, The Cheats of the Fair will undo, It will drive from your Head The Delights of the Bed; He that's drunk is not able to woo. (Exit Lyonel) [Celia:] <span style=\\\"font-weight:bold;\\\">Method in Madness</span>, Grace even in Distraction: I'll never leave him, 'till, by Art or Prayer, I have restored his Senses to their Office.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Every Man In His Humour (1598)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Bobadill:] [...] sure <span style=\\\"font-weight:bold;\\\">I was struck with a planet</span> then, for I had no power to touch my weapon. [Ed. Knowell:] Aye, like enough, I have heard of many that have been beaten under a planet: go, get you to a surgeon.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Titus Andronicus (1592)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Marcus:] Who is this? My niece, that flies away so fast? - Cousin, a word. Where is your husband? If I do dream, would all my wealth would wake me. If I do wake, <span style=\\\"font-weight:bold;\\\">some planet strike me down</span> That I may slumber an eternal sleep.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Coriolanus (1608)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Cominius:] [...] Alone he entered The mortal gate o' th' city, which he painted With shunless destiny; aidless came off And with a sudden reinforcement <span style=\\\"font-weight:bold;\\\">struck Corioles like a planet.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Atheist's Tragedy (1610)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[D'Amville:] With an amazement behold thine error and <span style=\\\"font-weight:bold;\\\">be planet struck.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Winter's Tale (1611)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Leontes:] [...] Physic for 't there's none. It is <span style=\\\"font-weight:bold;\\\">a bawdy planet, that will strike</span> Where 'tis predominant; and 'tis powerful, think it, From east, west, north, and south.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Dutch Courtesan (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Mrs. Mulligrub:] [...] <span style=\\\"font-weight:bold;\\\">nay and I bear not a brain</span> [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Fawn, or Parasitaster (1605)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Gonzago:] [...] is't not well thought my Lord, <span style=\\\"font-weight:bold;\\\">we must bear brain</span> [...]</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Ram Alley (1608)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Throat:] [...] Dash, <span style=\\\"font-weight:bold;\\\">we must bear some brain</span> to St. John's street, go, run, fly: and afar off enquire [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Golden Age (1611)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Clown:] [...] As I can bear a pack, so <span style=\\\"font-weight:bold;\\\">I can bear a brain</span> [...].</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Henry IV Part 2 (1597)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Falstaff:] [...] left the <span style=\\\"font-weight:bold;\\\">liver white and pale</span>, which is the badge of pusillanimity and cowardice.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Hamlet (1600)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Hamlet:] [...] <span style=\\\"font-weight:bold;\\\">But I am pigeon-livered and lack gall</span> [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">King Lear (1605)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Kent:] [...] a <span style=\\\"font-weight:bold;\\\">lily-livered,</span> action-taking, whoreson, glass-gazing [...] the composition of a knave, beggar, coward, pander, and the son and heir of a mongrel bitch.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Damon and Pythias (Edwards) (1564)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Damon:] [...] <span style=\\\"font-weight:bold;\\\">Pythagoras</span> said that <span style=\\\"font-weight:bold;\\\">this world was like a stage Whereon many play their parts.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Roman Actor (1626)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Aretinus:] Are you on <span style=\\\"font-weight:bold;\\\">the stage,</span> You talk so boldly? [Paris:] <span style=\\\"font-weight:bold;\\\">The whole world being one</span> This place is not exempted [...].</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Every Man Out of His Humour (1599)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Puntarvolo:] [...] Here comes one that will <span style=\\\"font-weight:bold;\\\">carry coals</span>; ergo, will hold my dog [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Henry V (1599)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Boy:] [...] Nym and Bardolph are sworn brothers in filching, and in Calais they stole a fire shovel. I knew by that piece of service the men would <span style=\\\"font-weight:bold;\\\">carry coals.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Antonio's Revenge (1600)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Balurdo:] [...] I were as he, <span style=\\\"font-weight:bold;\\\">I would bear no coals</span>, law I, I begin to swell, puff.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Malcontent (1603)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Malevole:] Great slaves fear better than love, born naturally for <span style=\\\"font-weight:bold;\\\">a coal-basket</span> [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">May Day (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Quintiliano:] And yet take heed you swear by no man's beard but your own, for that may breed a quarrel; above all things <span style=\\\"font-weight:bold;\\\">you must carry no coals.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">May Day (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Quintiliano:] Why sir, because I entertained this Gentleman for my Antient, (being my dear friend and an excellent scholar) he takes pepper ith' nose and sneezes it out upon my Antient; now sir <span style=\\\"font-weight:bold;\\\">(he being of an uncoal-carrying spirit)</span> falls foul on him, calls him gull openly [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Law-Tricks (1604)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Lurdo:] [...] <span style=\\\"font-weight:bold;\\\">I'll carry coals and you will,</span> no horns, I know the law.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">On the State of Matrimony (1571)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\"><span style=\\\"font-weight:bold;\\\">The woman is a weak creature</span> [...] <span style=\\\"font-weight:bold;\\\">She is the weaker vessel, of a frail heat, inconstant</span> [...]. <span style=\\\"font-weight:bold;\\\">The woman is the more frail part</span> [...]. <span style=\\\"font-weight:bold;\\\">The woman is a frail vessel.</span></div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Woman-Hater (1606)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Gondarino:] My Lord, I speak not this to gain new grace, But howsoever you esteem my words, My love and duty will not suffer me To see you favour such a prostitute, And I stand by dumb; without Rack, Torture, Or Strappado, I'll unrip myself, I do confess I was in company with <span style=\\\"font-weight:bold;\\\">that pleasing peace of frailty, that we call woman</span>; I do confess after along and tedious siege, I yielded.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Samson Agonistes (1671)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Dalila:] \\\"[...] To what I did thou shewdst me first the way. But I to enemies revealed, and should not. Nor shouldst thou have trusted that to <span style=\\\"font-weight:bold;\\\">woman's frailty</span>: Ere I to thee, thou to thyself wast cruel. [...]\\\"</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Sonnet (1687)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">How could you once so kind appear, To kiss, to sigh, and shed a tear, To cherish and caress me so, And now not let but bid me go? <span style=\\\"font-weight:bold;\\\">Oh Woman! Frailty is thy name</span>, Since she's untrue y'are all to blame, And but in man no truth is sound: 'Tis a fair Sex, we all must love it, But (on my conscience) could we prove it, They all are false even under ground.</div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Bishop's Bible (1568)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Therefore <span style=\\\"font-weight:bold;\\\">gapeth hell and openeth her mouth marvellous wide,</span> that their glory, multitude, and wealth, with such as rejoice in her, may descend into it.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">Doctor Faustus (1588)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Faustus:] [...] My God, my God, look not so fierce on me. Adders and serpents, let me breathe awhile. <span style=\\\"font-weight:bold;\\\">Ugly hell, gape not</span>, come not, Lucifer! I'll burn my books. Ah, Mephistophilis!</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Lovesick King (1619)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Canutus:] Thou'st spoke enough to damn thee, Impudent Traitor, go die unpitied; Though thou hast my hate, thou shalt not have the honour of my sword to take away thy life, you of our Guard; See a base death performed upon this Slave. [Captain:] Farewell my Liege you once must have a grave. (Exit with Guard.) [Harold:] My Resolution's firm, and I will speak, <span style=\\\"font-weight:bold;\\\">though hell should gape</span> to swallow me alive; What's he that's gone to death my Sovereign? [Canutus:] Traytor (Harold) to my best content.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Perfect-Cursed-Blessed Man (1628)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">This Sin faln Man raise-up t'integrity, Or rase Me out from Heav'ns society. What though He sinned? alas He was but Earth! Though dead in Sin? thy Grace can give new birth! Though grieved with pains? O thou canst 'ford him ease! <span style=\\\"font-weight:bold;\\\">Though Hell gape for Him?</span> thou canst Hell appease! Thou mad'st Him Thee to bless eternally: But damned Souls curse everlastingly.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Fatal Contract (1633)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[Clovis:] What is it, Eunuch? [...] <span style=\\\"font-weight:bold;\\\">Though death stood gaping wide to swallow me, I would not shrink nor fear.</span></div>\\n        </div>\\n        \",\"\",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Geneva Bible (1560)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Then God turned himself away and gave them up to serve <span style=\\\"font-weight:bold;\\\">the host of heaven,</span> as it is written in the book of the Prophets [...].</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Geneva Bible (1560)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">Thou art Lord alone: thou hast made heaven, and <span style=\\\"font-weight:bold;\\\">the heaven of all heavens, with all their host,</span> the earth, and all things that are therein, the seas, and all that are in them, and thou preservest them all, and <span style=\\\"font-weight:bold;\\\">the host of the heaven</span> worshippeth thee.</div>\\n        </div>\\n        \",\"\\n        <div style=\\\"margin-left:2em\\\">\\n            <span style=\\\"font-variant:small-caps; font-size: 14pt;\\\">The Atheist's Tragedy (1610)</style>\\n            <hr>\\n            <div style=\\\"font-variant:normal; font-size: 10pt;\\\">[D'Amville:] Now <span style=\\\"font-weight:bold;\\\">all the host of heaven</span> forbid!</div>\\n        </div>\\n        \"],\"index\":[\"to be or not to be\",\"ww_594e076e93ed4ccf\",\"ww_caf27ffdc954c844\",\"ww_66a52436edb537eb\",\"ww_a2a0ab6fbca2028d\",\"ww_90d74567307f904e\",\"ww_669475d1f67eb920\",\"ww_19b54f39c1d245a7\",\"ww_295a0d9237083fe7\",\"ww_971c475bea921406\",\"sea of troubles\",\"ww_13c9cec7f15c0056\",\"ww_01d64039ef29add5\",\"ww_22f51d85f231b2c7\",\"ww_6cc712b23bbafc5e\",\"ww_293034ae2a49b418\",\"pampered jades of Asia\",\"ww_4c352e527d12797b\",\"ww_f38739f4c2391f47\",\"ww_b0913df56f856f27\",\"ww_7ecb8d5ef454d329\",\"ww_8c5990fb7392c89b\",\"The rest is silence.\",\"ww_b7e69820855e7384\",\"ww_00ce5b9c822fbaca\",\"ww_3a359a22623b0162\",\"ww_d9ad6d0b3d1a5598\",\"ww_1bfaaa3ed70bb1ac\",\"an old man is twice a child\",\"ww_663f0143320cc310\",\"ww_6821daa19a2840e1\",\"ww_2239f6f9bbcb7446\",\"ww_18fac1f2fb1ce41f\",\"ww_028ce75225433cb6\",\"In my mind's eye\",\"ww_880962580e78c61f\",\"ww_909a37b5a02277e8\",\"ww_b4cc414fb56c2490\",\"a horse, a horse, my kingdom for a horse\",\"ww_32c26a7909c83bda\",\"ww_782a5cebdaa9d8ae\",\"ww_b5b8083a6a1282bc\",\"ww_a59a84adf088089f\",\"ww_9a6cb20b0b157545\",\"ww_d62468b11823d4c9\",\"ww_4d363cb4783e0b62\",\"ww_6f12e582a20c2675\",\"go, by Saint Hieronimo\",\"ww_d2361763e8343194\",\"ww_870f29a7dd6f45d9\",\"ww_e01ecfd8de9acc61\",\"ww_a6f4b0e3428ad510\",\"ww_e2adb2be413fd5a5\",\"thereby hangs a tale\",\"ww_b71ba2ae83103344\",\"ww_0dbc3e90bbb46334\",\"ww_d67a1f9d4d50f936\",\"springes to catch woodcocks\",\"ww_8e68a517bc3ecceb\",\"ww_ff8f000dd9cb7a3b\",\"ww_10ba3fba1c0c1529\",\"ww_3b6425a2159f1d62\",\"ww_8bdc1e03f5a6a253\",\"ww_379b8583a4d90080\",\"ww_d2d2a262485c5d23\",\"Illo, ho, ho, my lord\",\"ww_07f94c5a14cdc619\",\"ww_235b7f298b54d631\",\"ww_c2f29658555093e7\",\"ww_585eaf1c6368e7af\",\"ww_6e3b338bf16ebf06\",\"ww_98df9ad5de049472\",\"ww_25e51ae4b6234004\",\"though this be madness, yet there is method in it\",\"ww_ba8b881de92fd8a4\",\"ww_7286ff06f562c07b\",\"ww_1917adced358cb4a\",\"ww_89443854a5459695\",\"ww_b42fadea9f893c6e\",\"planets strike\",\"ww_57dcec5b9ff2c4b9\",\"ww_aaeb0dba8a5ac513\",\"ww_3bed848d2566166b\",\"ww_aef241d5b9263438\",\"ww_2560885c478fdfe0\",\"I do bear a brain.\",\"ww_ff2cef3775c5331c\",\"ww_58a0dec287a4170c\",\"ww_01bd7d0624c8a955\",\"ww_97b67e22685ce125\",\"livers white as milk\",\"ww_415def570b10b5b9\",\"ww_215595adbf8efd41\",\"ww_5cdd4abfb59f6f0f\",\"All the world's a stage\",\"ww_c9d35c25f4fc4189\",\"ww_7795f933b612f710\",\"we will not carry coals\",\"ww_0eaf5058b7a910c7\",\"ww_51946dd5d1d57d4a\",\"ww_21a4c5dc5b2f6ed6\",\"ww_994d23875f3f8ca7\",\"ww_f32f713a36985f62\",\"ww_7be6a3588e6b5de0\",\"ww_83e9ca4d103b86e1\",\"frailty, thy name is woman\",\"ww_95966dab4576ec42\",\"ww_3a6f583779747368\",\"ww_2b785e10c908c7ec\",\"ww_0747839b540f70c7\",\"hell itself should gape\",\"ww_2e3d7768539ae2ab\",\"ww_a055872533d2ab57\",\"ww_84f20b8b2eb71001\",\"ww_eb3ebae9ea90daa0\",\"ww_6c565da6889f5269\",\"O all you host of heaven!\",\"ww_3101379204640286\",\"ww_c6bb592912665657\",\"ww_cd82c881f4f118d0\"],\"node_color\":[\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#abdda4\",\"#2b83ba\",\"#abdda4\",\"#abdda4\",\"#abdda4\"],\"phrase\":[\"<i>to be or not to be</i>\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"<i>sea of troubles</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>pampered jades of Asia</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>The rest is silence.</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>an old man is twice a child</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>In my mind's eye</i>\",\"\",\"\",\"\",\"<i>a horse, a horse, my kingdom for a horse</i>\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"<i>go, by Saint Hieronimo</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>thereby hangs a tale</i>\",\"\",\"\",\"\",\"<i>springes to catch woodcocks</i>\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"<i>Illo, ho, ho, my lord</i>\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"<i>though this be madness, yet there is method in it</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>planets strike</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>I do bear a brain.</i>\",\"\",\"\",\"\",\"\",\"<i>livers white as milk</i>\",\"\",\"\",\"\",\"<i>All the world's a stage</i>\",\"\",\"\",\"<i>we will not carry coals</i>\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"<i>frailty, thy name is woman</i>\",\"\",\"\",\"\",\"\",\"<i>hell itself should gape</i>\",\"\",\"\",\"\",\"\",\"\",\"<i>O all you host of heaven!</i>\",\"\",\"\",\"\"],\"subset\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,8,8,8,8,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,11,11,11,11,11,11,12,12,12,12,12,12,13,13,13,13,13,14,14,14,14,15,15,15,16,16,16,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,18,19,19,19,19]},\"selected\":{\"id\":\"1046\"},\"selection_policy\":{\"id\":\"1045\"}},\"id\":\"1013\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1031\",\"type\":\"Title\"},{\"attributes\":{\"source\":{\"id\":\"1017\"}},\"id\":\"1019\",\"type\":\"CDSView\"},{\"attributes\":{\"end\":2.5,\"start\":-1.0},\"id\":\"1004\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1034\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"tooltips\":\"\\n        @phrase\\n        @context\\n        \"},\"id\":\"1009\",\"type\":\"HoverTool\"},{\"attributes\":{\"fill_color\":{\"field\":\"node_color\"},\"size\":{\"value\":10}},\"id\":\"1021\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1035\",\"type\":\"NodesOnly\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1036\",\"type\":\"NodesOnly\"}],\"root_ids\":[\"1005\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.1\"}};\n",
       "  var render_items = [{\"docid\":\"ecf8a63e-40fc-4318-ab55-eaf0bf87410c\",\"root_ids\":[\"1005\"],\"roots\":{\"1005\":\"489af4b7-bdd5-4c39-b1ec-11a5019ef05e\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1005"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbutils.plot_gold(gold_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917ba04",
   "metadata": {},
   "source": [
    "## 2.2 Overview of different types of embeddings <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c80d2b",
   "metadata": {},
   "source": [
    "**Word embeddings** take up the linguistic concept of collocations. For each word in a corpus it is recorded with which other words it occurs. These collocation profiles are then represented as vectors. If, for example, two words (e.g. \"car\" and \"truck\") occur with very similar words (e.g. „wheels, drive, street, etc.“) then they would also have very similar word vectors, i.e. they would be semantically - or at least structurally - very similar.\n",
    "\n",
    "There are now various established ways to compute embeddings for word similarity tasks. A first important distinction to be made is between *token* / *word* embeddings and *document* embeddings (see diagram below). While **token embeddings** model one embedding per token, **document embeddings** try to  map an entire document (i.e. a set of tokens) into one single embedding. There are two common ways to compute document embeddings. One way is to derive them from token embeddings, for instance by averaging those. More complex approaches train dedicated models that are optimized to produce good document embeddings.\n",
    "\n",
    "This means that, all in all, we can distinguish three types of embeddings:\n",
    "\n",
    "* original token embeddings\n",
    "* document embeddings derived from token embeddings (e.g. by averaging them)\n",
    "* document embeddings from dedicated models, such as Sentence-BERT (Reimers & Gurevych, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1718fe62-1f7c-4a36-adcb-d42985c73a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"634pt\" height=\"237pt\" viewBox=\"21.60 21.60 612.60 215.60\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(25.6 211.6)\">\n",
       "<title>my_graph</title>\n",
       "<polygon fill=\"transparent\" stroke=\"transparent\" points=\"-4,4 -4,-190 587,-190 587,4 -4,4\"/>\n",
       "<!-- document_embeddings -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>document_embeddings</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"227,-137 108,-137 108,-101 227,-101 227,-137\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.5\" y=\"-116.5\" font-family=\"Arial\" font-size=\"10.00\">Document Embeddings</text>\n",
       "</g>\n",
       "<!-- ded_models -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ded_models</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"448,-186 327,-186 327,-150 448,-150 448,-186\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.5\" y=\"-165.5\" font-family=\"Arial\" font-size=\"10.00\">From Dedicated Models</text>\n",
       "</g>\n",
       "<!-- document_embeddings&#45;&gt;ded_models -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>document_embeddings-&gt;ded_models</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M227.21,-132.2C254.85,-138.41 288.01,-145.86 316.9,-152.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"316.41,-155.83 326.93,-154.61 317.94,-149 316.41,-155.83\"/>\n",
       "</g>\n",
       "<!-- from_token -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>from_token</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"450,-136 325,-136 325,-100 450,-100 450,-136\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.5\" y=\"-115.5\" font-family=\"Arial\" font-size=\"10.00\">From Token Embeddings</text>\n",
       "</g>\n",
       "<!-- document_embeddings&#45;&gt;from_token -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>document_embeddings-&gt;from_token</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M227.21,-118.73C254.05,-118.61 286.11,-118.46 314.4,-118.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.65,-121.83 324.64,-118.28 314.62,-114.83 314.65,-121.83\"/>\n",
       "</g>\n",
       "<!-- token_embeddings -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>token_embeddings</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"217.5,-87 117.5,-87 117.5,-51 217.5,-51 217.5,-87\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.5\" y=\"-66.5\" font-family=\"Arial\" font-size=\"10.00\">Token Embeddings</text>\n",
       "</g>\n",
       "<!-- contextual -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>contextual</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"448,-86 327,-86 327,-50 448,-50 448,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.5\" y=\"-65.5\" font-family=\"Arial\" font-size=\"10.00\">Contextual Embeddings</text>\n",
       "</g>\n",
       "<!-- token_embeddings&#45;&gt;contextual -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>token_embeddings-&gt;contextual</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.7,-68.77C246.82,-68.64 284.19,-68.47 316.36,-68.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"316.63,-71.82 326.61,-68.27 316.59,-64.82 316.63,-71.82\"/>\n",
       "</g>\n",
       "<!-- static -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>static</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"437,-36 338,-36 338,0 437,0 437,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.5\" y=\"-15.5\" font-family=\"Arial\" font-size=\"10.00\">Static Embeddings</text>\n",
       "</g>\n",
       "<!-- token_embeddings&#45;&gt;static -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>token_embeddings-&gt;static</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.7,-57.49C250.38,-49.84 293.47,-39.76 327.93,-31.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.92,-35.07 337.86,-29.38 327.33,-28.25 328.92,-35.07\"/>\n",
       "</g>\n",
       "<!-- embeddings -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>embeddings</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"71,-111 0,-111 0,-75 71,-75 71,-111\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.5\" y=\"-90.5\" font-family=\"Arial\" font-size=\"10.00\">Embeddings</text>\n",
       "</g>\n",
       "<!-- embeddings&#45;&gt;document_embeddings -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>embeddings-&gt;document_embeddings</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.34,-99.97C79.62,-101.62 88.75,-103.45 97.97,-105.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.48,-108.77 107.97,-107.29 98.85,-101.9 97.48,-108.77\"/>\n",
       "</g>\n",
       "<!-- embeddings&#45;&gt;token_embeddings -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>embeddings-&gt;token_embeddings</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.34,-86.57C82.49,-84.51 95.17,-82.17 107.56,-79.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108.19,-83.32 117.39,-78.07 106.92,-76.44 108.19,-83.32\"/>\n",
       "</g>\n",
       "<!-- sbert -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>sbert</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"583,-186 491,-186 487,-182 487,-150 579,-150 583,-154 583,-186\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"579,-182 487,-182 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"579,-182 579,-150 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"579,-182 583,-186 \"/>\n",
       "<text text-anchor=\"start\" x=\"499.5\" y=\"-171\" font-family=\"Arial\" font-size=\"10.00\">Sentence-BERT</text>\n",
       "<text text-anchor=\"start\" x=\"495\" y=\"-161\" font-family=\"Arial\" font-style=\"italic\" font-size=\"10.00\">on document level</text>\n",
       "</g>\n",
       "<!-- ded_models&#45;&gt;sbert -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>ded_models-&gt;sbert</title>\n",
       "<path fill=\"none\" stroke=\"orange\" d=\"M448.1,-168C457.55,-168 467.31,-168 476.71,-168\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"476.73,-171.5 486.73,-168 476.73,-164.5 476.73,-171.5\"/>\n",
       "</g>\n",
       "<!-- from_token&#45;&gt;token_embeddings -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>from_token-&gt;token_embeddings</title>\n",
       "<path fill=\"none\" stroke=\"gray\" stroke-dasharray=\"5,2\" d=\"M324.6,-104.09C294.17,-97.25 257.67,-89.04 227.62,-82.29\"/>\n",
       "<polygon fill=\"gray\" stroke=\"gray\" points=\"228.22,-78.84 217.7,-80.06 226.68,-85.67 228.22,-78.84\"/>\n",
       "<text text-anchor=\"middle\" x=\"276\" y=\"-103\" font-family=\"Arial\" font-size=\"10.00\">e.g. averaging</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbutils.plot_dot(\"miscellaneous/diagram_embeddings_1.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a5ecf",
   "metadata": {},
   "source": [
    "When focussing only on token embeddings, there are various options, as the diagram below illustrates (as in the diagram above, orange arrows indicate specific embeddings used in this notebook). The most recent option are contextual token embeddings (also sometimes called *dynamic* embeddings), which will incorporate a specific token's context and can be obtained from architectures like <a href=\"https://jalammar.github.io/illustrated-bert/\">ELMO or BERT</a>. Another option are static token embeddings, which map one token to one embedding, independent of its specific occurence in a text. For an overview of static and contextual embeddings, and their differences, see (Wang et al. 2020).\n",
    "\n",
    "For static embeddings there is now a variety of established options like <a href=\"https://fasttext.cc/\">fastText</a> or <a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe</a>. We can also combine embeddings or stack them (i.e. concatenate embedding vectors) to simply create new embeddings from existing ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4360e891-7da5-4617-aef1-6987ef4e3979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"542pt\" height=\"242pt\" viewBox=\"21.60 21.60 520.60 220.60\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(25.6 216.6)\">\n",
       "<title>my_graph</title>\n",
       "<polygon fill=\"transparent\" stroke=\"transparent\" points=\"-4,4 -4,-195 495,-195 495,4 -4,4\"/>\n",
       "<!-- token_embeddings -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>token_embeddings</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"100,-126 0,-126 0,-90 100,-90 100,-126\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-105.5\" font-family=\"Arial\" font-size=\"10.00\">Token Embeddings</text>\n",
       "</g>\n",
       "<!-- contextual -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>contextual</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"257,-171 136,-171 136,-135 257,-135 257,-171\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-150.5\" font-family=\"Arial\" font-size=\"10.00\">Contextual Embeddings</text>\n",
       "</g>\n",
       "<!-- token_embeddings&#45;&gt;contextual -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>token_embeddings-&gt;contextual</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.32,-123.36C109.24,-126.14 118.68,-129.08 128.02,-131.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.09,-135.36 137.68,-134.99 129.17,-128.68 127.09,-135.36\"/>\n",
       "</g>\n",
       "<!-- static -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>static</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"246,-101 147,-101 147,-65 246,-65 246,-101\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-80.5\" font-family=\"Arial\" font-size=\"10.00\">Static Embeddings</text>\n",
       "</g>\n",
       "<!-- token_embeddings&#45;&gt;static -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>token_embeddings-&gt;static</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.32,-99.47C112.08,-97.43 124.73,-95.24 136.86,-93.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.63,-96.56 146.89,-91.41 136.44,-89.67 137.63,-96.56\"/>\n",
       "</g>\n",
       "<!-- sbert -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>sbert</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"380,-191 297,-191 293,-187 293,-155 376,-155 380,-159 380,-191\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"376,-187 293,-187 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"376,-187 376,-155 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"376,-187 380,-191 \"/>\n",
       "<text text-anchor=\"start\" x=\"301\" y=\"-176\" font-family=\"Arial\" font-size=\"10.00\">Sentence-BERT</text>\n",
       "<text text-anchor=\"start\" x=\"306\" y=\"-166\" font-family=\"Arial\" font-style=\"italic\" font-size=\"10.00\">on token level</text>\n",
       "</g>\n",
       "<!-- contextual&#45;&gt;sbert -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>contextual-&gt;sbert</title>\n",
       "<path fill=\"none\" stroke=\"orange\" d=\"M257.23,-161.66C265.72,-162.89 274.4,-164.15 282.75,-165.36\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"282.33,-168.83 292.73,-166.8 283.34,-161.9 282.33,-168.83\"/>\n",
       "</g>\n",
       "<!-- glove -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>glove</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"363.5,-141 313.5,-141 309.5,-137 309.5,-105 359.5,-105 363.5,-109 363.5,-141\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"359.5,-137 309.5,-137 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"359.5,-137 359.5,-105 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"359.5,-137 363.5,-141 \"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-120.5\" font-family=\"Arial\" font-size=\"10.00\">GloVe</text>\n",
       "</g>\n",
       "<!-- static&#45;&gt;glove -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>static-&gt;glove</title>\n",
       "<path fill=\"none\" stroke=\"orange\" d=\"M246.15,-97.1C263.76,-102.21 283.31,-107.87 299.62,-112.6\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"298.73,-115.99 309.31,-115.41 300.68,-109.26 298.73,-115.99\"/>\n",
       "</g>\n",
       "<!-- fasttext -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>fasttext</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"480.5,-93 430.5,-93 426.5,-89 426.5,-57 476.5,-57 480.5,-61 480.5,-93\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"476.5,-89 426.5,-89 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"476.5,-89 476.5,-57 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"476.5,-89 480.5,-93 \"/>\n",
       "<text text-anchor=\"middle\" x=\"453.5\" y=\"-72.5\" font-family=\"Arial\" font-size=\"10.00\">fastText</text>\n",
       "</g>\n",
       "<!-- static&#45;&gt;fasttext -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>static-&gt;fasttext</title>\n",
       "<path fill=\"none\" stroke=\"orange\" d=\"M246.06,-81.48C295.7,-79.92 371.64,-77.54 416.26,-76.14\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"416.47,-79.63 426.35,-75.82 416.25,-72.64 416.47,-79.63\"/>\n",
       "</g>\n",
       "<!-- numberbatch -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>numberbatch</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"491,-36 420,-36 416,-32 416,0 487,0 491,-4 491,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"487,-32 416,-32 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"487,-32 487,0 \"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"487,-32 491,-36 \"/>\n",
       "<text text-anchor=\"middle\" x=\"453.5\" y=\"-15.5\" font-family=\"Arial\" font-size=\"10.00\">Numberbatch</text>\n",
       "</g>\n",
       "<!-- static&#45;&gt;numberbatch -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>static-&gt;numberbatch</title>\n",
       "<path fill=\"none\" stroke=\"orange\" d=\"M216.49,-64.9C234.55,-49.05 263.29,-27.15 293,-18 329.66,-6.71 373.34,-7.64 405.71,-10.87\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"405.48,-14.36 415.8,-11.98 406.25,-7.4 405.48,-14.36\"/>\n",
       "</g>\n",
       "<!-- stacked -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>stacked</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"351.5,-55 309.5,-55 309.5,-31 351.5,-31 363.5,-43 351.5,-55\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.5\" y=\"-40.5\" font-family=\"Arial\" font-size=\"10.00\">stacked</text>\n",
       "</g>\n",
       "<!-- static&#45;&gt;stacked -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>static-&gt;stacked</title>\n",
       "<path fill=\"none\" stroke=\"orange\" d=\"M246.15,-68.9C263.76,-63.79 283.31,-58.13 299.62,-53.4\"/>\n",
       "<polygon fill=\"orange\" stroke=\"orange\" points=\"300.68,-56.74 309.31,-50.59 298.73,-50.01 300.68,-56.74\"/>\n",
       "</g>\n",
       "<!-- stacked&#45;&gt;fasttext -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>stacked-&gt;fasttext</title>\n",
       "<path fill=\"none\" stroke=\"lightblue\" d=\"M363.6,-50.26C379.22,-54.61 399.3,-60.2 416.43,-64.96\"/>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"415.54,-68.35 426.11,-67.66 417.41,-61.6 415.54,-68.35\"/>\n",
       "</g>\n",
       "<!-- stacked&#45;&gt;numberbatch -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>stacked-&gt;numberbatch</title>\n",
       "<path fill=\"none\" stroke=\"lightblue\" d=\"M363.6,-37.33C376.09,-34.61 391.43,-31.28 405.83,-28.14\"/>\n",
       "<polygon fill=\"lightblue\" stroke=\"lightblue\" points=\"406.58,-31.56 415.61,-26.02 405.09,-24.72 406.58,-31.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbutils.plot_dot(\"miscellaneous/diagram_embeddings_2.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2820b9f",
   "metadata": {},
   "source": [
    "In this notebook, we showcase the following four classes of embeddings:\n",
    "\n",
    "* **Static token embeddings**: these operate on the token level. We experiment with <a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe</a> (Pennington et al. 2014), <a href=\"https://fasttext.cc/\">fastText</a> (Mikolov et al., 2018) and <a href=\"https://github.com/commonsense/conceptnet-numberbatch\">Numberbatch</a> (Speer et al, 2017). We use these three embeddings to compute token similarity and combine them with alignment algorithms (such as <a href=\"http://rna.informatik.uni-freiburg.de/Teaching/index.jsp?toolName=Waterman-Smith-Beyer\">Waterman-Smith-Beyer</a>) to compute document similarity. We also investigate the effect of stacking two static embeddings (fastText and Numberbatch).\n",
    "* **Contextual token embeddings**: these also operate on the token level, but embeddings can change according to a specific token instance's context. In this notebook we experiment with using such token embeddings from the <a href=\"https://www.sbert.net/\">Sentence-BERT</a> model (Reimers & Gurevych, 2019).\n",
    "* **Document embeddings derived from specially trained models**: document embeddings represent one document via one single embedding. We use document embeddings obtained from a BERT model. More specifically, we use a Siamese BERT model named Sentence-BERT, which is trained specifically for the semantic textual similarity (STS) task (Reimers & Gurevych, 2019).\n",
    "* **Document embeddings derived from token embeddings**: We also experiment with averaging different kinds of token embeddings (static and contextual) to derive document embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746071b",
   "metadata": {},
   "source": [
    "## 2.3 \"Shapespeare in the Vectorian Age\" – Meet the Vectorian framework <a class=\"anchor\" id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afbd3e",
   "metadata": {},
   "source": [
    "To conduct our actual investigations, we rely on a framework called the **<a href=\"https://github.com/poke1024/vectorian\">Vectorian</a>**, which we first introduced in 2020 (Liebl & Burghardt, 2020a/b). Using highly optimized algorithms and data structures, the Vectorian enables fast searching over the gold standard data using a variety of approaches and strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb0e38",
   "metadata": {},
   "source": [
    "In order to use the Vectorian, we need to map the gold standard concepts to Vectorian concepts as follows: From the gold standard, we want to gather all texts that contain some kind of text reuse. These texts reside in the `context` attribute of the `Evidence` instances (see Figure above). In the Vectorian, we then create `Documents` from those texts. A `Document` in Vectorian terminology is something we can perform a search on. `Documents` in the Vectorian are created using different kinds of `Importers` that perform necessary natural language processing tasks using an additional `NLP` class (see diagram below). Since this step can be very time-consuming, we precomputed this step and use the `Corpus` class to quickly load these preprocessed Documents into the notebook. For details about the full preprocessing, see `code/prepare_corpus.ipynb`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467944b",
   "metadata": {},
   "source": [
    "Using the loaded `Documents` and a set of `Embeddings` we want to work with, we can then create a `Session` that allows us to perform intertextuality searches. More details about\n",
    "the technical architecture be build on in this notebook can be found in the [source code](https://github.com/poke1024/vectorian) and the [API Documentation](https://poke1024.github.io/vectorian/index.html) for the Vectorian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62e898",
   "metadata": {},
   "source": [
    "### 2.3.1 Loading word embeddings <a class=\"anchor\" id=\"section_2_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9406706",
   "metadata": {},
   "source": [
    "In this step the static embeddings that were described above are loaded from Vectorian's model zoo. This zoo contains a number of prebuilt embeddings for various languages, for instance GloVe, fastText and Numberbatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994c5c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fasttext-af',\n",
       " 'fasttext-ba',\n",
       " 'fasttext-bs',\n",
       " 'fasttext-de',\n",
       " 'fasttext-fa',\n",
       " 'fasttext-gv',\n",
       " 'fasttext-id',\n",
       " 'fasttext-kn',\n",
       " 'fasttext-mai',\n",
       " 'fasttext-mt',\n",
       " 'fasttext-nl',\n",
       " 'fasttext-pl',\n",
       " 'fasttext-sah',\n",
       " 'fasttext-sq',\n",
       " 'fasttext-tl',\n",
       " 'fasttext-vo',\n",
       " 'glove-6B-50',\n",
       " 'numberbatch-19.08-eo',\n",
       " 'numberbatch-19.08-io',\n",
       " 'numberbatch-19.08-oc',\n",
       " 'numberbatch-19.08-vi')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vectorian.embeddings import Zoo\n",
    "\n",
    "Zoo.list()[::20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a5d4b",
   "metadata": {},
   "source": [
    "For reasons of limited RAM in the interactive Binder environment (and to limit download times), we use smaller or compressed versions of the static embeddings:\n",
    "\n",
    "* for **<a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\">GloVe</a>**, we use the official 50-dimensional version of the 6B variant\n",
    "* for **<a href=\"https://fasttext.cc/docs/en/crawl-vectors.html\" target=\"_blank\">fastText</a>** we use a version that was trained on *Common Crawl* and *Wikipedia* using CBOW, and then compressed using the standard settings in https://github.com/avidale/compress-fasttext\n",
    "* for **<a href=\"https://github.com/commonsense/conceptnet-numberbatch\" target=\"_blank\">Numberbatch</a>** we use version 19.08 that was reduced into a 50-dimension version using a standard PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1bc8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_embeddings = {}\n",
    "\n",
    "the_embeddings[\"glove\"] = Zoo.load(\"glove-6B-50\")\n",
    "the_embeddings[\"fasttext\"] = Zoo.load(\"fasttext-en-mini\")\n",
    "\n",
    "if nbutils.running_inside_binder():  # use precomputed version of Numberbatch?\n",
    "    the_embeddings[\"numberbatch\"] = nbutils.download_word2vec_embedding(\n",
    "        \"data/raw_data/numberbatch-19.08-en-pca-50\",\n",
    "        \"https://zenodo.org/record/4916056/files/numberbatch-19.08-en-pca-50.zip\",\n",
    "    )\n",
    "else:\n",
    "    # The following reduction of full Numberbatch to n=50 only works in envs\n",
    "    # with enough memory. For Binder etc. use the Zenodo version above.\n",
    "    the_embeddings[\"numberbatch\"] = Zoo.load(\"numberbatch-19.08-en\").pca(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae47fc7",
   "metadata": {},
   "source": [
    "We also use one **stacked embedding**, in which we combine fastText and Numberbatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d9f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import StackedEmbedding\n",
    "\n",
    "the_embeddings[\"fasttext_numberbatch\"] = StackedEmbedding(\n",
    "    [the_embeddings[\"fasttext\"], the_embeddings[\"numberbatch\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04037dac",
   "metadata": {},
   "source": [
    "Next, we instantiate an NLP parser that is able to provide embeddings based on Sentence-BERT (Reimers & Gurevych, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dd961f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f19f00a0860>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f19f0a20cc0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f19f0147760>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f19f0420040>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f19f010c180>),\n",
       " ('sentence_bert',\n",
       "  <spacy_sentence_bert.language.SentenceBert at 0x7f1b011b4fa0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = nbutils.make_nlp(\"en_paraphrase_distilroberta_base_v1\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ec251",
   "metadata": {},
   "source": [
    "Finally, we add a wrapper that allows us to use Sentence-BERT's contextual token embeddings in the Vectorian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ef1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import SpacyVectorEmbedding, VectorCache\n",
    "\n",
    "the_embeddings[\"sbert\"] = SpacyVectorEmbedding(\n",
    "    nlp, 768, cache=VectorCache(\"data/processed_data/sbert_contextual\", readonly=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d16656",
   "metadata": {},
   "source": [
    "### 2.3.2 Creating the session <a class=\"anchor\" id=\"section_2_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c3ada",
   "metadata": {},
   "source": [
    "The Vectorian `Session` is created with the specified embeddings and the preprocessed documents, which are loaded via the `Corpus` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2ab856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d2d342795249b8bf68d0dda1fe631b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Opening Corpus:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b9ffac03fe4bbe9fab2f0103b97858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing Documents:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da456e9635b42a78d3aacf667f4db99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Vectors:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectorian.session import LabSession\n",
    "from vectorian.corpus import Corpus\n",
    "\n",
    "corpus = Corpus(\"data/processed_data/corpus\", mutable=False)\n",
    "\n",
    "session = LabSession(\n",
    "    corpus,\n",
    "    embeddings=the_embeddings.values())\n",
    "\n",
    "# the following command loads all contextual embedding vectors into RAM.\n",
    "# while not necessary, this speeds up some of the ensuing computations.\n",
    "session.cache_contextual_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24786e14",
   "metadata": {},
   "source": [
    "The session now contains all embeddings we will work with as well as the list of documents that contain the texts from the gold standard `Evidence` items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09829d8-c158-43b3-be08-1a76f343d3bf",
   "metadata": {},
   "source": [
    "# 3. Embeddings as a tool for intertextuality research <a class=\"anchor\" id=\"section_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52e6c6-f0c6-47ff-934b-ec5d8d2c51e0",
   "metadata": {},
   "source": [
    "## 3.1  Exploring word embeddings <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c309d",
   "metadata": {},
   "source": [
    "### 3.1.1 An introduction to word embeddings and token similarity <a class=\"anchor\" id=\"section_3_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bfeb6",
   "metadata": {},
   "source": [
    "Before we dive into the acutal intertextuality analyses, we first take a brief look at the inner workings of embeddings. Mathematically speaking, a word embedding is a vector **x** of dimension *n*, i.e. a vector consisting of *n* scalars.\n",
    "\n",
    "$$\\mathbf{x}=(x_1, x_2, ..., x_{n-1}, x_n)$$\n",
    "\n",
    "For example, the compressed numberbatch embedding we use has *n*=50 and thus represents the word \"coffee\" with the following 50 scalar values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474c8b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ca2bee0a6d40a9aa3bc1aa8856890e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Label(value='-0.05'), Label(value='-0.13'), Label(value='0.06'), Label(value='-0.09'), Label…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.GridBox(\n",
    "    [\n",
    "        widgets.Label(f\"{x:.2f}\")\n",
    "        for x in session.word_vec(the_embeddings[\"numberbatch\"], \"coffee\")\n",
    "    ],\n",
    "    layout=widgets.Layout(grid_template_columns=\"repeat(10, 50px)\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea3512",
   "metadata": {},
   "source": [
    "Since the above representation is difficult to understand, we visualize the values of\n",
    "\n",
    "$$x_1, x_2, ..., x_{n-1}, x_n$$\n",
    "\n",
    "through different colors. By default, all values are normalized by ||**x**||&#x2082;, i.e. the dot product of these values gives the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea9c7758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b467a3c7db4a3b8a93aea47dda556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='embedding', index=2, options=(('glove', <vectorian.embeddings.Word…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    embedding=widgets.Dropdown(\n",
    "        options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "        value=the_embeddings[\"numberbatch\"],\n",
    "    ),\n",
    "    normalize=True,\n",
    ")\n",
    "def plot(embedding, normalize):\n",
    "    nbutils.plot_embedding_vectors_val(\n",
    "        [\"sail\", \"boat\", \"coffee\", \"tea\", \"guitar\", \"piano\"],\n",
    "        get_vec=lambda w: session.word_vec(embedding, w),\n",
    "        normalize=normalize,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f2826",
   "metadata": {},
   "source": [
    "By looking at these color patterns, we can gain some intuitive understanding of why and how word embeddings are appropriate for word similarity calculations. For example, *sail* and *boat* both show a strong activation for dimension 27. Similarly, *guitar* and *piano* share similar values for dimension 24. The words *coffee* and *tea* also share some similar patterns for dimension 2 and dimension 49, which also set them apart from the other four words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef6760",
   "metadata": {},
   "source": [
    "A common approach to compute the similarity between two word vectors **u** and **v** in this kind of high-dimensional vector spaces is to compute the cosine of the angle &theta; between the vectors, which is called **cosine similarity**:\n",
    "\n",
    "$$cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{||\\mathbf{u}||_2 ||\\mathbf{v}||_2} = \\frac{\\sum_1^n \\mathbf{u}_i \\mathbf{v}_i}{\\sqrt{\\sum_1^n \\mathbf{u}_i^2} \\sqrt{\\sum_1^n \\mathbf{v}_i^2}} = \\sum_1^n \\left( \\frac{\\mathbf{u}}{||\\mathbf{u}||_2} \\right)_i \\left( \\frac{\\mathbf{v}}{||\\mathbf{v}||_2} \\right)_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8876b1",
   "metadata": {},
   "source": [
    "A large positive value (i.e. a small &theta; between **u** and **v**) indicates higher similarity, whereas a small or even negative value (i.e. a large &theta;) indicates lower similarity. For a discussion of issues with this notion of similarity, see Faruqui et al. (2016).\n",
    "\n",
    "The visualization below encodes\n",
    "\n",
    "$$\\left( \\frac{\\mathbf{u}}{||\\mathbf{u}||_2} \\right)_i \\left( \\frac{\\mathbf{v}}{||\\mathbf{v}||_2} \\right)_i$$\n",
    "\n",
    "for different i, 1 &le; i &le; n, through colors to illustrate how different components contribute to the cosine similarity for two words. Brighter colors (orange/yellow) indicate dimensions with higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e2ade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9920bc774f9e41978f23c0feed2943be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='embedding', index=2, options=(('glove', <vectorian.embeddings.Word…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    embedding=widgets.Dropdown(\n",
    "        options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "        value=the_embeddings[\"numberbatch\"],\n",
    "    )\n",
    ")\n",
    "def plot(embedding):\n",
    "    nbutils.plot_embedding_vectors_mul(\n",
    "        [(\"sail\", \"boat\"), (\"coffee\", \"tea\"), (\"guitar\", \"piano\")],\n",
    "        get_vec=lambda w: session.word_vec(embedding, w),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff150311",
   "metadata": {},
   "source": [
    "A comparable investigation of fastText shows similar spots of positive dimensions. The plot here is somewhat more complex due to the higher number of dimensions (*n* = 300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc1d2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9490ca3f7be4536a3697bbb4527f752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='embedding', index=1, options=(('glove', <vectorian.embeddings.Word…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    embedding=widgets.Dropdown(\n",
    "        options=[(k, v) for k, v in the_embeddings.items() if not v.is_contextual],\n",
    "        value=the_embeddings[\"fasttext\"],\n",
    "    )\n",
    ")\n",
    "def plot(embedding):\n",
    "    nbutils.plot_embedding_vectors_mul(\n",
    "        [(\"sail\", \"boat\"), (\"coffee\", \"tea\"), (\"guitar\", \"piano\")],\n",
    "        get_vec=lambda w: session.word_vec(embedding, w),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c6ff5",
   "metadata": {},
   "source": [
    "Computing the overall cosine similarity for two words is mathematically equivalent to summing up the terms in the diagram above. The overall similarity between *guitar* and *piano* is approx. 68% with the fastText embedding we use. For *guitar* and *coffee* it is significantly lower with a similarity of approx. 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9d13ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.68097234, 0.19857685]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vectorian.metrics import TokenSimilarity, CosineSimilarity\n",
    "\n",
    "token_sim = TokenSimilarity(the_embeddings[\"fasttext\"], CosineSimilarity())\n",
    "\n",
    "[session.similarity(token_sim, \"guitar\", x) for x in [\"piano\", \"coffee\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea91e0",
   "metadata": {},
   "source": [
    "To compute the similarity between tokens of a contextual embedding, we need to reference the actual token instances within a text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45a1647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spoke', 'her', 0.47414798]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sim = TokenSimilarity(the_embeddings[\"sbert\"], CosineSimilarity())\n",
    "\n",
    "a = list(session.documents[0].spans(session.partition(\"document\")))[0][2]\n",
    "b = list(session.documents[6].spans(session.partition(\"document\")))[0][5]\n",
    "[a.text, b.text, session.similarity(token_sim, a, b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d77e8",
   "metadata": {},
   "source": [
    "### 3.1.2 Detecting Shakespearean intertextuality through word embeddings <a class=\"anchor\" id=\"section_3_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ebd8b",
   "metadata": {},
   "source": [
    "We explore the usefulness of embeddings and token similarity with the gold standard dataset that was introduced earlier. In the following example the pattern \"the rest is silence\" is quoted as \"the rest is all but wind\". While the syntactic structure is mirrored between pattern and occurrence, the term \"silence\" is replaced with \"all but wind\". If we focus on nouns only, we can expect \"silence\" and \"wind\" to be semantically related - at least to a certain degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53cb1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = nbutils.TokenSimPlotterFactory(session, nlp, gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08494bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beca2177f830490a8a858fe3ca5038d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='pattern:', index=3, layout=Layout(width='max-content'), op…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter1 = vis.make(\"rest is silence\", \"Fig for Fortune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2a451",
   "metadata": {},
   "source": [
    "In the following, we inspect the cosine similarity of the token \"silence\" with other tokens in the document's (\"A Fig of Fortune, 1596\") context for three different embedding models.It becomes clear that for all three embeddings there is a strong connection between \"silence\" and \"wind\". The cosine similarity is particularly high with the Numberbatch model. Nevertheless, the absolute value of 0.3 for Numberbatch is still in a rather low range. Interestingly, GloVe associates \"silence\" with \"action\", which can be understood as quite the opposite of silence. The phenomenon that embeddings sometimes cluster opposites is a common observation and can be a problem when trying to distinguish between synonyms and antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3df8a4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1536\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:33787/autoload.js?bokeh-autoload-element=1536&bokeh-absolute-url=http://localhost:33787&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "6cb9daf37e354dafa4a1abc2ed0677e9"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter1(\"silence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d78987",
   "metadata": {},
   "source": [
    "In an example for the pattern \"sea of troubles\", we see that the word \"sea\" in one document is paraphrased as \"waves\", and \"troubles\" is substituted by \"troublesome\". If we take a closer look at the cosine similarities of the tokens \"sea\" and \"troubles\" with all the other tokens in the document's context, we see that they are – expectedly – rather high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fb62321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff3d8b71deb489fadca7ca03463146d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='pattern:', index=1, layout=Layout(width='max-content'), op…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter2 = vis.make(\"sea of troubles\", \"Book of Common Prayer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f7b8069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1540\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:33703/autoload.js?bokeh-autoload-element=1540&bokeh-absolute-url=http://localhost:33703&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "9ec64c38ca8442c3a6f75065f03a9e5e"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter2(\"sea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a3192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1544\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:36847/autoload.js?bokeh-autoload-element=1544&bokeh-absolute-url=http://localhost:36847&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "1506d7292f7e4a3b9bc4054328e4801f"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter2(\"troubles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee2f24",
   "metadata": {},
   "source": [
    "It is also interesting to investigate how out-of-vocabulary words like \"troublesomest\" produce zero similarities with standard key-value embeddings, whereas fastText is still able to produce a vector thanks to subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac79d82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1548\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:33741/autoload.js?bokeh-autoload-element=1548&bokeh-absolute-url=http://localhost:33741&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "d0095f1af87943829d4bceeb2cba1f57"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter2(\"troublesomest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e6051",
   "metadata": {},
   "source": [
    "## 3.2 Exploring document embeddings <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22375ca1",
   "metadata": {},
   "source": [
    "Next, we consider the representation of each document with a single embedding to gain an understanding of how different embedding strategies relate to document similarity. We will later return to individual token embeddings.\n",
    "\n",
    "For this purpose, we will use the two strategies already mentioned for computing document embeddings:\n",
    "\n",
    "* averaging over token embeddings\n",
    "* computing document embeddings through a dedicated model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb44d3",
   "metadata": {},
   "source": [
    "In order to achieve the latter, we compute document embeddings using Sentence-BERT. The `CachedPartitionEncoder` in the following code will encode and cache the document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f78d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorian.embeddings import CachedPartitionEncoder, SpanEncoder\n",
    "\n",
    "# create an encoder that basically calls nlp(t).vector\n",
    "sbert_encoder = CachedPartitionEncoder(\n",
    "    SpanEncoder(lambda texts: [nlp(t).vector for t in texts])\n",
    ")\n",
    "\n",
    "# compute encodings and/or save cached data\n",
    "sbert_encoder.try_load(\"data/processed_data/doc_embeddings\")\n",
    "sbert_encoder.cache(session.documents, session.partition(\"document\"))\n",
    "sbert_encoder.save(\"data/processed_data/doc_embeddings\")\n",
    "\n",
    "# extract name of encoder for later use\n",
    "sbert_encoder_name = nlp.meta[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323d646",
   "metadata": {},
   "source": [
    "We first present a strategy that creates embeddings for a document using the Sentence-BERT encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a3a06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1551\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:45237/autoload.js?bokeh-autoload-element=1551&bokeh-absolute-url=http://localhost:45237&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "d94fb1bedd8b4662807214a8f9752e9b"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder = nbutils.DocEmbedder(\n",
    "    session=session,\n",
    "    nlp=nlp,\n",
    "    doc_encoders={sbert_encoder_name: sbert_encoder},\n",
    "    encoder=\"paraphrase_distilroberta\",\n",
    ")\n",
    "embedder.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0755a1",
   "metadata": {},
   "source": [
    "Similar to the investigation of token embedding values, we now look at the feature dimensions of the document embeddings. In the following plot we observe that the pattern \"an old man is twice a child\" and the corresponding text reuses from the gold standard (i.e. the true positives) show some salient contribution around dimensions 25 and 300 (see the 5 upper rows). When comparing the same pattern with non-matching text reuse occurences from the \"go, by Saint Hieronimo\" pattern on the other hand (see the 5 lower rows), there is less activation in these areas. Therefore these areas seem to offer some good features to differentiate the matching of a pattern with the correct occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d2e9473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78a729017a41d0b4aeb72c3dabd83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='pattern', index=4, layout=Layout(width='max-content'), options=('t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bars = nbutils.DocEmbeddingBars(embedder, session, gold_data)\n",
    "bars.plot(\"an old man is twice a child\", \"Saint Hieronimo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59876d",
   "metadata": {},
   "source": [
    "Instead of focusing on only one pattern, we now look at a plot of the embeddings of all documents in our gold standard data. The plot uses a dimensionality reduction technique known as t-Distributed Stochastic Neighbor Embedding (t-SNE) and allows us to reduce multiple dimensions to just two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3a0c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1701\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:44879/autoload.js?bokeh-autoload-element=1701&bokeh-absolute-url=http://localhost:44879&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "b4f7ec91217b499d89443963730455a9"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_embedding_explorer = nbutils.DocEmbeddingExplorer(\n",
    "    session=session,\n",
    "    nlp=nlp,\n",
    "    gold=gold_data,\n",
    "    doc_encoders={sbert_encoder_name: sbert_encoder},\n",
    ")\n",
    "\n",
    "doc_embedding_explorer.plot(\n",
    "    [\n",
    "        {\n",
    "            \"encoder\": \"paraphrase_distilroberta\",\n",
    "            \"locator\": (\"fixed\", \"carry coals\")\n",
    "        },\n",
    "        {\n",
    "            \"encoder\": \"paraphrase_distilroberta\",\n",
    "            \"locator\": (\"fixed\", \"an old man is twice\"),\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dee41",
   "metadata": {},
   "source": [
    "In the t-SNE visualization above, the dots represent documents and the colors represent the query that results in this document in our gold standard (more details on the underlying documents are shown when hovering the mouse cursor over the nodes). Dots that are close to each other indicate that the underlying documents share a certain similarity. Nearby dots of the same color indicate that the embedding tends to cluster documents similar to our gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d40738",
   "metadata": {},
   "source": [
    "In the left plot, we searched for the phrase \"we will not carry coals\" (visualized as large yellow circle with a cross). The plot shows that the query is in fact part of a document cluster (smaller green-yellow circles) that contains a variation of that phrase. Similarly, on the right we see that the phrase \"an old man is twice a child\" clusters with the actual (green) documents we associate with it in our gold standard.\n",
    "\n",
    "For these phrases and documents, the `paraphrase_distilroberta` model automatically produces a document embedding that actually recognizes and separates inherent structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dd3d5",
   "metadata": {},
   "source": [
    "In the plot above we looked at the document embedding produced by a **token-based** embedding. This has the advantage that we can actually look at token embeddings that make up the document embedding (through averaging), which we will do in the following plot. On the right side, we see a t-SNE plot of all the token embeddings that occur in the documents that are selected on the left. This visualization makes more transparent why certain documents on the left are clustered to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13942a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1704\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:42117/autoload.js?bokeh-autoload-element=1704&bokeh-absolute-url=http://localhost:42117&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "b3c5febbac604e919974f6d323d0745a"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_embedding_explorer.plot(\n",
    "    [\n",
    "        {\n",
    "            \"encoder\": \"numberbatch\",\n",
    "            \"selection\": [\n",
    "                \"ww_32c26a7909c83bda\",\n",
    "                \"ww_b5b8083a6a1282bc\",\n",
    "                \"ww_9a6cb20b0b157545\",\n",
    "                \"ww_a6f4b0e3428ad510\",\n",
    "                \"ww_8e68a517bc3ecceb\",\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cda845",
   "metadata": {},
   "source": [
    "The red circles on the left represent contexts that match the phrase \"a horse, a horse, my kingdom for a horse\". When we look at the token embeddings in the right plot (that includes other documents as well), we  see that a group is formed due to word embeddings that cluster around \"horse\", but we also see a cluster around \"boat\", \"sail\" and \"river\" on the left. In fact document 1 contains \"muscle boat\", document 2 contains \"To swim the river villain\", and document 3 contains \"A boat, a boat\". We see that this kind of unsupervised document clustering groups items due to inherent qualities that might not actually match the initial query criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5041ac3",
   "metadata": {},
   "source": [
    "Custom token embedding plots can be generated by selecting different documents from the left plot (drag the mouse to lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10659f",
   "metadata": {},
   "source": [
    "## 3.3 Exploring word mappings: WSB vs. WMD <a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c044ad9",
   "metadata": {},
   "source": [
    "So far, we have experimented with different token embeddings and seen how similarity comparison can be implemented for single tokens. However, for the detection of intertextual references it is necessary to compare longer token sequences with each other. The problem here is to identify the right segment in the target text, because a quotation like \"to be or not to be\" will occur as a local phenomenon, only at a certain position in a document. The rest of the document will likely be sentences that do not match with the quote phrase at all. To identify the segment where a potential quote occurs, there are different approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16977478",
   "metadata": {},
   "source": [
    "One popular class of techniques are sequence alignment algorithms as well as adjacent approaches like Dynamic Time Warping, see Kruskal (1983). In this section, we introduce the **<a href=\"http://rna.informatik.uni-freiburg.de/Teaching/index.jsp?toolName=Waterman-Smith-Beyer\" target=\"_blank\">Waterman-Smith-Beyer</a> (WSB)** algorithm, which produces optimal local alignments and provides a general (e.g. non-affine) cost function (Waterman, Smith & Beyer, 1974). Other commonly used global alignment algorithms - such as <a href=\"http://rna.informatik.uni-freiburg.de/Teaching/index.jsp?toolName=Smith-Waterman\" target=\"_blank\">Smith-Waterman</a> and <a href=\"http://rna.informatik.uni-freiburg.de/Teaching/index.jsp?toolName=Gotoh\" target=\"_blank\">Gotoh</a> - can be regarded as special cases of WSB. In comparison to the popular Needleman-Wunsch global alignment algorithm, WSB produces local alignments. In contrast to classic formulations of WSB - which often use a fixed substitution cost - we use the word distance from word embeddings to compute the substitution penalty for specific pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03983ce2",
   "metadata": {},
   "source": [
    "Another approach to compute a measure of similarity between bags of words is the so-called **<a href=\"https://nbviewer.jupyter.org/github/vene/vene.github.io/blob/pelican/content/blog/word-movers-distance-in-python.ipynb\" target=\"_blank\">Word Mover's Distance</a>** introduced by Kusner et al. (2015). The main idea is computing similarity through finding the optimal solution of a transportation problem between words.\n",
    "\n",
    "In the following, we will actually experiment with two variants of the WMD. In addition to the classic WMD, where a transportation problem is solved over the normalized bag of words (nbow) vector, we also introduce a new variant of WMD. In this new variant, we keep the bag of words (bow) unnormalized, i.e. we pose the transportation problem on absolute word occurrence counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e18bd",
   "metadata": {},
   "source": [
    "Note: document embeddings do not need any of the above techniques, as they embed documents into a vector space in a way that queries and target documents that share similar features are close to each other in that space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b6557",
   "metadata": {},
   "source": [
    "### 3.3.1 Mapping quote queries to longer text documents <a class=\"anchor\" id=\"section_3_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f33423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_builder(**kwargs):\n",
    "    return nbutils.InteractiveIndexBuilder(\n",
    "        session, nlp, partition_encoders={sbert_encoder_name: sbert_encoder}, **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3448bfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d4c93b7caa41fbaf99955bccc40fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<span style=\"line-height:normal;\"><p>Partition similarity is computed via <b>alignme…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_builder = make_index_builder()\n",
    "index_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a45d8",
   "metadata": {},
   "source": [
    "What can be seen above is the description of a search strategy that we will employ in the following sections of this notebook. By switching to the \"Edit\" part, it is possible to explore the settings in more detail and even change them to something completely different. Note that various parameters in the \"Edit\" GUI - e.g. mixing of embeddings - are beyond the scope of this notebook. For for more details see Liebl & Burghardt (2020a/b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeab1a4",
   "metadata": {},
   "source": [
    "Example: For the pattern \"to be or not to be\" we find the following top matches (\"to be named or not to be named\"), with a similarity score of 96.6%. By increasing the n value we can always display more ranked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cad0598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to be or not to be'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data.patterns[0].phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e79f163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t<iframe\n",
       "\t\tid=\"vectorian-1631187562631220722-139748625099504-1\"\n",
       "\t\twidth=\"100%\"\n",
       "\t\theight=\"100%\"\n",
       "\t\tsrcdoc=\"&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css&quot; /&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;container&quot; height=&quot;100%&quot;&gt;&lt;div class=&quot;section&quot;&gt;&lt;article class=&quot;media&quot;&gt;&lt;div class=&quot;media-left&quot;&gt;&lt;p class=&quot;image is-64x64&quot;&gt;&lt;span class=&quot;buttons&quot;&gt;&lt;span class=&quot;has-text-weight-bold&quot;&gt;91.4%&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;media-content&quot;&gt;&lt;div class=&quot;is-pulled-right&quot;&gt;&lt;small&gt;Lording Barry&lt;/small&gt;&lt;small class=&quot;is-italic&quot;&gt;, Ram Alley (1608)&lt;/small&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;span class=&quot;has-text-grey-light&quot;&gt;Sir Oliver Smallshank:] We &lt;/span&gt; &lt;span&gt;&lt;span style=&quot;display:inline-table;&quot;&gt;&lt;span style=&quot;display:table-row;&quot;&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;has-text-black has-text-weight-bold&quot;&gt;old&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;tag is-light&quot;&gt;old&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell; opacity:1.0;&quot;&gt;&lt;span class=&quot;tag is-success&quot;&gt;100%&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;display:inline-table;&quot;&gt;&lt;span style=&quot;display:table-row;&quot;&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;has-text-black has-text-weight-bold&quot;&gt;men&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;tag is-light&quot;&gt;men&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell; opacity:1.0;&quot;&gt;&lt;span class=&quot;tag is-success&quot;&gt;100%&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span style=&quot;display:inline-table;&quot;&gt;&lt;span style=&quot;display:table-row;&quot;&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;has-text-black has-text-weight-bold&quot;&gt;have&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;tag is-light&quot;&gt;have&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell; opacity:1.0;&quot;&gt;&lt;span class=&quot;tag is-success&quot;&gt;100%&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;has-text-grey-light&quot;&gt;our &lt;/span&gt; &lt;span&gt;&lt;span style=&quot;display:inline-table;&quot;&gt;&lt;span style=&quot;display:table-row;&quot;&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;has-text-black has-text-weight-bold&quot;&gt;crotchets&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell;&quot;&gt;&lt;span class=&quot;tag is-light&quot;&gt;crotchet&lt;/span&gt; &lt;/span&gt;&lt;span style=&quot;display:table-cell; opacity:1.0;&quot;&gt;&lt;span class=&quot;tag is-success&quot;&gt;86%&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;has-text-grey-light&quot;&gt;our conundrums, Our fegares, quirks and quibibles, As well as &lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/article&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script&gt;\n",
       "\t\t(function() {\n",
       "\t\t\tvar f = parent.document.getElementById(&#x27;vectorian-1631187562631220722-139748625099504-1&#x27;);\n",
       "\t\t\tf.height = f.contentWindow.document.body.scrollHeight + &#x27;px&#x27;;\n",
       "\t\t})();\n",
       "\t&lt;/script&gt;\"\n",
       "\t\tonload=\"\n",
       "\t\t(function() {\n",
       "\t\t\tvar f = parent.document.getElementById('vectorian-1631187562631220722-139748625099504-1');\n",
       "\t\t\tf.height = f.contentWindow.document.body.scrollHeight + 'px';\n",
       "\t\t})();\n",
       "\t\"\n",
       "\t\tframeborder=\"0\"\n",
       "\t\tallowfullscreen\n",
       "\t></iframe>\n",
       "\t"
      ],
      "text/plain": [
       "<vectorian.session.LabResult at 0x7f19c3297c70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_builder.build_index().find(\"old men have crotchet\", n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e87a9",
   "metadata": {},
   "source": [
    "### 3.3.2 Evaluation: Plotting the nDCG over the corpus <a class=\"anchor\" id=\"section_3_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b2b2f",
   "metadata": {},
   "source": [
    "In the following we will systematically evaluate different strategies for identifying intertextuality in our gold standard data. We investigate WSB and the two variants of WMD (bow vs. nbow) in combination with the fastText embedding. As another variant we evaluate the performance of Sentence-BERT document embeddings. The evaluation metric is **normalized discounted cumulative gain** [(nDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (also see Liebl & Burghardt, 2020b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22449b9",
   "metadata": {},
   "source": [
    "In the summary below you will find more detailed descriptions of the search strategies that will be evaluated in the following. By using \"Edit\", it is possible to change these settings to something else (a rerun of the following sections of the notebook would then be necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "203ab1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee1c723a112427591e2483f3873539d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Tab(children=(HTML(value='<span style=\"line-height:normal;\"><p>Partition similarity is com…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# define 4 different search stratgies via make_index_builder \n",
    "index_builders = collections.OrderedDict(\n",
    "    {\n",
    "        \"wsb\": make_index_builder(\n",
    "            strategy=\"Alignment\",\n",
    "            strategy_options={\n",
    "                \"alignment\": vectorian.alignment.LocalAlignment(\n",
    "                    gap={\n",
    "                        \"s\": vectorian.alignment.smooth_gap_cost(5),\n",
    "                        \"t\": vectorian.alignment.smooth_gap_cost(5)\n",
    "                    }\n",
    "                )\n",
    "            },\n",
    "        ),\n",
    "        \"wmd nbow\": make_index_builder(\n",
    "            strategy=\"Alignment\",\n",
    "            strategy_options={\n",
    "                \"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"nbow\")\n",
    "            },\n",
    "        ),\n",
    "        \"wmd bow\": make_index_builder(\n",
    "            strategy=\"Alignment\",\n",
    "            strategy_options={\n",
    "                \"alignment\": vectorian.alignment.WordMoversDistance.wmd(\"bow\")\n",
    "            },\n",
    "        ),\n",
    "        \"doc embedding\": make_index_builder(strategy=\"Partition Embedding\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# present UI of various options that allows for editing\n",
    "accordion = widgets.Accordion(children=[x.displayable for x in index_builders.values()])\n",
    "for i, k in enumerate(index_builders.keys()):\n",
    "    accordion.set_title(i, k)\n",
    "accordion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bf126",
   "metadata": {},
   "source": [
    "With the following command we will get an overview of the quality of the results we obtain when using the index configures with `index_builder` by computing the nDCG over the 20 queries in our gold standard with regard to the known optimal results (this may take a few seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d34c972d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde1e32adeb1432f957bb7f9d5bb4175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"716bb90e-8886-4357-a384-494dbc5b30fc\" data-root-id=\"1726\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"5c1fcd87-dadb-42d8-9118-da4387913bf5\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1736\"}],\"center\":[{\"id\":\"1739\"},{\"id\":\"1742\"},{\"id\":\"1752\"}],\"height\":1600,\"left\":[{\"id\":\"1740\"}],\"renderers\":[{\"id\":\"1750\"}],\"title\":{\"id\":\"1727\"},\"toolbar\":{\"id\":\"1743\"},\"toolbar_location\":null,\"width\":1000,\"x_range\":{\"id\":\"1744\"},\"x_scale\":{\"id\":\"1732\"},\"y_range\":{\"id\":\"1725\"},\"y_scale\":{\"id\":\"1734\"}},\"id\":\"1726\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"formatter\":{\"id\":\"1815\"},\"group_label_orientation\":0,\"major_label_policy\":{\"id\":\"1813\"},\"ticker\":{\"id\":\"1741\"}},\"id\":\"1740\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"1745\"},\"glyph\":{\"id\":\"1748\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1749\"},\"view\":{\"id\":\"1751\"}},\"id\":\"1750\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"level\":\"glyph\",\"source\":{\"id\":\"1745\"},\"text\":{\"field\":\"ndcg_str\"},\"text_align\":{\"value\":\"right\"},\"text_baseline\":{\"value\":\"middle\"},\"text_color\":{\"value\":\"white\"},\"text_font_size\":{\"value\":\"8pt\"},\"x\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"1752\",\"type\":\"LabelSet\"},{\"attributes\":{},\"id\":\"1820\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\",\"transform\":{\"id\":\"1746\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\",\"transform\":{\"id\":\"1746\"}},\"right\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"1749\",\"type\":\"HBar\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\",\"transform\":{\"id\":\"1746\"}},\"line_color\":{\"field\":\"color\",\"transform\":{\"id\":\"1746\"}},\"right\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"1748\",\"type\":\"HBar\"},{\"attributes\":{},\"id\":\"1816\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1813\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1815\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1819\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"1745\"}},\"id\":\"1751\",\"type\":\"CDSView\"},{\"attributes\":{\"factors\":[[\"O all you host of heaven!\",\"doc embedding\"],[\"O all you host of heaven!\",\"wmd bow\"],[\"O all you host of heaven!\",\"wmd nbow\"],[\"O all you host of heaven!\",\"wsb\"],[\"hell itself should gape\",\"doc embedding\"],[\"hell itself should gape\",\"wmd bow\"],[\"hell itself should gape\",\"wmd nbow\"],[\"hell itself should gape\",\"wsb\"],[\"frailty, thy name is woman\",\"doc embedding\"],[\"frailty, thy name is woman\",\"wmd bow\"],[\"frailty, thy name is woman\",\"wmd nbow\"],[\"frailty, thy name is woman\",\"wsb\"],[\"we will not carry coals\",\"doc embedding\"],[\"we will not carry coals\",\"wmd bow\"],[\"we will not carry coals\",\"wmd nbow\"],[\"we will not carry coals\",\"wsb\"],[\"All the world's a stage\",\"doc embedding\"],[\"All the world's a stage\",\"wmd bow\"],[\"All the world's a stage\",\"wmd nbow\"],[\"All the world's a stage\",\"wsb\"],[\"livers white as milk\",\"doc embedding\"],[\"livers white as milk\",\"wmd bow\"],[\"livers white as milk\",\"wmd nbow\"],[\"livers white as milk\",\"wsb\"],[\"I do bear a brain.\",\"doc embedding\"],[\"I do bear a brain.\",\"wmd bow\"],[\"I do bear a brain.\",\"wmd nbow\"],[\"I do bear a brain.\",\"wsb\"],[\"planets strike\",\"doc embedding\"],[\"planets strike\",\"wmd bow\"],[\"planets strike\",\"wmd nbow\"],[\"planets strike\",\"wsb\"],[\"though this be madness, yet there is method in it\",\"doc embedding\"],[\"though this be madness, yet there is method in it\",\"wmd bow\"],[\"though this be madness, yet there is method in it\",\"wmd nbow\"],[\"though this be madness, yet there is method in it\",\"wsb\"],[\"Illo, ho, ho, my lord\",\"doc embedding\"],[\"Illo, ho, ho, my lord\",\"wmd bow\"],[\"Illo, ho, ho, my lord\",\"wmd nbow\"],[\"Illo, ho, ho, my lord\",\"wsb\"],[\"springes to catch woodcocks\",\"doc embedding\"],[\"springes to catch woodcocks\",\"wmd bow\"],[\"springes to catch woodcocks\",\"wmd nbow\"],[\"springes to catch woodcocks\",\"wsb\"],[\"thereby hangs a tale\",\"doc embedding\"],[\"thereby hangs a tale\",\"wmd bow\"],[\"thereby hangs a tale\",\"wmd nbow\"],[\"thereby hangs a tale\",\"wsb\"],[\"go, by Saint Hieronimo\",\"doc embedding\"],[\"go, by Saint Hieronimo\",\"wmd bow\"],[\"go, by Saint Hieronimo\",\"wmd nbow\"],[\"go, by Saint Hieronimo\",\"wsb\"],[\"a horse, a horse, my kingdom for a horse\",\"doc embedding\"],[\"a horse, a horse, my kingdom for a horse\",\"wmd bow\"],[\"a horse, a horse, my kingdom for a horse\",\"wmd nbow\"],[\"a horse, a horse, my kingdom for a horse\",\"wsb\"],[\"In my mind's eye\",\"doc embedding\"],[\"In my mind's eye\",\"wmd bow\"],[\"In my mind's eye\",\"wmd nbow\"],[\"In my mind's eye\",\"wsb\"],[\"an old man is twice a child\",\"doc embedding\"],[\"an old man is twice a child\",\"wmd bow\"],[\"an old man is twice a child\",\"wmd nbow\"],[\"an old man is twice a child\",\"wsb\"],[\"The rest is silence.\",\"doc embedding\"],[\"The rest is silence.\",\"wmd bow\"],[\"The rest is silence.\",\"wmd nbow\"],[\"The rest is silence.\",\"wsb\"],[\"pampered jades of Asia\",\"doc embedding\"],[\"pampered jades of Asia\",\"wmd bow\"],[\"pampered jades of Asia\",\"wmd nbow\"],[\"pampered jades of Asia\",\"wsb\"],[\"sea of troubles\",\"doc embedding\"],[\"sea of troubles\",\"wmd bow\"],[\"sea of troubles\",\"wmd nbow\"],[\"sea of troubles\",\"wsb\"],[\"to be or not to be\",\"doc embedding\"],[\"to be or not to be\",\"wmd bow\"],[\"to be or not to be\",\"wmd nbow\"],[\"to be or not to be\",\"wsb\"],[\"mean NDCG\",\"doc embedding\"],[\"mean NDCG\",\"wmd bow\"],[\"mean NDCG\",\"wmd nbow\"],[\"mean NDCG\",\"wsb\"]]},\"id\":\"1725\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1744\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1818\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"high\":1,\"low\":0,\"palette\":[\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\"]},\"id\":\"1746\",\"type\":\"LinearColorMapper\"},{\"attributes\":{},\"id\":\"1727\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1737\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1736\"},\"ticker\":null},\"id\":\"1739\",\"type\":\"Grid\"},{\"attributes\":{\"axis_label\":\"NDCG\",\"formatter\":{\"id\":\"1818\"},\"major_label_policy\":{\"id\":\"1816\"},\"ticker\":{\"id\":\"1737\"}},\"id\":\"1736\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1734\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"1732\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_multi\":null},\"id\":\"1743\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis\":{\"id\":\"1740\"},\"dimension\":1,\"ticker\":null,\"visible\":false},\"id\":\"1742\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1741\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"data\":{\"color\":{\"__ndarray__\":\"AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/AAAAAAAAAABVVVVVVVXVP1VVVVVVVeU/AAAAAAAA8D8AAAAAAAAAAFVVVVVVVdU/VVVVVVVV5T8AAAAAAADwPwAAAAAAAAAAVVVVVVVV1T9VVVVVVVXlPwAAAAAAAPA/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[84]},\"ndcg\":{\"__ndarray__\":\"AAAAAAAA8D8AAAAAAADwPw3M0AFVc+c/AAAAAAAA8D/LV/ypc5rnP8rV+HarKOA/BraH0gBa7T8i7qLgmgPsPwAAAAAAAPA/ch1uxGwt6D92yhiMcfPsPwAAAAAAAPA/6Izh+8ka7j9wbTTmgv7pP/frNracbus/XXL6/YBq7D8AAAAAAADwPwrFXHQYnOo/xFVlPAEY6T9Z/Uhuc8/pP2Fs+fcEyeM/6CTqUgZN7j955obRH6HfP+4uNV9vB+U/AAAAAAAA8D9Ql1fYf3rsP/XYLvLYEe8/3QpwGNpz7z9eBOtsOlvuP4FjE/NWnOo/AAAAAAAA8D8AAAAAAADwP3VfecZ2I+4/PGWLik3a2j+baGP7lSflP3P7J+HKBdg/pwyQsCsy7D9hlELneCnsP9Kc5XTt4O4/NjZs2yaV7j93UfFXpQ3tP2NOF5O5C+M/AAAAAAAA8D/5TKjKQaTvP+gk6lIGTe4/SZkuXrOv6z8AAAAAAADwPwAAAAAAAPA/QsB7XTc97T8+GaOs2qrvPwAAAAAAAPA/AAAAAAAA8D/Urn7nd6jqPzWWZYb0ve0/HP1prQgy7T+2GwzZS8nvP6aeC3eK6+E/JLzlMd2o5T+Aoul6i0rlP14gFQ085+g/AAAAAAAA8D+SB/FAXULePxZewdUoDO0/e6U8JLpF7j+6u8R6Ki/lP1+6Z86ELuk/vYfmaXxw7j8AAAAAAADwP83XAh2lfOo/NdpRSEdb4D8AAAAAAADwPwAAAAAAAPA/PhmjrNqq7z+Ile9o94LqPz4Zo6zaqu8/AAAAAAAA8D+EpSYwM87rP/KE4Bq7d+4/eeQhvsfV7D+5xOVzrVPvP5iHqYT+H+w/VGdNv7LE6D+L3qZFAi/sP1iW3+qNKe0/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[84]},\"ndcg_str\":[\"100.0%\",\"100.0%\",\"73.3%\",\"100.0%\",\"73.8%\",\"50.5%\",\"91.7%\",\"87.5%\",\"100.0%\",\"75.6%\",\"90.5%\",\"100.0%\",\"94.1%\",\"81.2%\",\"85.7%\",\"88.8%\",\"100.0%\",\"83.2%\",\"78.4%\",\"80.7%\",\"61.8%\",\"94.7%\",\"49.4%\",\"65.7%\",\"100.0%\",\"89.0%\",\"97.1%\",\"98.3%\",\"94.9%\",\"83.2%\",\"100.0%\",\"100.0%\",\"94.2%\",\"42.0%\",\"66.1%\",\"37.5%\",\"88.1%\",\"88.0%\",\"96.5%\",\"95.6%\",\"90.8%\",\"59.5%\",\"100.0%\",\"98.9%\",\"94.7%\",\"86.5%\",\"100.0%\",\"100.0%\",\"91.4%\",\"99.0%\",\"100.0%\",\"100.0%\",\"83.3%\",\"92.9%\",\"91.2%\",\"99.3%\",\"56.0%\",\"67.7%\",\"66.5%\",\"77.8%\",\"100.0%\",\"47.3%\",\"90.8%\",\"94.6%\",\"66.2%\",\"78.7%\",\"95.1%\",\"100.0%\",\"82.8%\",\"51.1%\",\"100.0%\",\"100.0%\",\"99.0%\",\"82.8%\",\"99.0%\",\"100.0%\",\"86.9%\",\"95.2%\",\"90.1%\",\"97.9%\",\"87.9%\",\"77.4%\",\"88.1%\",\"91.1%\"],\"phrase\":[[\"O all you host of heaven!\",\"doc embedding\"],[\"O all you host of heaven!\",\"wmd bow\"],[\"O all you host of heaven!\",\"wmd nbow\"],[\"O all you host of heaven!\",\"wsb\"],[\"hell itself should gape\",\"doc embedding\"],[\"hell itself should gape\",\"wmd bow\"],[\"hell itself should gape\",\"wmd nbow\"],[\"hell itself should gape\",\"wsb\"],[\"frailty, thy name is woman\",\"doc embedding\"],[\"frailty, thy name is woman\",\"wmd bow\"],[\"frailty, thy name is woman\",\"wmd nbow\"],[\"frailty, thy name is woman\",\"wsb\"],[\"we will not carry coals\",\"doc embedding\"],[\"we will not carry coals\",\"wmd bow\"],[\"we will not carry coals\",\"wmd nbow\"],[\"we will not carry coals\",\"wsb\"],[\"All the world's a stage\",\"doc embedding\"],[\"All the world's a stage\",\"wmd bow\"],[\"All the world's a stage\",\"wmd nbow\"],[\"All the world's a stage\",\"wsb\"],[\"livers white as milk\",\"doc embedding\"],[\"livers white as milk\",\"wmd bow\"],[\"livers white as milk\",\"wmd nbow\"],[\"livers white as milk\",\"wsb\"],[\"I do bear a brain.\",\"doc embedding\"],[\"I do bear a brain.\",\"wmd bow\"],[\"I do bear a brain.\",\"wmd nbow\"],[\"I do bear a brain.\",\"wsb\"],[\"planets strike\",\"doc embedding\"],[\"planets strike\",\"wmd bow\"],[\"planets strike\",\"wmd nbow\"],[\"planets strike\",\"wsb\"],[\"though this be madness, yet there is method in it\",\"doc embedding\"],[\"though this be madness, yet there is method in it\",\"wmd bow\"],[\"though this be madness, yet there is method in it\",\"wmd nbow\"],[\"though this be madness, yet there is method in it\",\"wsb\"],[\"Illo, ho, ho, my lord\",\"doc embedding\"],[\"Illo, ho, ho, my lord\",\"wmd bow\"],[\"Illo, ho, ho, my lord\",\"wmd nbow\"],[\"Illo, ho, ho, my lord\",\"wsb\"],[\"springes to catch woodcocks\",\"doc embedding\"],[\"springes to catch woodcocks\",\"wmd bow\"],[\"springes to catch woodcocks\",\"wmd nbow\"],[\"springes to catch woodcocks\",\"wsb\"],[\"thereby hangs a tale\",\"doc embedding\"],[\"thereby hangs a tale\",\"wmd bow\"],[\"thereby hangs a tale\",\"wmd nbow\"],[\"thereby hangs a tale\",\"wsb\"],[\"go, by Saint Hieronimo\",\"doc embedding\"],[\"go, by Saint Hieronimo\",\"wmd bow\"],[\"go, by Saint Hieronimo\",\"wmd nbow\"],[\"go, by Saint Hieronimo\",\"wsb\"],[\"a horse, a horse, my kingdom for a horse\",\"doc embedding\"],[\"a horse, a horse, my kingdom for a horse\",\"wmd bow\"],[\"a horse, a horse, my kingdom for a horse\",\"wmd nbow\"],[\"a horse, a horse, my kingdom for a horse\",\"wsb\"],[\"In my mind's eye\",\"doc embedding\"],[\"In my mind's eye\",\"wmd bow\"],[\"In my mind's eye\",\"wmd nbow\"],[\"In my mind's eye\",\"wsb\"],[\"an old man is twice a child\",\"doc embedding\"],[\"an old man is twice a child\",\"wmd bow\"],[\"an old man is twice a child\",\"wmd nbow\"],[\"an old man is twice a child\",\"wsb\"],[\"The rest is silence.\",\"doc embedding\"],[\"The rest is silence.\",\"wmd bow\"],[\"The rest is silence.\",\"wmd nbow\"],[\"The rest is silence.\",\"wsb\"],[\"pampered jades of Asia\",\"doc embedding\"],[\"pampered jades of Asia\",\"wmd bow\"],[\"pampered jades of Asia\",\"wmd nbow\"],[\"pampered jades of Asia\",\"wsb\"],[\"sea of troubles\",\"doc embedding\"],[\"sea of troubles\",\"wmd bow\"],[\"sea of troubles\",\"wmd nbow\"],[\"sea of troubles\",\"wsb\"],[\"to be or not to be\",\"doc embedding\"],[\"to be or not to be\",\"wmd bow\"],[\"to be or not to be\",\"wmd nbow\"],[\"to be or not to be\",\"wsb\"],[\"mean NDCG\",\"doc embedding\"],[\"mean NDCG\",\"wmd bow\"],[\"mean NDCG\",\"wmd nbow\"],[\"mean NDCG\",\"wsb\"]],\"y\":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166]},\"selected\":{\"id\":\"1819\"},\"selection_policy\":{\"id\":\"1820\"}},\"id\":\"1745\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"1726\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.1\"}};\n",
       "  var render_items = [{\"docid\":\"5c1fcd87-dadb-42d8-9118-da4387913bf5\",\"root_ids\":[\"1726\"],\"roots\":{\"1726\":\"716bb90e-8886-4357-a384-494dbc5b30fc\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1726"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbutils.plot_ndcgs(\n",
    "    gold_data, dict((k, v.build_index()) for k, v in index_builders.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c628c94",
   "metadata": {},
   "source": [
    "We see that some queries obtain 100%, i.e. the top results match the optimal ones given in our gold standard. We see that Waterman-Smith-Beyer (WSB) tends to perform a little better than Word Mover's Distance (WMD), with the exception of \"though this be madness...\", where WMD outperforms WSB. In general the Vectorian modification of WMD, which does not use nbow, performs better than the original description of WMD. The one exception here is \"livers white as milk\".\n",
    "\n",
    "One advantage of WSB over the full WMD variants is its easy interpretability. WSB produces an alignment that relates one document token to at most one query token. For WMD, this assumption often breaks down, which makes the results harder to understand. We use this characteristic of WSB in the following section to illustrate which mappings actually occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd85e5",
   "metadata": {},
   "source": [
    "### 3.3.3 Focussing on single queries <a class=\"anchor\" id=\"section_3_3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66f030",
   "metadata": {},
   "source": [
    "We now investigate some queries, for which the performance for WSB is bad, to get a better understanding for why our search fails to obtain the optimal (true positive) results at the top of the result list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a94f0e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85de1adfda9849f38c285674547af1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<span style=\"line-height:normal;\"><p>Partition similarity is computed via <b>alignme…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_builder = make_index_builder()\n",
    "index_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96320ac9",
   "metadata": {},
   "source": [
    "We turn to the query with the lowest score (\"though this be madness, yet there is a method in it\") in the previous evaluation, and look at its results in some more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdd1f1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1897\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:42609/autoload.js?bokeh-autoload-element=1897&bokeh-absolute-url=http://localhost:42609&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "9ac05383ff2c4d97851747da3f8a1189"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c70a130616741c89b2044c21d2d1e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n\\t<iframe\\n\\t\\tid=\"vectorian-1631187567575916449-139748624151456-1\"\\n\\t\\twidth=\"1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_a = nbutils.plot_results(\n",
    "    gold_data, index_builder.build_index(), \"though this be madness\", rank=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa1df7",
   "metadata": {},
   "source": [
    "The best match obtained here (red bar on rank 6) is anchored on two word matches, namely `madness` (a 100% match) and `methods` (a 72% match). The other words are quite different and there is no good alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a52b9ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1929\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:43749/autoload.js?bokeh-autoload-element=1929&bokeh-absolute-url=http://localhost:43749&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "923e87cdb4b54a569b592b2bf880f5d0"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d2851f71534c6fbdb118b666bc1fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n\\t<iframe\\n\\t\\tid=\"vectorian-1631187567701455375-139748623432624-1\"\\n\\t\\twidth=\"1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_b = nbutils.plot_results(\n",
    "    gold_data, index_builder.build_index(), \"though this be madness\", rank=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08472e31",
   "metadata": {},
   "source": [
    "Above we see the rank 3 result from the same query, which is a false positive - i.e. our search proclaims it is a better result than the one we saw before, but in fact this result is not relevant according to our gold standard. If we analyze why this result gets such a high score nonetheless, we see that \"is\" and \"in\" both contribute 100% scores. In contrast to the scores before, 100% for \"madness\" and 72% for \"methods\", this partially explains the higher overall score (if we assume for now that the contributions from the other tokens are somewhat similar).\n",
    "\n",
    "We will now try to understand why the true positive results are ranked rather low. To find a solution for this issue, we will be looking at the composition of scores for each result we obtain for this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c42e0bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf2523bb2994c0a9c64e7e6de8d1547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='indicate_gap_penalty'), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function nbutils.vis_token_scores.<locals>.plot(indicate_gap_penalty)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbutils.vis_token_scores(\n",
    "    plot_b.matches[:50],\n",
    "    highlight={\"token\": [\"madness\", \"method\"], \"rank\": [6, 20, 35, 45]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193d37f",
   "metadata": {},
   "source": [
    "The true positive results are marked with black triangles. We see that our current search strategy isn't doing a very good job of ranking them highly. Looking at the score composition of the relevant results, we can identify two distinct features: all relevant results show a rather large contribution of either \"madness\" (look at rank 6 and rank 35, for example) and/or a rather large contribution of \"method\" (ranks 45 and 6). However, these contributions do not lead to higher ranks necessarily, since other words such as \"is\", \"this\" and \"though\" score higher for other results: for example, look at the contribution of \"in\" for rank 1.\n",
    "\n",
    "In the plot below, we visualize this observation using ranks 1, 6 and 35. Comparing the rank 1 result on the left - which is a false positive - with the two relevant results on the right, we see that \"in\", \"through\" and \"is\" make up for large parts of the score for rank 1, whereas \"madness\" is a considerable factor for the two relevant matches. Unfortunately, this contribution is not sufficient to bring these results to higher ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5baf690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ae7ba74a0b4806ad57f70b9c755a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='plot_as', index=1, options=('bar', 'pie'), value='pie'), Outp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(plot_as=widgets.ToggleButtons(options=[\"bar\", \"pie\"], value=\"pie\"))\n",
    "def plot(plot_as):\n",
    "    nbutils.vis_token_scores(\n",
    "        plot_b.matches, kind=plot_as, ranks=[1, 6, 35], plot_width=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44136c",
   "metadata": {},
   "source": [
    "The distributions of score contributions we just observed are the motivation for our approach to tag-weighted alignments as they are described in Liebl & Burghardt (2020a/b). Nagoudi and Schwab (2017) used a similar idea of POS tag-weighting for computing sentence similarity, but did not combine it with alignments.\n",
    "\n",
    "We now demonstrate (POS) tag-weighted alignments, by using a tag-weighted alignment that will weight nouns like \"madness\" and \"method\" 3 times higher than other word types. \"NN\" is a Penn Treebank tag and identifies singular nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7311b2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d063aa91ee7472cb8d789ddbd9346b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<span style=\"line-height:normal;\"><p>Partition similarity is computed via <b>tag-wei…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tag_weighted_index_builder = make_index_builder(\n",
    "    strategy=\"Tag-Weighted Alignment\", strategy_options={\"tag_weights\": {\"NN\": 3}}\n",
    ")\n",
    "tag_weighted_index_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "663137b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"2625\">\n",
       "  var xhr = new XMLHttpRequest()\n",
       "  xhr.responseType = 'blob';\n",
       "  xhr.open('GET', \"http://localhost:42675/autoload.js?bokeh-autoload-element=2625&bokeh-absolute-url=http://localhost:42675&resources=none\", true);\n",
       "  \n",
       "  xhr.onload = function (event) {\n",
       "    var script = document.createElement('script'),\n",
       "    src = URL.createObjectURL(event.target.response);\n",
       "    script.src = src;\n",
       "    document.body.appendChild(script);\n",
       "  };\n",
       "xhr.send();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "3e4ba78fa65d4abaaf1bbf0fb274035d"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d7c0cf0a8049779d02b7de944e12c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<nbutils.ResultScoresPlotter at 0x7f19c2f21370>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbutils.plot_results(\n",
    "    gold_data, tag_weighted_index_builder.build_index(), \"though this be madness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2ee24",
   "metadata": {},
   "source": [
    "Tag-weighting moves the correct results far to the top, namely to ranks 1, 2, 4 and 6. By increasing the NN weight to 5, it is possible to promote the true positive on rank 73 to rank 15. This is a bit of an extreme measure though and we will not investigate it further here. Instead we investigate how the weighting affects the other queries. Therefore, we re-run the nDCG computation and compare it against unweighted WSB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06013d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54e7ed1b614483e86b7f78995209f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<span style=\"line-height:normal;\"><p>Partition similarity is computed via <b>alignme…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_builder_unweighted = make_index_builder()\n",
    "index_builder_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b88c896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f9214384fb40ef8ab0641c4170cc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"899956c8-c610-4623-b60c-d26de72f5dcf\" data-root-id=\"2647\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"f779f5b3-d684-4827-8e8a-a84dd26b5804\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"2657\"}],\"center\":[{\"id\":\"2660\"},{\"id\":\"2663\"},{\"id\":\"2673\"}],\"height\":800,\"left\":[{\"id\":\"2661\"}],\"renderers\":[{\"id\":\"2671\"}],\"title\":{\"id\":\"2648\"},\"toolbar\":{\"id\":\"2664\"},\"toolbar_location\":null,\"width\":1000,\"x_range\":{\"id\":\"2665\"},\"x_scale\":{\"id\":\"2653\"},\"y_range\":{\"id\":\"2646\"},\"y_scale\":{\"id\":\"2655\"}},\"id\":\"2647\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis\":{\"id\":\"2657\"},\"ticker\":null},\"id\":\"2660\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2667\"}},\"line_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2667\"}},\"right\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"2669\",\"type\":\"HBar\"},{\"attributes\":{\"formatter\":{\"id\":\"2789\"},\"group_label_orientation\":0,\"major_label_policy\":{\"id\":\"2787\"},\"ticker\":{\"id\":\"2662\"}},\"id\":\"2661\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"2666\"},\"glyph\":{\"id\":\"2669\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2670\"},\"view\":{\"id\":\"2672\"}},\"id\":\"2671\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"2648\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"2655\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"level\":\"glyph\",\"source\":{\"id\":\"2666\"},\"text\":{\"field\":\"ndcg_str\"},\"text_align\":{\"value\":\"right\"},\"text_baseline\":{\"value\":\"middle\"},\"text_color\":{\"value\":\"white\"},\"text_font_size\":{\"value\":\"8pt\"},\"x\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"2673\",\"type\":\"LabelSet\"},{\"attributes\":{},\"id\":\"2794\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"2662\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"factors\":[[\"O all you host of heaven!\",\"wsb_weighted\"],[\"O all you host of heaven!\",\"wsb_unweighted\"],[\"hell itself should gape\",\"wsb_weighted\"],[\"hell itself should gape\",\"wsb_unweighted\"],[\"frailty, thy name is woman\",\"wsb_weighted\"],[\"frailty, thy name is woman\",\"wsb_unweighted\"],[\"we will not carry coals\",\"wsb_weighted\"],[\"we will not carry coals\",\"wsb_unweighted\"],[\"All the world's a stage\",\"wsb_weighted\"],[\"All the world's a stage\",\"wsb_unweighted\"],[\"livers white as milk\",\"wsb_weighted\"],[\"livers white as milk\",\"wsb_unweighted\"],[\"I do bear a brain.\",\"wsb_weighted\"],[\"I do bear a brain.\",\"wsb_unweighted\"],[\"planets strike\",\"wsb_weighted\"],[\"planets strike\",\"wsb_unweighted\"],[\"though this be madness, yet there is method in it\",\"wsb_weighted\"],[\"though this be madness, yet there is method in it\",\"wsb_unweighted\"],[\"Illo, ho, ho, my lord\",\"wsb_weighted\"],[\"Illo, ho, ho, my lord\",\"wsb_unweighted\"],[\"springes to catch woodcocks\",\"wsb_weighted\"],[\"springes to catch woodcocks\",\"wsb_unweighted\"],[\"thereby hangs a tale\",\"wsb_weighted\"],[\"thereby hangs a tale\",\"wsb_unweighted\"],[\"go, by Saint Hieronimo\",\"wsb_weighted\"],[\"go, by Saint Hieronimo\",\"wsb_unweighted\"],[\"a horse, a horse, my kingdom for a horse\",\"wsb_weighted\"],[\"a horse, a horse, my kingdom for a horse\",\"wsb_unweighted\"],[\"In my mind's eye\",\"wsb_weighted\"],[\"In my mind's eye\",\"wsb_unweighted\"],[\"an old man is twice a child\",\"wsb_weighted\"],[\"an old man is twice a child\",\"wsb_unweighted\"],[\"The rest is silence.\",\"wsb_weighted\"],[\"The rest is silence.\",\"wsb_unweighted\"],[\"pampered jades of Asia\",\"wsb_weighted\"],[\"pampered jades of Asia\",\"wsb_unweighted\"],[\"sea of troubles\",\"wsb_weighted\"],[\"sea of troubles\",\"wsb_unweighted\"],[\"to be or not to be\",\"wsb_weighted\"],[\"to be or not to be\",\"wsb_unweighted\"],[\"mean NDCG\",\"wsb_weighted\"],[\"mean NDCG\",\"wsb_unweighted\"]]},\"id\":\"2646\",\"type\":\"FactorRange\"},{\"attributes\":{\"axis\":{\"id\":\"2661\"},\"dimension\":1,\"ticker\":null,\"visible\":false},\"id\":\"2663\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"2790\",\"type\":\"AllLabels\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2667\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2667\"}},\"right\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"2670\",\"type\":\"HBar\"},{\"attributes\":{},\"id\":\"2658\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"2792\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"2653\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_multi\":null},\"id\":\"2664\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"2665\",\"type\":\"Range1d\"},{\"attributes\":{\"data\":{\"color\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/AAAAAAAAAAAAAAAAAADwPwAAAAAAAAAAAAAAAAAA8D8AAAAAAAAAAAAAAAAAAPA/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[42]},\"ndcg\":{\"__ndarray__\":\"AAAAAAAA8D8AAAAAAADwP/hDX8Uaa+w/pQrXk54n7D/dCnAY2nPvPwAAAAAAAPA/0Dwx+Tlo7D/QPDH5OWjsPwj42kpabu0/Wf1IbnPP6T8+C5tRzxHnP7ooV0PGFeU/AAAAAAAA8D/dCnAY2nPvPwAAAAAAAPA/AAAAAAAA8D85N1pJEQbsPzjs6MNgg9c/NSZnkTQy7T/Wxr5jHFvuP/zTguGSQO8//NOC4ZJA7z8AAAAAAADwPwAAAAAAAPA/PhmjrNqq7z8+GaOs2qrvP05BFeIW4u8/AAAAAAAA8D/oaqBqf/XuPwfI5Us01+c/D6FvxED/7D9eBOtsOlvuP72H5ml8cO4/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/PhmjrNqq7z8AAAAAAADwP2RwzRnmd+8/ZHDNGeZ37z/QguRnDlHuP8VkevmSFO0/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[42]},\"ndcg_str\":[\"100.0%\",\"100.0%\",\"88.8%\",\"88.0%\",\"98.3%\",\"100.0%\",\"88.8%\",\"88.8%\",\"92.0%\",\"80.7%\",\"72.1%\",\"65.9%\",\"100.0%\",\"98.3%\",\"100.0%\",\"100.0%\",\"87.6%\",\"36.7%\",\"91.2%\",\"94.9%\",\"97.7%\",\"97.7%\",\"100.0%\",\"100.0%\",\"99.0%\",\"99.0%\",\"99.6%\",\"100.0%\",\"96.7%\",\"74.5%\",\"90.6%\",\"94.9%\",\"95.1%\",\"100.0%\",\"100.0%\",\"100.0%\",\"99.0%\",\"100.0%\",\"98.3%\",\"98.3%\",\"94.7%\",\"90.9%\"],\"phrase\":[[\"O all you host of heaven!\",\"wsb_weighted\"],[\"O all you host of heaven!\",\"wsb_unweighted\"],[\"hell itself should gape\",\"wsb_weighted\"],[\"hell itself should gape\",\"wsb_unweighted\"],[\"frailty, thy name is woman\",\"wsb_weighted\"],[\"frailty, thy name is woman\",\"wsb_unweighted\"],[\"we will not carry coals\",\"wsb_weighted\"],[\"we will not carry coals\",\"wsb_unweighted\"],[\"All the world's a stage\",\"wsb_weighted\"],[\"All the world's a stage\",\"wsb_unweighted\"],[\"livers white as milk\",\"wsb_weighted\"],[\"livers white as milk\",\"wsb_unweighted\"],[\"I do bear a brain.\",\"wsb_weighted\"],[\"I do bear a brain.\",\"wsb_unweighted\"],[\"planets strike\",\"wsb_weighted\"],[\"planets strike\",\"wsb_unweighted\"],[\"though this be madness, yet there is method in it\",\"wsb_weighted\"],[\"though this be madness, yet there is method in it\",\"wsb_unweighted\"],[\"Illo, ho, ho, my lord\",\"wsb_weighted\"],[\"Illo, ho, ho, my lord\",\"wsb_unweighted\"],[\"springes to catch woodcocks\",\"wsb_weighted\"],[\"springes to catch woodcocks\",\"wsb_unweighted\"],[\"thereby hangs a tale\",\"wsb_weighted\"],[\"thereby hangs a tale\",\"wsb_unweighted\"],[\"go, by Saint Hieronimo\",\"wsb_weighted\"],[\"go, by Saint Hieronimo\",\"wsb_unweighted\"],[\"a horse, a horse, my kingdom for a horse\",\"wsb_weighted\"],[\"a horse, a horse, my kingdom for a horse\",\"wsb_unweighted\"],[\"In my mind's eye\",\"wsb_weighted\"],[\"In my mind's eye\",\"wsb_unweighted\"],[\"an old man is twice a child\",\"wsb_weighted\"],[\"an old man is twice a child\",\"wsb_unweighted\"],[\"The rest is silence.\",\"wsb_weighted\"],[\"The rest is silence.\",\"wsb_unweighted\"],[\"pampered jades of Asia\",\"wsb_weighted\"],[\"pampered jades of Asia\",\"wsb_unweighted\"],[\"sea of troubles\",\"wsb_weighted\"],[\"sea of troubles\",\"wsb_unweighted\"],[\"to be or not to be\",\"wsb_weighted\"],[\"to be or not to be\",\"wsb_unweighted\"],[\"mean NDCG\",\"wsb_weighted\"],[\"mean NDCG\",\"wsb_unweighted\"]],\"y\":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82]},\"selected\":{\"id\":\"2793\"},\"selection_policy\":{\"id\":\"2794\"}},\"id\":\"2666\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"high\":1,\"low\":0,\"palette\":[\"#66c2a5\",\"#fc8d62\",\"#8da0cb\"]},\"id\":\"2667\",\"type\":\"LinearColorMapper\"},{\"attributes\":{},\"id\":\"2793\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"2666\"}},\"id\":\"2672\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2787\",\"type\":\"AllLabels\"},{\"attributes\":{\"axis_label\":\"NDCG\",\"formatter\":{\"id\":\"2792\"},\"major_label_policy\":{\"id\":\"2790\"},\"ticker\":{\"id\":\"2658\"}},\"id\":\"2657\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"2789\",\"type\":\"CategoricalTickFormatter\"}],\"root_ids\":[\"2647\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.1\"}};\n",
       "  var render_items = [{\"docid\":\"f779f5b3-d684-4827-8e8a-a84dd26b5804\",\"root_ids\":[\"2647\"],\"roots\":{\"2647\":\"899956c8-c610-4623-b60c-d26de72f5dcf\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "2647"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbutils.plot_ndcgs(\n",
    "    gold_data,\n",
    "    {\n",
    "        \"wsb_unweighted\": index_builder_unweighted.build_index(),\n",
    "        \"wsb_weighted\": tag_weighted_index_builder.build_index(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a49983",
   "metadata": {},
   "source": [
    "In the end, the evaluation shows that we considerably increased our accuracy through employing POS tag weighting in this scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283d93a",
   "metadata": {},
   "source": [
    "## 3.4 The influence of different embeddings <a class=\"anchor\" id=\"section_3_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265e37a",
   "metadata": {},
   "source": [
    "While we have varied the similarity metrics in the previous example, we will now look into the effect that different embeddings might have on the results. The caveat here is that we are using compressed embeddings, i.e. we would need to verify these results with uncompressed embeddings. Still, the performance of compressed fastText seems very solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e78507f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22609f7fec98449086a6b2acd93ac8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Tab(children=(HTML(value='<span style=\"line-height:normal;\"><p>Partition similarity is com…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_builders = {}\n",
    "\n",
    "# for each embedding, define a search strategy based on tag-weighted alignments\n",
    "for e in the_embeddings.values():\n",
    "    index_builders[e.name] = make_index_builder(\n",
    "        strategy=\"Tag-Weighted Alignment\",\n",
    "        strategy_options={\"tag_weights\": {\"NN\": 3}, \"similarity\": {\"embedding\": e}},\n",
    "    )\n",
    "\n",
    "# present an UI to interactively edit these search strategies\n",
    "accordion = widgets.Accordion(children=[x.displayable for x in index_builders.values()])\n",
    "for i, k in enumerate(index_builders.keys()):\n",
    "    accordion.set_title(i, k)\n",
    "accordion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db53567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ced8b080344181b65f5a23a777f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"f2043488-a2bb-4bc0-b13d-70fe3dacc447\" data-root-id=\"2861\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"35729b9d-d8a8-4ae3-8286-95eeeeb46aa1\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"2871\"}],\"center\":[{\"id\":\"2874\"},{\"id\":\"2877\"},{\"id\":\"2887\"}],\"height\":2000,\"left\":[{\"id\":\"2875\"}],\"renderers\":[{\"id\":\"2885\"}],\"title\":{\"id\":\"2862\"},\"toolbar\":{\"id\":\"2878\"},\"toolbar_location\":null,\"width\":1000,\"x_range\":{\"id\":\"2879\"},\"x_scale\":{\"id\":\"2867\"},\"y_range\":{\"id\":\"2860\"},\"y_scale\":{\"id\":\"2869\"}},\"id\":\"2861\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"3012\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"2862\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"2872\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis_label\":\"NDCG\",\"formatter\":{\"id\":\"3015\"},\"major_label_policy\":{\"id\":\"3013\"},\"ticker\":{\"id\":\"2872\"}},\"id\":\"2871\",\"type\":\"LinearAxis\"},{\"attributes\":{\"factors\":[[\"O all you host of heaven!\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"O all you host of heaven!\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"O all you host of heaven!\",\"numberbatch-19.08-en\"],[\"O all you host of heaven!\",\"fasttext-en-mini\"],[\"O all you host of heaven!\",\"glove-6B-50\"],[\"hell itself should gape\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"hell itself should gape\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"hell itself should gape\",\"numberbatch-19.08-en\"],[\"hell itself should gape\",\"fasttext-en-mini\"],[\"hell itself should gape\",\"glove-6B-50\"],[\"frailty, thy name is woman\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"frailty, thy name is woman\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"frailty, thy name is woman\",\"numberbatch-19.08-en\"],[\"frailty, thy name is woman\",\"fasttext-en-mini\"],[\"frailty, thy name is woman\",\"glove-6B-50\"],[\"we will not carry coals\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"we will not carry coals\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"we will not carry coals\",\"numberbatch-19.08-en\"],[\"we will not carry coals\",\"fasttext-en-mini\"],[\"we will not carry coals\",\"glove-6B-50\"],[\"All the world's a stage\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"All the world's a stage\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"All the world's a stage\",\"numberbatch-19.08-en\"],[\"All the world's a stage\",\"fasttext-en-mini\"],[\"All the world's a stage\",\"glove-6B-50\"],[\"livers white as milk\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"livers white as milk\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"livers white as milk\",\"numberbatch-19.08-en\"],[\"livers white as milk\",\"fasttext-en-mini\"],[\"livers white as milk\",\"glove-6B-50\"],[\"I do bear a brain.\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"I do bear a brain.\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"I do bear a brain.\",\"numberbatch-19.08-en\"],[\"I do bear a brain.\",\"fasttext-en-mini\"],[\"I do bear a brain.\",\"glove-6B-50\"],[\"planets strike\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"planets strike\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"planets strike\",\"numberbatch-19.08-en\"],[\"planets strike\",\"fasttext-en-mini\"],[\"planets strike\",\"glove-6B-50\"],[\"though this be madness, yet there is method in it\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"though this be madness, yet there is method in it\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"though this be madness, yet there is method in it\",\"numberbatch-19.08-en\"],[\"though this be madness, yet there is method in it\",\"fasttext-en-mini\"],[\"though this be madness, yet there is method in it\",\"glove-6B-50\"],[\"Illo, ho, ho, my lord\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"Illo, ho, ho, my lord\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"Illo, ho, ho, my lord\",\"numberbatch-19.08-en\"],[\"Illo, ho, ho, my lord\",\"fasttext-en-mini\"],[\"Illo, ho, ho, my lord\",\"glove-6B-50\"],[\"springes to catch woodcocks\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"springes to catch woodcocks\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"springes to catch woodcocks\",\"numberbatch-19.08-en\"],[\"springes to catch woodcocks\",\"fasttext-en-mini\"],[\"springes to catch woodcocks\",\"glove-6B-50\"],[\"thereby hangs a tale\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"thereby hangs a tale\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"thereby hangs a tale\",\"numberbatch-19.08-en\"],[\"thereby hangs a tale\",\"fasttext-en-mini\"],[\"thereby hangs a tale\",\"glove-6B-50\"],[\"go, by Saint Hieronimo\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"go, by Saint Hieronimo\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"go, by Saint Hieronimo\",\"numberbatch-19.08-en\"],[\"go, by Saint Hieronimo\",\"fasttext-en-mini\"],[\"go, by Saint Hieronimo\",\"glove-6B-50\"],[\"a horse, a horse, my kingdom for a horse\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"a horse, a horse, my kingdom for a horse\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"a horse, a horse, my kingdom for a horse\",\"numberbatch-19.08-en\"],[\"a horse, a horse, my kingdom for a horse\",\"fasttext-en-mini\"],[\"a horse, a horse, my kingdom for a horse\",\"glove-6B-50\"],[\"In my mind's eye\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"In my mind's eye\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"In my mind's eye\",\"numberbatch-19.08-en\"],[\"In my mind's eye\",\"fasttext-en-mini\"],[\"In my mind's eye\",\"glove-6B-50\"],[\"an old man is twice a child\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"an old man is twice a child\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"an old man is twice a child\",\"numberbatch-19.08-en\"],[\"an old man is twice a child\",\"fasttext-en-mini\"],[\"an old man is twice a child\",\"glove-6B-50\"],[\"The rest is silence.\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"The rest is silence.\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"The rest is silence.\",\"numberbatch-19.08-en\"],[\"The rest is silence.\",\"fasttext-en-mini\"],[\"The rest is silence.\",\"glove-6B-50\"],[\"pampered jades of Asia\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"pampered jades of Asia\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"pampered jades of Asia\",\"numberbatch-19.08-en\"],[\"pampered jades of Asia\",\"fasttext-en-mini\"],[\"pampered jades of Asia\",\"glove-6B-50\"],[\"sea of troubles\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"sea of troubles\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"sea of troubles\",\"numberbatch-19.08-en\"],[\"sea of troubles\",\"fasttext-en-mini\"],[\"sea of troubles\",\"glove-6B-50\"],[\"to be or not to be\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"to be or not to be\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"to be or not to be\",\"numberbatch-19.08-en\"],[\"to be or not to be\",\"fasttext-en-mini\"],[\"to be or not to be\",\"glove-6B-50\"],[\"mean NDCG\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"mean NDCG\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"mean NDCG\",\"numberbatch-19.08-en\"],[\"mean NDCG\",\"fasttext-en-mini\"],[\"mean NDCG\",\"glove-6B-50\"]]},\"id\":\"2860\",\"type\":\"FactorRange\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2881\"}},\"line_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2881\"}},\"right\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"2883\",\"type\":\"HBar\"},{\"attributes\":{\"high\":1,\"low\":0,\"palette\":[\"#66c2a5\",\"#fc8d62\",\"#8da0cb\",\"#e78ac3\",\"#a6d854\"]},\"id\":\"2881\",\"type\":\"LinearColorMapper\"},{\"attributes\":{\"axis\":{\"id\":\"2871\"},\"ticker\":null},\"id\":\"2874\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"3017\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"3010\",\"type\":\"AllLabels\"},{\"attributes\":{\"axis\":{\"id\":\"2875\"},\"dimension\":1,\"ticker\":null,\"visible\":false},\"id\":\"2877\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"3013\",\"type\":\"AllLabels\"},{\"attributes\":{\"active_multi\":null},\"id\":\"2878\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"2879\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"2867\",\"type\":\"LinearScale\"},{\"attributes\":{\"data\":{\"color\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/AAAAAAAAAAAAAAAAAADQPwAAAAAAAOA/AAAAAAAA6D8AAAAAAADwPwAAAAAAAAAAAAAAAAAA0D8AAAAAAADgPwAAAAAAAOg/AAAAAAAA8D8AAAAAAAAAAAAAAAAAANA/AAAAAAAA4D8AAAAAAADoPwAAAAAAAPA/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[105]},\"ndcg\":{\"__ndarray__\":\"AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwP+NCSYC2jOw/OS04KHde7D/YqoyRFn/tP/hDX8Uaa+w/mCM9bWYh7j/12C7y2BHvP90KcBjac+8/7y+e4wCo7T/dCnAY2nPvPwAAAAAAAPA/ItlcZ9iS6z/s5Wo8HG7sP3OkA9hjhus/0Dwx+Tlo7D8WI9rQLOnrPwj42kpabu0/CPjaSlpu7T8AAAAAAADwPwj42kpabu0/AAAAAAAA8D8QjBmmgVflP//FMOIkV+Y/5exikoVY1j8+C5tRzxHnP6AJbOQzItk/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwP1Nn4c9/5ew/PhmjrNqq7z8+GaOs2qrvPwAAAAAAAPA/5HL8ZlKU7j/35sZZ1TbsP27qK3ofyOs/YSVr6rL16j85N1pJEQbsP09tv62MwOk/HyjBnuVL7D+MWQkV+QvtP1S/xdKFWOw/NSZnkTQy7T84Mu/T+s3tP3GFE4BCt+8/2akMWUSx7j8twAjCGZ/tP/zTguGSQO8/KXgHotgv7j8AAAAAAADwPwAAAAAAAPA/VebeDSn+7D8AAAAAAADwP//jnM+Gwug/v2YCxIwR7z8+GaOs2qrvP3aNHR3wyuY/PhmjrNqq7z/SW08BpQTnPzUPJ5pStO8/thsM2UvJ7z9UafWlOaHrP05BFeIW4u8/Aymvcw257T8AAAAAAADwP+hqoGp/9e4/AAAAAAAA8D/oaqBqf/XuP1Xm3g0p/uw/5HL8ZlKU7j+RN0lSd53sP1pnoxXamuo/D6FvxED/7D9HN/ERcffsPz4Zo6zaqu8/vYfmaXxw7j/3c4opSQPpP72H5ml8cO4/uq3U1+dA7D8AAAAAAADwPwAAAAAAAPA/RYiB1aDU7D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8+GaOs2qrvP/NvUaWbfOw/PhmjrNqq7z8aGotJHvrrP/Xq0w326O8/148GrgNr7z8iX2855GLvP2RwzRnmd+8/zKpyt4if7j+zkb4tcDPuPytytTYuMO4/YiO75uwb7D/QguRnDlHuP8KwnH9Wduw/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[105]},\"ndcg_str\":[\"100.0%\",\"100.0%\",\"100.0%\",\"100.0%\",\"100.0%\",\"89.2%\",\"88.7%\",\"92.2%\",\"88.8%\",\"94.2%\",\"97.1%\",\"98.3%\",\"92.7%\",\"98.3%\",\"100.0%\",\"86.2%\",\"88.8%\",\"86.0%\",\"88.8%\",\"87.2%\",\"92.0%\",\"92.0%\",\"100.0%\",\"92.0%\",\"100.0%\",\"66.7%\",\"69.8%\",\"34.9%\",\"72.1%\",\"39.3%\",\"100.0%\",\"100.0%\",\"100.0%\",\"100.0%\",\"100.0%\",\"90.3%\",\"99.0%\",\"99.0%\",\"100.0%\",\"95.6%\",\"88.2%\",\"86.8%\",\"84.2%\",\"87.6%\",\"80.5%\",\"88.4%\",\"90.8%\",\"88.6%\",\"91.2%\",\"93.1%\",\"99.1%\",\"95.9%\",\"92.6%\",\"97.7%\",\"94.3%\",\"100.0%\",\"100.0%\",\"90.6%\",\"100.0%\",\"77.4%\",\"97.1%\",\"99.0%\",\"71.2%\",\"99.0%\",\"71.9%\",\"99.1%\",\"99.3%\",\"86.3%\",\"99.6%\",\"92.9%\",\"100.0%\",\"96.7%\",\"100.0%\",\"96.7%\",\"90.6%\",\"95.6%\",\"89.4%\",\"83.1%\",\"90.6%\",\"90.5%\",\"99.0%\",\"95.1%\",\"78.2%\",\"95.1%\",\"88.3%\",\"100.0%\",\"100.0%\",\"90.1%\",\"100.0%\",\"100.0%\",\"100.0%\",\"99.0%\",\"89.0%\",\"99.0%\",\"87.4%\",\"99.7%\",\"98.2%\",\"98.1%\",\"98.3%\",\"95.7%\",\"94.4%\",\"94.3%\",\"87.8%\",\"94.7%\",\"88.9%\"],\"phrase\":[[\"O all you host of heaven!\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"O all you host of heaven!\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"O all you host of heaven!\",\"numberbatch-19.08-en\"],[\"O all you host of heaven!\",\"fasttext-en-mini\"],[\"O all you host of heaven!\",\"glove-6B-50\"],[\"hell itself should gape\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"hell itself should gape\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"hell itself should gape\",\"numberbatch-19.08-en\"],[\"hell itself should gape\",\"fasttext-en-mini\"],[\"hell itself should gape\",\"glove-6B-50\"],[\"frailty, thy name is woman\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"frailty, thy name is woman\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"frailty, thy name is woman\",\"numberbatch-19.08-en\"],[\"frailty, thy name is woman\",\"fasttext-en-mini\"],[\"frailty, thy name is woman\",\"glove-6B-50\"],[\"we will not carry coals\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"we will not carry coals\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"we will not carry coals\",\"numberbatch-19.08-en\"],[\"we will not carry coals\",\"fasttext-en-mini\"],[\"we will not carry coals\",\"glove-6B-50\"],[\"All the world's a stage\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"All the world's a stage\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"All the world's a stage\",\"numberbatch-19.08-en\"],[\"All the world's a stage\",\"fasttext-en-mini\"],[\"All the world's a stage\",\"glove-6B-50\"],[\"livers white as milk\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"livers white as milk\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"livers white as milk\",\"numberbatch-19.08-en\"],[\"livers white as milk\",\"fasttext-en-mini\"],[\"livers white as milk\",\"glove-6B-50\"],[\"I do bear a brain.\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"I do bear a brain.\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"I do bear a brain.\",\"numberbatch-19.08-en\"],[\"I do bear a brain.\",\"fasttext-en-mini\"],[\"I do bear a brain.\",\"glove-6B-50\"],[\"planets strike\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"planets strike\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"planets strike\",\"numberbatch-19.08-en\"],[\"planets strike\",\"fasttext-en-mini\"],[\"planets strike\",\"glove-6B-50\"],[\"though this be madness, yet there is method in it\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"though this be madness, yet there is method in it\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"though this be madness, yet there is method in it\",\"numberbatch-19.08-en\"],[\"though this be madness, yet there is method in it\",\"fasttext-en-mini\"],[\"though this be madness, yet there is method in it\",\"glove-6B-50\"],[\"Illo, ho, ho, my lord\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"Illo, ho, ho, my lord\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"Illo, ho, ho, my lord\",\"numberbatch-19.08-en\"],[\"Illo, ho, ho, my lord\",\"fasttext-en-mini\"],[\"Illo, ho, ho, my lord\",\"glove-6B-50\"],[\"springes to catch woodcocks\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"springes to catch woodcocks\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"springes to catch woodcocks\",\"numberbatch-19.08-en\"],[\"springes to catch woodcocks\",\"fasttext-en-mini\"],[\"springes to catch woodcocks\",\"glove-6B-50\"],[\"thereby hangs a tale\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"thereby hangs a tale\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"thereby hangs a tale\",\"numberbatch-19.08-en\"],[\"thereby hangs a tale\",\"fasttext-en-mini\"],[\"thereby hangs a tale\",\"glove-6B-50\"],[\"go, by Saint Hieronimo\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"go, by Saint Hieronimo\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"go, by Saint Hieronimo\",\"numberbatch-19.08-en\"],[\"go, by Saint Hieronimo\",\"fasttext-en-mini\"],[\"go, by Saint Hieronimo\",\"glove-6B-50\"],[\"a horse, a horse, my kingdom for a horse\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"a horse, a horse, my kingdom for a horse\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"a horse, a horse, my kingdom for a horse\",\"numberbatch-19.08-en\"],[\"a horse, a horse, my kingdom for a horse\",\"fasttext-en-mini\"],[\"a horse, a horse, my kingdom for a horse\",\"glove-6B-50\"],[\"In my mind's eye\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"In my mind's eye\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"In my mind's eye\",\"numberbatch-19.08-en\"],[\"In my mind's eye\",\"fasttext-en-mini\"],[\"In my mind's eye\",\"glove-6B-50\"],[\"an old man is twice a child\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"an old man is twice a child\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"an old man is twice a child\",\"numberbatch-19.08-en\"],[\"an old man is twice a child\",\"fasttext-en-mini\"],[\"an old man is twice a child\",\"glove-6B-50\"],[\"The rest is silence.\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"The rest is silence.\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"The rest is silence.\",\"numberbatch-19.08-en\"],[\"The rest is silence.\",\"fasttext-en-mini\"],[\"The rest is silence.\",\"glove-6B-50\"],[\"pampered jades of Asia\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"pampered jades of Asia\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"pampered jades of Asia\",\"numberbatch-19.08-en\"],[\"pampered jades of Asia\",\"fasttext-en-mini\"],[\"pampered jades of Asia\",\"glove-6B-50\"],[\"sea of troubles\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"sea of troubles\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"sea of troubles\",\"numberbatch-19.08-en\"],[\"sea of troubles\",\"fasttext-en-mini\"],[\"sea of troubles\",\"glove-6B-50\"],[\"to be or not to be\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"to be or not to be\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"to be or not to be\",\"numberbatch-19.08-en\"],[\"to be or not to be\",\"fasttext-en-mini\"],[\"to be or not to be\",\"glove-6B-50\"],[\"mean NDCG\",\"https://explosion.ai/en/core_web_sm_AND_en_paraphrase_distilroberta_base_v1/3.1.0\"],[\"mean NDCG\",\"fasttext-en-mini + numberbatch-19.08-en\"],[\"mean NDCG\",\"numberbatch-19.08-en\"],[\"mean NDCG\",\"fasttext-en-mini\"],[\"mean NDCG\",\"glove-6B-50\"]],\"y\":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208]},\"selected\":{\"id\":\"3016\"},\"selection_policy\":{\"id\":\"3017\"}},\"id\":\"2880\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"2880\"},\"glyph\":{\"id\":\"2883\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2884\"},\"view\":{\"id\":\"2886\"}},\"id\":\"2885\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"3015\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"level\":\"glyph\",\"source\":{\"id\":\"2880\"},\"text\":{\"field\":\"ndcg_str\"},\"text_align\":{\"value\":\"right\"},\"text_baseline\":{\"value\":\"middle\"},\"text_color\":{\"value\":\"white\"},\"text_font_size\":{\"value\":\"8pt\"},\"x\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"2887\",\"type\":\"LabelSet\"},{\"attributes\":{\"source\":{\"id\":\"2880\"}},\"id\":\"2886\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2869\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"3016\",\"type\":\"Selection\"},{\"attributes\":{\"formatter\":{\"id\":\"3012\"},\"group_label_orientation\":0,\"major_label_policy\":{\"id\":\"3010\"},\"ticker\":{\"id\":\"2876\"}},\"id\":\"2875\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"2876\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2881\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\",\"transform\":{\"id\":\"2881\"}},\"right\":{\"field\":\"ndcg\"},\"y\":{\"field\":\"phrase\"}},\"id\":\"2884\",\"type\":\"HBar\"}],\"root_ids\":[\"2861\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.1\"}};\n",
       "  var render_items = [{\"docid\":\"35729b9d-d8a8-4ae3-8286-95eeeeb46aa1\",\"root_ids\":[\"2861\"],\"roots\":{\"2861\":\"f2043488-a2bb-4bc0-b13d-70fe3dacc447\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "2861"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbutils.plot_ndcgs(\n",
    "    gold_data, dict((k, v.build_index()) for k, v in index_builders.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac03a32",
   "metadata": {},
   "source": [
    "Some observations to be made here:\n",
    "* In a few queries (\"llo, ho, ho my lord\", \"frailty, thy name is woman\", \"hell itself should gape\"), GloVe gives slightly better results than fastText, but this cannot be generalized to the overall performance.\n",
    "\n",
    "* For some queries (\"I do bear a brain.\", \"O all you host of heaven!\") the embedding does not seem to matter at all.\n",
    "\n",
    "* A real competitor for fastText are contextual embeddings from Sentence-BERT - however, these are much more expensive in terms of computation time, storage space and code complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221deb5d",
   "metadata": {},
   "source": [
    "# 4. Conclusion <a class=\"anchor\" id=\"section_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cf643",
   "metadata": {},
   "source": [
    "In this interactive notebook we have demonstrated how different types of word embeddings can be used to detect intertextual phenomena in a semi-automatic way. We also provide a basic ground truth dataset of 100 short documents that contain 20 different quotes from Shakespeare's plays. This setup enables us to investigate the inner workings of different embeddings and to evaluate their suitability for the case of Shakespearean intertextuality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3b3ac",
   "metadata": {},
   "source": [
    "The following main findings – that also open up perspectives for future research – were obtained from this evaluation study:\n",
    "\n",
    "1. POS tag-weighted alignments achieve the highest overall performance (in terms of nDCG) for our specific data. As a result, we want to encourage other researchers to explore the use of tag-weighting in alignments, as it seems to be a powerful approach that is not widely known in the literature.\n",
    "\n",
    "2. Document embeddings also show a strong performance. Their special appeal lies in easy use (assuming a pretrained model), as they do not rely on additional WSB or WMD mappings. \n",
    "\n",
    "3. When using WMD on short texts, our bow variant outperforms the original nbow variant by Kusner et al. (2015) by quite a margin. Further research must show whether these findings also hold for longer documents and how they evaluate when compared to optimized variants like RWMD (Kusner et al., 2015) and other variants (Werner & Laber, 2020).\n",
    "\n",
    "4. In terms of embeddings, compressed fastText embeddings clearly outperform compressed GloVe and Numberbatch embeddings in our evaluations. Since we used low-dimensional embeddings for this notebook, these results are only a first hint and need to be verified through the use of full (high-dimensional) embbeddings. Contextual token embeddings extracted from Sentence-BERT provide a similar performance as fastText - however in terms of computation time and memory they are way more complex. All in all, our evaluation seems to highlight the solid performance of fastText."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829e464",
   "metadata": {},
   "source": [
    "While we were able to gather some interesting insights for the specific use case of Shakespearean intertextuality, the main goal of this notebook is to provide an interactive platform that enables other researchers to investigate other parameter combinations as well. The code blocks and widgets above offer many ways to play around with the settings and explore their effects on the ground truth data. \n",
    "\n",
    "More importantly, researchers can also import their own (English language) data to the notebook and experiment with all of the funtions and parameters that are part of the [Vectorian API](https://github.com/poke1024/vectorian-2021). We hope this notebook provides a low-threshold access to the toolbox of embeddings for other researchers in the field of intertextuality studies and thus adds to a critical reflection of these new methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce0575",
   "metadata": {},
   "source": [
    "# 5. Interactive searches with your own data <a class=\"anchor\" id=\"section_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53cf6d",
   "metadata": {},
   "source": [
    "First specify the text documents you want to search through by an upload widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55478ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f985757020d468898e5a9c7511a52ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.txt', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "upload = widgets.FileUpload(accept=\".txt\", multiple=True)\n",
    "\n",
    "upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ba2ba",
   "metadata": {},
   "source": [
    "From this upload widget contents, we now build a Vectorian session we can perform search through. As always with Vectorian session, we need to specify the embeddings that we want to employ for searching. We also need an `nlp` instance for importing the text documents. Depending on the size and number of documents, this step can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b767872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>cannot run on empty upload. please provide at least one text file.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectorian.importers import TextImporter\n",
    "from vectorian.session import LabSession\n",
    "\n",
    "import codecs\n",
    "\n",
    "\n",
    "def files_to_session(upload):\n",
    "    if not upload.value:\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<strong>cannot run on empty upload. please provide at least one text file.</strong>\"))\n",
    "        return None\n",
    "\n",
    "    im = TextImporter(nlp)\n",
    "\n",
    "    # for each uploaded file, import it via importer \"im\" and add to \"docs\"\n",
    "    docs = []\n",
    "    for k, data in upload.value.items():\n",
    "        docs.append(\n",
    "            im(codecs.decode(data[\"content\"], encoding=\"utf-8\"), title=k)\n",
    "        )\n",
    "\n",
    "    return LabSession(\n",
    "        docs,\n",
    "        embeddings=[the_embeddings[x] for x in [\"numberbatch\", \"fasttext\"]],\n",
    "        normalizers=\"default\",\n",
    "    )\n",
    "\n",
    "\n",
    "upload_session = files_to_session(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850e23d",
   "metadata": {},
   "source": [
    "Now we present the full interactive search interface the Vectorian offers (we have hidden it so far and focussed on a subset). Note that in contrast to our experiments earlier, we do not search on the *document* level by default, but rather the *sentence* level - i.e. we split each document into sentences and then search on each sentence. You can change this in the \"Partition\" dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "022a1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_session.interact(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd87c8",
   "metadata": {},
   "source": [
    "# 6. References\n",
    " <a class=\"anchor\" id=\"section_6\"></a>\n",
    "Bamman, David & Crane, Gregory (2008). The logic and discovery of textual allusion. In Proceedings of the 2008 LREC Workshop on Language Technology for Cultural Heritage Data.\n",
    "\n",
    "Bär, Daniel, Zesch, Torsten & Gurevych, Iryna (2012). Text reuse detection using a composition of text similarity measures. In Proceedings of COLING 2012, p. 167–184.\n",
    "\n",
    "Büchler, Marco, Geßner, Annette, Berti, Monica & Eckart, Thomas (2013). Measuring the influence of a work by text re-use. Bulletin of the Institute of Classical Studies. Supplement, p. 63–79.\n",
    "\n",
    "Burghardt, Manuel, Meyer, Selina, Schmidtbauer, Stephanie & Molz, Johannes (2019). “The Bard meets the Doctor” – Computergestützte Identifikation intertextueller Shakespearebezüge in der Science Fiction-Serie Dr. Who. Book of Abstracts, DHd.\n",
    "\n",
    "Chandrasekaran, Dhivya & Mago, Vijay (2021). Evolution of Semantic Similarity – A Survey. ACM Computing Surveys (CSUR), 54(2), p. 1-37.\n",
    "\n",
    "Faruqui, Manaal, Tsvetkov, Yulia, Rastogi, Pushpendre & Dyer, Chris (2016). Problems With Evaluation of Word Embeddings Using Word Similarity Tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, p. 30-35.\n",
    "\n",
    "Forstall, Christopher, Coffee, Neil, Buck, Thomas, Roache, Katherine & Jacobson, Sarah (2015). Modeling the scholars: Detecting intertextuality through enhanced word-level n-gram matching. Digital Scholarship in the Humanities, 30(4), p. 503–515.\n",
    "\n",
    "Genette, Gérard (1993). Palimpseste. Die Literatur auf zweiter Stufe. Suhrkamp.\n",
    "\n",
    "Kusner, Matt, Sun, Yu, Kolkin, Nicholas & Weinberger, Kilian (2015). From word embeddings to document distances. In International conference on machine learning, p. 957-966.\n",
    "\n",
    "Liebl, Bernhard & Burghardt, Manuel (2020a). „The Vectorian“ – Eine parametrisierbare Suchmaschine für intertextuelle Referenzen. Book of Abstracts, DHd 2020, Paderborn.\n",
    "\n",
    "Liebl, Bernhard & Burghardt, Manuel (2020b). “Shakespeare in The Vectorian Age” – An Evaluation of Different Word Embeddings and NLP Parameters for the Detection of Shakespeare Quotes”. Proceedings of the 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LateCH), co-located with COLING’2020.\n",
    "\n",
    "Manjavacas, Enrique, Long, Brian & Kestemont, Mike (2019). On the feasibility of automated detection of allusive text reuse. Proceedings of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature.\n",
    "\n",
    "Mikolov, Tomas, Chen, Kai, Corrado, Greg & Dean, Jeffrey (2013). Efficient estimation of word representations in vector space. In Proceedings of International Conference on Learning Representations (ICLR 2013). arXiv preprint arXiv:1301.3781.\n",
    "\n",
    "Mikolov, Tomas, Grave, Edouard, Bojanowski, Piotr, Puhrsch, Christian & Joulin, Armand (2018). Advances in pretraining distributed word representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). arXiv preprint arXiv:1712.09405.\n",
    "\n",
    "Nagoudi, El Moatez Billah & Schwab, Didier (2017). Semantic Similarity of Arabic Sentences with Word Embeddings. In Proceedings of the 3rd Arabic Natural Language Processing Workshop, Association for Computational Linguistics, 2017, p. 18–24.\n",
    "\n",
    "Pennington, Jeffrey, Socher, Richard & Manning, Christopher D. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), p. 1532-1543.\n",
    "\n",
    "Reimers, Nils & Gurevych, Iryna (2019). Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\n",
    "\n",
    "Scheirer, Walter, Forstall, Christopher & Coffee, Neil (2014). The sense of a connection: Automatic tracing of intertextuality by meaning. Digital Scholarship in the Humanities, 31(1), p. 204–217.\n",
    "\n",
    "Sohangir, Sahar & Wang, Dingding (2017). Document Understanding Using Improved Sqrt-Cosine Similarity. In Proceedings of the 2017 IEEE 11th International Conference on Semantic Computing (ICSC), p. 278-279.\n",
    "\n",
    "Speer, Robyn, Chin, Joshua & Havasi, Catherine (2017). Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), p. 4444–4451.\n",
    "\n",
    "Wang, Yuxuan, Hou, Yutai, Che, Wanxiang & Liu, Ting (2020). From static to dynamic word representations: A survey. International Journal of Machine Learning and Cybernetics 11, p. 1611–1630.\n",
    "\n",
    "Waterman, Michael S., Smith, Temple F. & Beyer, William A. (1976). Some biological sequence metrics. Advances in Mathematics 20(3), p. 367-387.\n",
    "\n",
    "Werner, Matheus & Laber, Eduardo (2020). Speeding up Word Mover's Distance and its variants via properties of distances between embeddings. In Proceedings of the 24th European Conference on Artificial Intelligence (ECAI 2020), p. 2204-2211.\n",
    "\n",
    "Wieting, John, Bansal, Mohit, Gimpel, Kevin & Livescu, Karen (2015). Towards universal paraphrastic sentence embeddings. Proceedings of the 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
